diff --git a/CMakeLists.txt b/CMakeLists.txt
index 58330608769..f647e7fb3d4 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -48,7 +48,6 @@ endif()
 
 if(ENABLE_PYTHON)
     add_compile_definitions(ENABLE_PYTHON)
-    add_compile_definitions(ENABLE_MINDDATA_PYTHON)
 endif()
 
 if(${CMAKE_SYSTEM_NAME} MATCHES "Darwin")
diff --git a/build.bat b/build.bat
index 2c8b6dfa979..0c524033f4d 100644
--- a/build.bat
+++ b/build.bat
@@ -70,10 +70,10 @@ IF "%1%" == "lite" (
     rd /s /q "%BASE_PATH%\output"
     (git log -1 | findstr "^commit") > %BUILD_PATH%\.commit_id
     IF defined VisualStudioVersion (
-        cmake -DMSLITE_MINDDATA_IMPLEMENT=off -DMSLITE_ENABLE_TRAIN=off -DVERSION_STR=%VERSION_STR% ^
+        cmake -DMSLITE_ENABLE_TRAIN=off -DVERSION_STR=%VERSION_STR% ^
             -DCMAKE_BUILD_TYPE=Release -G "Ninja" "%BASE_PATH%/mindspore/lite"
     ) ELSE (
-        cmake -DMSLITE_MINDDATA_IMPLEMENT=off -DMSLITE_ENABLE_TRAIN=off -DVERSION_STR=%VERSION_STR% ^
+        cmake -DMSLITE_ENABLE_TRAIN=off -DVERSION_STR=%VERSION_STR% ^
             -DCMAKE_BUILD_TYPE=Release -G "CodeBlocks - MinGW Makefiles" "%BASE_PATH%/mindspore/lite"
     )
 ) ELSE (
diff --git a/cmake/package_lite.cmake b/cmake/package_lite.cmake
index de8de6c1ab9..9e0408fad2c 100644
--- a/cmake/package_lite.cmake
+++ b/cmake/package_lite.cmake
@@ -224,131 +224,131 @@ function(__install_ascend_ascendc)
 endfunction()
 
 # full mode will also package the files of lite_cv mode.
-if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "full")
-    # full header files
-    install(FILES
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/constants.h
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/data_helper.h
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/execute.h
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/iterator.h
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/samplers.h
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/transforms.h
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/vision_lite.h
-            ${TOP_DIR}/mindspore/lite/minddata/dataset/liteapi/include/datasets.h
-        DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-
-    if(PLATFORM_ARM64)
-        if((MSLITE_ENABLE_CLOUD_FUSION_INFERENCE OR MSLITE_ENABLE_CLOUD_INFERENCE) AND MSLITE_ENABLE_ACL)
-            install(FILES ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/vision_ascend.h
-                    DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-            install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/kernels-dvpp-image/utils/libdvpp_utils.so
-                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        endif()
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.a DESTINATION
-                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/mindspore/lite/build/securec/src/libsecurec.a
-                DESTINATION ${SECUREC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-    elseif(PLATFORM_ARM32)
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.a DESTINATION
-                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/mindspore/lite/build/securec/src/libsecurec.a
-                DESTINATION ${SECUREC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-    else()
-        if((MSLITE_ENABLE_CLOUD_FUSION_INFERENCE OR MSLITE_ENABLE_CLOUD_INFERENCE) AND MSLITE_ENABLE_ACL)
-                install(FILES ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/vision_ascend.h
-                        DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-                install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/kernels-dvpp-image/utils/libdvpp_utils.so
-                        DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        endif()
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.a DESTINATION
-                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${jpeg_turbo_LIBPATH}/libjpeg.so.62.4.0 DESTINATION ${TURBO_DIR}/lib
-                RENAME libjpeg.so.62 COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${jpeg_turbo_LIBPATH}/libturbojpeg.so.0.3.0 DESTINATION ${TURBO_DIR}/lib
-                RENAME libturbojpeg.so.0 COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/mindspore/lite/build/securec/src/libsecurec.a
-                DESTINATION ${SECUREC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-    endif()
-
-    # lite_cv header files
-    install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
-            DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
-endif()
+#if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "full")
+#    # full header files
+#    install(FILES
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/constants.h
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/data_helper.h
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/execute.h
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/iterator.h
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/samplers.h
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/transforms.h
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/vision_lite.h
+#            ${TOP_DIR}/mindspore/lite/minddata/dataset/liteapi/include/datasets.h
+#        DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#
+#    if(PLATFORM_ARM64)
+#        if((MSLITE_ENABLE_CLOUD_FUSION_INFERENCE OR MSLITE_ENABLE_CLOUD_INFERENCE) AND MSLITE_ENABLE_ACL)
+#            install(FILES ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/vision_ascend.h
+#                    DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#            install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/kernels-dvpp-image/utils/libdvpp_utils.so
+#                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        endif()
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.a DESTINATION
+#                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/securec/src/libsecurec.a
+#                DESTINATION ${SECUREC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    elseif(PLATFORM_ARM32)
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.a DESTINATION
+#                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/securec/src/libsecurec.a
+#                DESTINATION ${SECUREC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    else()
+#        if((MSLITE_ENABLE_CLOUD_FUSION_INFERENCE OR MSLITE_ENABLE_CLOUD_INFERENCE) AND MSLITE_ENABLE_ACL)
+#                install(FILES ${TOP_DIR}/mindspore/lite/minddata/dataset/include/dataset/vision_ascend.h
+#                        DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/kernels-dvpp-image/utils/libdvpp_utils.so
+#                        DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        endif()
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.a DESTINATION
+#                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${jpeg_turbo_LIBPATH}/libjpeg.so.62.4.0 DESTINATION ${TURBO_DIR}/lib
+#                RENAME libjpeg.so.62 COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${jpeg_turbo_LIBPATH}/libturbojpeg.so.0.3.0 DESTINATION ${TURBO_DIR}/lib
+#                RENAME libturbojpeg.so.0 COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/securec/src/libsecurec.a
+#                DESTINATION ${SECUREC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    endif()
+#
+#    # lite_cv header files
+#    install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
+#            DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
+#endif()
 
-if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "wrapper")
-    install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/include/ DESTINATION ${MIND_DATA_INC_DIR}
-            COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h" PATTERN "vision.h" EXCLUDE)
-    if(PLATFORM_ARM64)
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
-    elseif(PLATFORM_ARM32)
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
-    else()
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${jpeg_turbo_LIBPATH}/libjpeg.so.62.4.0 DESTINATION ${TURBO_DIR}/lib RENAME libjpeg.so.62
-                COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${jpeg_turbo_LIBPATH}/libturbojpeg.so.0.3.0 DESTINATION ${TURBO_DIR}/lib RENAME libturbojpeg.so.0
-                COMPONENT ${RUNTIME_COMPONENT_NAME})
-    endif()
-endif()
+#if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "wrapper")
+#    install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/include/ DESTINATION ${MIND_DATA_INC_DIR}
+#            COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h" PATTERN "vision.h" EXCLUDE)
+#    if(PLATFORM_ARM64)
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    elseif(PLATFORM_ARM32)
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${JPEGTURBO_LIB_LIST} DESTINATION ${TURBO_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    else()
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${jpeg_turbo_LIBPATH}/libjpeg.so.62.4.0 DESTINATION ${TURBO_DIR}/lib RENAME libjpeg.so.62
+#                COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${jpeg_turbo_LIBPATH}/libturbojpeg.so.0.3.0 DESTINATION ${TURBO_DIR}/lib RENAME libturbojpeg.so.0
+#                COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    endif()
+#endif()
 
-if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "lite")
-    install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/include/ DESTINATION ${MIND_DATA_INC_DIR}
-            COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
-    if(PLATFORM_ARM64)
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libjpeg.so DESTINATION ${TURBO_DIR}/lib
-                COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libturbojpeg.so DESTINATION ${TURBO_DIR}/lib
-                COMPONENT ${RUNTIME_COMPONENT_NAME})
-    elseif(PLATFORM_ARM32)
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libjpeg.so DESTINATION ${TURBO_DIR}/lib
-                COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libturbojpeg.so DESTINATION ${TURBO_DIR}/lib
-                COMPONENT ${RUNTIME_COMPONENT_NAME})
-    else()
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libjpeg.so.62.4.0
-                DESTINATION ${TURBO_DIR}/lib RENAME libjpeg.so.62 COMPONENT ${RUNTIME_COMPONENT_NAME})
-        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libturbojpeg.so.0.3.0
-                DESTINATION ${TURBO_DIR}/lib RENAME libturbojpeg.so.0 COMPONENT ${RUNTIME_COMPONENT_NAME})
-    endif()
-endif()
+#if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "lite")
+#    install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/include/ DESTINATION ${MIND_DATA_INC_DIR}
+#            COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
+#    if(PLATFORM_ARM64)
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libjpeg.so DESTINATION ${TURBO_DIR}/lib
+#                COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libturbojpeg.so DESTINATION ${TURBO_DIR}/lib
+#                COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    elseif(PLATFORM_ARM32)
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libjpeg.so DESTINATION ${TURBO_DIR}/lib
+#                COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libturbojpeg.so DESTINATION ${TURBO_DIR}/lib
+#                COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    else()
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libjpeg.so.62.4.0
+#                DESTINATION ${TURBO_DIR}/lib RENAME libjpeg.so.62 COMPONENT ${RUNTIME_COMPONENT_NAME})
+#        install(FILES ${TOP_DIR}/third_party/libjpeg-turbo/lib/libturbojpeg.so.0.3.0
+#                DESTINATION ${TURBO_DIR}/lib RENAME libturbojpeg.so.0 COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    endif()
+#endif()
 
-if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "lite_cv")
-    if(PLATFORM_ARM64)
-        install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
-                DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so
-                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-    elseif(PLATFORM_ARM32)
-        install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
-                DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-    else()
-        install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
-                DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
-        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
-                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
-    endif()
-endif()
+#if(MSLITE_MINDDATA_IMPLEMENT STREQUAL "lite_cv")
+#    if(PLATFORM_ARM64)
+#        install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
+#                DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so
+#                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    elseif(PLATFORM_ARM32)
+#        install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
+#                DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#        ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    else()
+#        install(DIRECTORY ${TOP_DIR}/mindspore/lite/minddata/dataset/kernels/image/lite_cv
+#                DESTINATION ${MIND_DATA_INC_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME} FILES_MATCHING PATTERN "*.h")
+#        install(FILES ${TOP_DIR}/mindspore/lite/build/minddata/libminddata-lite.so DESTINATION
+#                ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#    endif()
+#endif()
 
 if(WIN32)
     install(FILES ${TOP_DIR}/build/.commit_id DESTINATION ${RUNTIME_PKG_NAME}
@@ -413,14 +413,16 @@ if(PLATFORM_ARM64)
         install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/convert/libruntime_convert_plugin.so
                 DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
         if(MSLITE_ENABLE_ACL)
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/kernel/ascend/libascend_kernel_plugin.so
-                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+        #     install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/kernel/ascend/libascend_kernel_plugin.so
+        #             DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             if(NOT MSLITE_SIMPLEST_CLOUD_INFERENCE)
                 install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/delegate/ascend_ge/libascend_ge_plugin.so
                         DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+                install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/delegate/ascend_acl/libascend_acl_plugin.so
+                        DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             endif()
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/cxx_api/llm_engine/libllm_engine_plugin.so
-                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/cxx_api/llm_engine/libllm_engine_plugin.so
+#                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             __install_ascend_tbe_and_aicpu()
             __install_ascend_ascendc()
         endif()
@@ -434,8 +436,8 @@ if(PLATFORM_ARM64)
         install(FILES ${TOP_DIR}/mindspore/lite/build/src/${MINDSPORE_LITE_LIB_NAME}.a DESTINATION ${RUNTIME_LIB_DIR}
                 COMPONENT ${RUNTIME_COMPONENT_NAME})
         if(MSLITE_ENABLE_ACL)
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/litert/kernel/ascend/libascend_kernel_plugin.so
-                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+        #     install(FILES ${TOP_DIR}/mindspore/lite/build/src/litert/kernel/ascend/libascend_kernel_plugin.so
+        #             DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
         endif()
     endif()
     if(MSLITE_ENABLE_MODEL_OBF)
@@ -538,11 +540,11 @@ if(PLATFORM_ARM64)
             endif()
             if(MSLITE_ENABLE_ACL)
                 set(LITE_ACL_DIR ${TOP_DIR}/mindspore/lite/build/tools/converter/adapter/acl)
-                install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
-                        DESTINATION ${CONVERTER_ROOT_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
+#                        DESTINATION ${CONVERTER_ROOT_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
                 if(MSLITE_ENABLE_RUNTIME_CONVERT)
-                    install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
-                            DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                    install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
+#                            DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
                     install(FILES ${glog_LIBPATH}/${glog_name} DESTINATION ${RUNTIME_LIB_DIR}
                             RENAME libmindspore_glog.so.0 COMPONENT ${RUNTIME_COMPONENT_NAME})
                     install(TARGETS mindspore_core mindspore_ops DESTINATION ${CONVERTER_ROOT_DIR}/lib
@@ -586,25 +588,25 @@ if(PLATFORM_ARM64)
                             COMPONENT ${RUNTIME_COMPONENT_NAME})
                 endif()
             endif()
-            if((MSLITE_ENABLE_CLOUD_FUSION_INFERENCE OR MSLITE_ENABLE_CLOUD_INFERENCE)
-                AND MSLITE_ENABLE_GRAPH_KERNEL AND CMAKE_SYSTEM_NAME MATCHES "Linux")
-                if(EXISTS ${BUILD_DIR}/akg)
-                    set(AKG_PATH ${BUILD_DIR}/akg)
-                    file(REMOVE_RECURSE ${AKG_PATH}/build/akg/lib)
-                    install(DIRECTORY  ${AKG_PATH}/build/akg
-                            DESTINATION ${BUILD_DIR}/package/mindspore_lite
-                            COMPONENT ${RUNTIME_COMPONENT_NAME})
-                    install(FILES ${AKG_PATH}/${AKG_PKG_PATH}
-                            DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
-                            COMPONENT ${RUNTIME_COMPONENT_NAME})
-                    install(FILES ${AKG_PATH}/${AKG_PKG_PATH}.sha256
-                            DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
-                            COMPONENT ${RUNTIME_COMPONENT_NAME})
-                    install(FILES ${AKG_PATH}/build/libakg.so
-                            DESTINATION ${BUILD_DIR}/package/mindspore_lite/lib
-                            COMPONENT ${RUNTIME_COMPONENT_NAME})
-                endif()
-            endif()
+#            if((MSLITE_ENABLE_CLOUD_FUSION_INFERENCE OR MSLITE_ENABLE_CLOUD_INFERENCE)
+#                AND MSLITE_ENABLE_GRAPH_KERNEL AND CMAKE_SYSTEM_NAME MATCHES "Linux")
+#                if(EXISTS ${BUILD_DIR}/akg)
+#                    set(AKG_PATH ${BUILD_DIR}/akg)
+#                    file(REMOVE_RECURSE ${AKG_PATH}/build/akg/lib)
+#                    install(DIRECTORY  ${AKG_PATH}/build/akg
+#                            DESTINATION ${BUILD_DIR}/package/mindspore_lite
+#                            COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                    install(FILES ${AKG_PATH}/${AKG_PKG_PATH}
+#                            DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
+#                            COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                    install(FILES ${AKG_PATH}/${AKG_PKG_PATH}.sha256
+#                            DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
+#                            COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                    install(FILES ${AKG_PATH}/build/libakg.so
+#                            DESTINATION ${BUILD_DIR}/package/mindspore_lite/lib
+#                            COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                endif()
+#            endif()
         endif()
     endif()
     if(MSLITE_ENABLE_TESTCASES)
@@ -673,14 +675,16 @@ elseif(PLATFORM_ARM32)
         install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/convert/libruntime_convert_plugin.so
                 DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
         if(MSLITE_ENABLE_ACL)
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/kernel/ascend/libascend_kernel_plugin.so
-                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+        #     install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/kernel/ascend/libascend_kernel_plugin.so
+        #             DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             if(NOT MSLITE_SIMPLEST_CLOUD_INFERENCE)
                 install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/delegate/ascend_ge/libascend_ge_plugin.so
                         DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+                install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/delegate/ascend_acl/libascend_acl_plugin.so
+                        DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             endif()
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/cxx_api/llm_engine/libllm_engine_plugin.so
-                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/cxx_api/llm_engine/libllm_engine_plugin.so
+#                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             __install_ascend_tbe_and_aicpu()
             __install_ascend_ascendc()
         endif()
@@ -694,8 +698,8 @@ elseif(PLATFORM_ARM32)
         install(FILES ${TOP_DIR}/mindspore/lite/build/src/${MINDSPORE_LITE_LIB_NAME}.a DESTINATION ${RUNTIME_LIB_DIR}
                 COMPONENT ${RUNTIME_COMPONENT_NAME})
         if(MSLITE_ENABLE_ACL)
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/litert/kernel/ascend/libascend_kernel_plugin.so
-                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+        #     install(FILES ${TOP_DIR}/mindspore/lite/build/src/litert/kernel/ascend/libascend_kernel_plugin.so
+        #             DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
         endif()
     endif()
     if(MSLITE_ENABLE_MODEL_OBF)
@@ -882,14 +886,16 @@ else()
         install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/convert/libruntime_convert_plugin.so
                 DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
         if(MSLITE_ENABLE_ACL)
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/kernel/ascend/libascend_kernel_plugin.so
-                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+        #     install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/kernel/ascend/libascend_kernel_plugin.so
+        #             DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             if(NOT MSLITE_SIMPLEST_CLOUD_INFERENCE)
                 install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/delegate/ascend_ge/libascend_ge_plugin.so
                         DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+                install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/delegate/ascend_acl/libascend_acl_plugin.so
+                        DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             endif()
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/cxx_api/llm_engine/libllm_engine_plugin.so
-                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#            install(FILES ${TOP_DIR}/mindspore/lite/build/src/extendrt/cxx_api/llm_engine/libllm_engine_plugin.so
+#                DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
             __install_ascend_tbe_and_aicpu()
             __install_ascend_ascendc()
             if(MSLITE_ASCEND_TARGET)
@@ -913,8 +919,8 @@ else()
         install(FILES ${TOP_DIR}/mindspore/lite/build/src/${MINDSPORE_LITE_LIB_NAME}.a DESTINATION ${RUNTIME_LIB_DIR}
                 COMPONENT ${RUNTIME_COMPONENT_NAME})
         if(MSLITE_ENABLE_ACL)
-            install(FILES ${TOP_DIR}/mindspore/lite/build/src/litert/kernel/ascend/libascend_kernel_plugin.so
-                    DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+        #     install(FILES ${TOP_DIR}/mindspore/lite/build/src/litert/kernel/ascend/libascend_kernel_plugin.so
+        #             DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
         endif()
     endif()
     if(MSLITE_ENABLE_MODEL_OBF)
@@ -977,11 +983,11 @@ else()
 
         if(MSLITE_ENABLE_ACL)
             set(LITE_ACL_DIR ${TOP_DIR}/mindspore/lite/build/tools/converter/adapter/acl)
-            install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
-                    DESTINATION ${CONVERTER_ROOT_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
+#            install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
+#                    DESTINATION ${CONVERTER_ROOT_DIR}/lib COMPONENT ${RUNTIME_COMPONENT_NAME})
             if(MSLITE_ENABLE_RUNTIME_CONVERT)
-                install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
-                        DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                install(FILES ${LITE_ACL_DIR}/mslite_shared_lib/libmslite_shared_lib.so
+#                        DESTINATION ${RUNTIME_LIB_DIR} COMPONENT ${RUNTIME_COMPONENT_NAME})
                 install(FILES ${glog_LIBPATH}/${glog_name} DESTINATION ${RUNTIME_LIB_DIR}
                         RENAME libmindspore_glog.so.0 COMPONENT ${RUNTIME_COMPONENT_NAME})
                 install(TARGETS mindspore_core mindspore_ops DESTINATION ${RUNTIME_LIB_DIR}
@@ -1032,24 +1038,24 @@ else()
         endif()
     endif()
     if(MSLITE_ENABLE_TOOLS)
-        if(MSLITE_ENABLE_GRAPH_KERNEL AND CMAKE_SYSTEM_NAME MATCHES "Linux")
-            if(EXISTS ${BUILD_DIR}/akg)
-                set(AKG_PATH ${BUILD_DIR}/akg)
-                file(REMOVE_RECURSE ${AKG_PATH}/build/akg/lib)
-                install(DIRECTORY  ${AKG_PATH}/build/akg
-                        DESTINATION ${BUILD_DIR}/package/mindspore_lite
-                        COMPONENT ${RUNTIME_COMPONENT_NAME})
-                install(FILES ${AKG_PATH}/${AKG_PKG_PATH}
-                        DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
-                        COMPONENT ${RUNTIME_COMPONENT_NAME})
-                install(FILES ${AKG_PATH}/${AKG_PKG_PATH}.sha256
-                        DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
-                        COMPONENT ${RUNTIME_COMPONENT_NAME})
-                install(FILES ${AKG_PATH}/build/libakg.so
-                        DESTINATION ${BUILD_DIR}/package/mindspore_lite/lib
-                        COMPONENT ${RUNTIME_COMPONENT_NAME})
-            endif()
-        endif()
+#        if(MSLITE_ENABLE_GRAPH_KERNEL AND CMAKE_SYSTEM_NAME MATCHES "Linux")
+#            if(EXISTS ${BUILD_DIR}/akg)
+#                set(AKG_PATH ${BUILD_DIR}/akg)
+#                file(REMOVE_RECURSE ${AKG_PATH}/build/akg/lib)
+#                install(DIRECTORY  ${AKG_PATH}/build/akg
+#                        DESTINATION ${BUILD_DIR}/package/mindspore_lite
+#                        COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                install(FILES ${AKG_PATH}/${AKG_PKG_PATH}
+#                        DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
+#                        COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                install(FILES ${AKG_PATH}/${AKG_PKG_PATH}.sha256
+#                        DESTINATION ${RUNTIME_PKG_NAME}/tools/akg
+#                        COMPONENT ${RUNTIME_COMPONENT_NAME})
+#                install(FILES ${AKG_PATH}/build/libakg.so
+#                        DESTINATION ${BUILD_DIR}/package/mindspore_lite/lib
+#                        COMPONENT ${RUNTIME_COMPONENT_NAME})
+#            endif()
+#        endif()
         if(NOT MSLITE_COMPILE_TWICE)
             install(TARGETS ${BENCHMARK_NAME} RUNTIME DESTINATION ${BENCHMARK_ROOT_DIR}
                     COMPONENT ${RUNTIME_COMPONENT_NAME})
diff --git a/include/api/types.h b/include/api/types.h
index 0fca6ed72b9..5db78fad5c6 100644
--- a/include/api/types.h
+++ b/include/api/types.h
@@ -350,6 +350,8 @@ class MS_API MSTensor {
   friend class ModelImpl;
   std::shared_ptr<Impl> impl_;
 };
+using MSTensorPtr = std::shared_ptr<MSTensor>;
+using MSTensorOrderMap = std::map<std::string, std::shared_ptr<MSTensor>>;
 
 class MS_API Buffer {
  public:
diff --git a/mindspore/ccsrc/backend/common/graph_kernel/core/graph_kernel_utils.cc b/mindspore/ccsrc/backend/common/graph_kernel/core/graph_kernel_utils.cc
index 1e9a655551f..263ed409e91 100644
--- a/mindspore/ccsrc/backend/common/graph_kernel/core/graph_kernel_utils.cc
+++ b/mindspore/ccsrc/backend/common/graph_kernel/core/graph_kernel_utils.cc
@@ -132,7 +132,7 @@ std::vector<PrimitivePtr> GkUtils::GetValidOps(const std::vector<OpWithLevel> &o
 }
 
 std::vector<PrimitivePtr> GkUtils::FilterExcludedOps(const std::vector<PrimitivePtr> &ops) {
-#ifndef MSLITE_ENABLE_GRAPH_KERNEL
+  //#ifndef MSLITE_ENABLE_GRAPH_KERNEL
   if (Callback::Instance()->GetTargetFromContext() != kGPUDevice) {
     return ops;
   }
@@ -165,9 +165,9 @@ std::vector<PrimitivePtr> GkUtils::FilterExcludedOps(const std::vector<Primitive
     }
   }
   return dst_ops;
-#else
-  return ops;
-#endif
+  //#else
+  //  return ops;
+  //#endif
 }
 
 void GkUtils::CheckOpLevel(const AnfNodePtr &node, const std::vector<OpWithLevel> &ops_with_level,
diff --git a/mindspore/ccsrc/backend/common/graph_kernel/core/parallel_op_combine.cc b/mindspore/ccsrc/backend/common/graph_kernel/core/parallel_op_combine.cc
index 97a47eae99d..d820127d424 100644
--- a/mindspore/ccsrc/backend/common/graph_kernel/core/parallel_op_combine.cc
+++ b/mindspore/ccsrc/backend/common/graph_kernel/core/parallel_op_combine.cc
@@ -229,44 +229,44 @@ bool ParallelOpCombiner::AutoUpdateInfo(const CNodePtr &to_update) {
                   << to_update->size();
     return false;
   }
-#ifndef MSLITE_ENABLE_GRAPH_KERNEL
+  //#ifndef MSLITE_ENABLE_GRAPH_KERNEL
   Callback::Instance()->ResetKernelInfo(to_update);
-#else
-  auto rep_input = to_update->input(1);
-  // NOTE: We assume the inputs' formats and types are consistent with outputs'.
-  std::string input_format = Callback::Instance()->GetTargetFromContext() == kAscendDevice ? "" : kOpFormat_NCHW;
-  auto GetPrevOutFormat = [&input_format](const CNodePtr &cnode) -> bool {
-    if (cnode == nullptr || !cnode->HasAttr(kOutputsFormat)) {
-      return false;
-    }
-    auto prev_of = GetValue<std::vector<std::string> >(cnode->GetAttr(kOutputsFormat));
-    if (prev_of.size() > 0) {
-      input_format = prev_of[0];
-      return true;
-    }
-    return false;
-  };
-  if (AnfUtils::IsRealKernel(rep_input)) {
-    (void)GetPrevOutFormat(rep_input->cast<CNodePtr>());
-  }
-  if (input_format.empty()) {
-    auto it = children_map_.find(rep_input);
-    if (it != children_map_.end()) {
-      for (auto orig_user : it->second) {
-        if (GetPrevOutFormat(orig_user->cast<CNodePtr>())) {
-          break;
-        }
-      }
-    }
-  }
-  if (input_format.empty()) {
-    MS_LOG(WARNING) << "Cannot find prev node's input format, use " << layout_
-                    << " by default and that may cause error.";
-    input_format = layout_;
-  }
-  std::vector<std::string> outputs_formats(AnfUtils::GetOutputTensorNum(to_update), input_format);
-  to_update->AddAttr(kOutputsFormat, MakeValue(outputs_formats));
-#endif
+  //#else
+  //  auto rep_input = to_update->input(1);
+  //  // NOTE: We assume the inputs' formats and types are consistent with outputs'.
+  //  std::string input_format = Callback::Instance()->GetTargetFromContext() == kAscendDevice ? "" : kOpFormat_NCHW;
+  //  auto GetPrevOutFormat = [&input_format](const CNodePtr &cnode) -> bool {
+  //    if (cnode == nullptr || !cnode->HasAttr(kOutputsFormat)) {
+  //      return false;
+  //    }
+  //    auto prev_of = GetValue<std::vector<std::string> >(cnode->GetAttr(kOutputsFormat));
+  //    if (prev_of.size() > 0) {
+  //      input_format = prev_of[0];
+  //      return true;
+  //    }
+  //    return false;
+  //  };
+  //  if (AnfUtils::IsRealKernel(rep_input)) {
+  //    (void)GetPrevOutFormat(rep_input->cast<CNodePtr>());
+  //  }
+  //  if (input_format.empty()) {
+  //    auto it = children_map_.find(rep_input);
+  //    if (it != children_map_.end()) {
+  //      for (auto orig_user : it->second) {
+  //        if (GetPrevOutFormat(orig_user->cast<CNodePtr>())) {
+  //          break;
+  //        }
+  //      }
+  //    }
+  //  }
+  //  if (input_format.empty()) {
+  //    MS_LOG(WARNING) << "Cannot find prev node's input format, use " << layout_
+  //                    << " by default and that may cause error.";
+  //    input_format = layout_;
+  //  }
+  //  std::vector<std::string> outputs_formats(AnfUtils::GetOutputTensorNum(to_update), input_format);
+  //  to_update->AddAttr(kOutputsFormat, MakeValue(outputs_formats));
+  //#endif
   return true;
 }
 
diff --git a/mindspore/ccsrc/backend/common/graph_kernel/expanders/utils.cc b/mindspore/ccsrc/backend/common/graph_kernel/expanders/utils.cc
index 4fbbe68a24d..902cb0d8b3f 100644
--- a/mindspore/ccsrc/backend/common/graph_kernel/expanders/utils.cc
+++ b/mindspore/ccsrc/backend/common/graph_kernel/expanders/utils.cc
@@ -83,17 +83,17 @@ bool OpDesc::CheckOutputs() {
                    << outputs_info_[i].type << "]";
       return false;
     }
-#ifdef MSLITE_ENABLE_GRAPH_KERNEL
-    bool format_check_condition =
-      (outputs[i]->format != kOpFormat_DEFAULT && outputs_info_[i].format != kOpFormat_DEFAULT) &&
-      outputs[i]->format != outputs_info_[i].format;
-#else
+    //#ifdef MSLITE_ENABLE_GRAPH_KERNEL
+    //    bool format_check_condition =
+    //      (outputs[i]->format != kOpFormat_DEFAULT && outputs_info_[i].format != kOpFormat_DEFAULT) &&
+    //      outputs[i]->format != outputs_info_[i].format;
+    //#else
     bool format_check_condition = outputs[i]->format != outputs_info_[i].format;
     if ((outputs[i]->format == kOpFormat_DEFAULT && outputs_info_[i].format == kOpFormat_NCHW) ||
         (outputs[i]->format == kOpFormat_NCHW && outputs_info_[i].format == kOpFormat_DEFAULT)) {
       format_check_condition = false;
     }
-#endif
+    //#endif
     if (format_check_condition) {
       MS_LOG(INFO) << "Op " << this->name_ << "'s output format [" << outputs[i]->format << "] is wrong, expect: ["
                    << outputs_info_[i].format << "]";
diff --git a/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.cc b/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.cc
index 7f206d842d9..6e16e5a8a45 100644
--- a/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.cc
+++ b/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.cc
@@ -230,17 +230,17 @@ void GraphKernelFlags::SaveJitConfig(const std::map<std::string, std::string> &j
 }
 
 std::pair<std::string, bool> GraphKernelFlags::GetGraphKernelConfig() {
-#ifdef MSLITE_ENABLE_GRAPH_KERNEL
-  std::string flags = common::GetEnv("MS_DEV_GRAPH_KERNEL_FLAGS");
-  if (flags != "") {
-    return std::make_pair(flags, false);
-  }
-  const auto &jit_config = GetJitConfig();
-  if (jit_config.find("graph_kernel_flags") != jit_config.end()) {
-    flags = jit_config.at("graph_kernel_flags");
-  }
-  return std::make_pair(flags, false);
-#else
+  //#ifdef MSLITE_ENABLE_GRAPH_KERNEL
+  //  std::string flags = common::GetEnv("MS_DEV_GRAPH_KERNEL_FLAGS");
+  //  if (flags != "") {
+  //    return std::make_pair(flags, false);
+  //  }
+  //  const auto &jit_config = GetJitConfig();
+  //  if (jit_config.find("graph_kernel_flags") != jit_config.end()) {
+  //    flags = jit_config.at("graph_kernel_flags");
+  //  }
+  //  return std::make_pair(flags, false);
+  //#else
   const auto &jit_config = GetJitConfig();
   auto context = MsContext::GetInstance();
   MS_EXCEPTION_IF_NULL(context);
@@ -270,15 +270,15 @@ std::pair<std::string, bool> GraphKernelFlags::GetGraphKernelConfig() {
     flags = iter->second;
   }
   return std::make_pair(flags, enable_gk);
-#endif
+  //#endif
 }
 
 void GraphKernelFlags::CheckSupport() const {
-#ifndef MSLITE_ENABLE_GRAPH_KERNEL
+  //#ifndef MSLITE_ENABLE_GRAPH_KERNEL
   if (IsEnableGraphKernel()) {
     auto context = MsContext::GetInstance();
     MS_EXCEPTION_IF_NULL(context);
-#ifndef USE_LLVM
+    //#ifndef USE_LLVM
     auto is_cpu = (context->get_param<std::string>(MS_CTX_DEVICE_TARGET) == kCPUDevice);
     if (is_cpu && const_cast<GraphKernelFlags *>(this)->kernel_generator == "AKG") {
       MS_LOG(WARNING)
@@ -287,7 +287,7 @@ void GraphKernelFlags::CheckSupport() const {
       const_cast<GraphKernelFlags *>(this)->opt_level = OptLevel_0;
       return;
     }
-#endif
+    //#endif
     auto is_ascend = (context->get_param<std::string>(MS_CTX_DEVICE_TARGET) == kAscendDevice);
     if (is_ascend) {
 #ifndef ENABLE_DVM
@@ -312,7 +312,7 @@ void GraphKernelFlags::CheckSupport() const {
 #endif
     }
   }
-#endif
+  //#endif
 }
 
 void GraphKernelFlags::Refresh() {
@@ -327,11 +327,11 @@ void GraphKernelFlags::Refresh() {
          "valid flags, please refer to the source code file graph_kernel_flags.h at "
          "https://gitee.com/mindspore/mindspore.";
   }
-#ifndef MSLITE_ENABLE_GRAPH_KERNEL
+  //#ifndef MSLITE_ENABLE_GRAPH_KERNEL
   if (IsEnableGraphKernel()) {
     CheckSupport();
   }
-#endif
+  //#endif
   // If enable graphkernel, Dump flags so that people can check the setting.
   if (IsEnableGraphKernel()) {
     MS_LOG(INFO) << "graph_kernel_flags = \"" << flags_cache_ << "\", all flags: " << DumpAllFlags();
@@ -432,9 +432,9 @@ void GraphKernelFlags::RegisterFlags(std::map<std::string, std::string> *flag_ma
   }
 
   if (is_ascend && !has_kernel_generator) {
-#ifndef MSLITE_ENABLE_GRAPH_KERNEL
+    //#ifndef MSLITE_ENABLE_GRAPH_KERNEL
     kernel_generator = "DVM";
-#endif
+    //#endif
   }
   if (kernel_generator == "DVM" && !has_enable_dynamic_shape_fusion) {
     enable_dynamic_shape_fusion = true;
diff --git a/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.h b/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.h
index d6bffe7c817..a61973fd4f1 100644
--- a/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.h
+++ b/mindspore/ccsrc/backend/common/graph_kernel/graph_kernel_flags.h
@@ -45,7 +45,7 @@ class BACKEND_COMMON_EXPORT GraphKernelFlags {
   // Dump all flags to json-format string
   std::string DumpAllFlags() const;
 
-#if defined(ENABLE_AKG) || defined(MSLITE_ENABLE_GRAPH_KERNEL)
+#if defined(ENABLE_AKG)
   // Check whether graph_kernel is enabled
   bool IsEnableGraphKernel() const { return opt_level > OptLevel_0; }
 #else
diff --git a/mindspore/ccsrc/backend/common/optimizer/helper.cc b/mindspore/ccsrc/backend/common/optimizer/helper.cc
index bf64279f85c..2828e2fb79d 100644
--- a/mindspore/ccsrc/backend/common/optimizer/helper.cc
+++ b/mindspore/ccsrc/backend/common/optimizer/helper.cc
@@ -765,65 +765,6 @@ AnfNodePtr CreateNodeBase(const FuncGraphPtr &graph, const std::vector<AnfNodePt
   return new_node;
 }
 
-bool AnfEqual(const BaseRef &a, const BaseRef &b) {
-  if (utils::isa<AnfNodePtr>(a) && utils::isa<AnfNodePtr>(b)) {
-    auto a_node = utils::cast<AnfNodePtr>(a);
-    auto b_node = utils::cast<AnfNodePtr>(b);
-    MS_EXCEPTION_IF_NULL(a_node);
-    MS_EXCEPTION_IF_NULL(b_node);
-    if (IsValueNode<Primitive>(a_node) && IsValueNode<Primitive>(b_node)) {
-      auto a_value_node = a_node->cast<ValueNodePtr>();
-      MS_EXCEPTION_IF_NULL(a_value_node);
-      auto a_value = a_value_node->value();
-      MS_EXCEPTION_IF_NULL(a_value);
-      auto a_prim = a_value->cast<PrimitivePtr>();
-      MS_EXCEPTION_IF_NULL(a_prim);
-
-      auto b_value_node = b_node->cast<ValueNodePtr>();
-      MS_EXCEPTION_IF_NULL(b_value_node);
-      auto b_value = b_value_node->value();
-      MS_EXCEPTION_IF_NULL(b_value);
-      auto b_prim = b_value->cast<PrimitivePtr>();
-      MS_EXCEPTION_IF_NULL(b_prim);
-
-      return a_prim->name() == b_prim->name();
-    } else if (a_node->isa<ValueNode>() && b_node->isa<ValueNode>()) {
-      auto a_value_node_ptr = a_node->cast<ValueNodePtr>();
-      if (a_value_node_ptr == nullptr) {
-        MS_LOG(INTERNAL_EXCEPTION) << "Cast value node ptr fail, node: " << a_node->DebugString();
-      }
-      auto a_value_ptr = a_value_node_ptr->value();
-      if (a_value_ptr == nullptr) {
-        MS_LOG(INTERNAL_EXCEPTION) << "Value ptr is nullptr, node: " << a_node->DebugString();
-      }
-
-      auto b_value_node_ptr = b_node->cast<ValueNodePtr>();
-      if (b_value_node_ptr == nullptr) {
-        MS_LOG(INTERNAL_EXCEPTION) << "Cast value node ptr fail, node: " << b_node->DebugString();
-      }
-      auto b_value_ptr = b_value_node_ptr->value();
-      if (b_value_ptr == nullptr) {
-        MS_LOG(INTERNAL_EXCEPTION) << "Value ptr is nullptr, node: " << b_node->DebugString();
-      }
-      if (a_value_ptr->isa<tensor::Tensor>() && b_value_ptr->isa<tensor::Tensor>()) {
-        auto a_tensor_ptr = a_value_ptr->cast<tensor::TensorPtr>();
-        auto b_tensor_ptr = b_value_ptr->cast<tensor::TensorPtr>();
-        if (a_tensor_ptr == nullptr || b_tensor_ptr == nullptr) {
-          MS_LOG(INTERNAL_EXCEPTION) << "Cast value node ptr fail.";
-        }
-        return a_tensor_ptr->ValueEqual(*b_tensor_ptr);
-      } else {
-        return (*a_value_ptr) == (*b_value_ptr);
-      }
-    }
-    MS_LOG(DEBUG) << "check AnfNodePtr equal";
-  }
-  if (utils::isa<FuncGraphPtr>(a) && utils::isa<FuncGraphPtr>(b)) {
-    MS_LOG(DEBUG) << "check GraphPtr equal";
-  }
-  return a == b;
-}
-
 bool CNodeTypeEqual(const BaseRef &a, const BaseRef &b) {
   // To matchCNode and Kernel's type
   if (utils::isa<CNode>(a) && utils::isa<CNode>(b)) {
diff --git a/mindspore/ccsrc/backend/common/optimizer/inplace_node_pass.cc b/mindspore/ccsrc/backend/common/optimizer/inplace_node_pass.cc
index d5d52bad5e7..66e25d37e15 100644
--- a/mindspore/ccsrc/backend/common/optimizer/inplace_node_pass.cc
+++ b/mindspore/ccsrc/backend/common/optimizer/inplace_node_pass.cc
@@ -15,7 +15,7 @@
  */
 
 #include "include/backend/optimizer/inplace_node_pass.h"
-#include "include/backend/optimizer/helper.h"
+#include "utils/anf_utils.h"
 
 namespace mindspore {
 namespace opt {
@@ -40,7 +40,7 @@ AnfNodePtr InplaceNodePass::Run(const FuncGraphPtr &, const AnfNodePtr &node) {
     for (size_t i = 0; i < inputs.size(); i++) {
       MS_EXCEPTION_IF_NULL(inputs[i]);
       MS_EXCEPTION_IF_NULL(pre_inputs[i]);
-      if (!opt::AnfEqual(inputs[i], pre_inputs[i])) {
+      if (!AnfUtils::AnfEqual(inputs[i], pre_inputs[i])) {
         MS_LOG_WITH_NODE(EXCEPTION, node)
           << "InplaceNodePass ERROR, the pass modify node: " << node->DebugString() << ", pass name: " << name()
           << ", before node " << i << ":" << inputs[i]->DebugString() << ", after node " << i << ":"
diff --git a/mindspore/ccsrc/backend/common/optimizer/pattern_engine.cc b/mindspore/ccsrc/backend/common/optimizer/pattern_engine.cc
index 3e0ab70388a..ffe9d19c6a5 100644
--- a/mindspore/ccsrc/backend/common/optimizer/pattern_engine.cc
+++ b/mindspore/ccsrc/backend/common/optimizer/pattern_engine.cc
@@ -18,6 +18,7 @@
 #include "ir/anf.h"
 #include "utils/convert_utils_base.h"
 #include "include/backend/optimizer/helper.h"
+#include "utils/anf_utils.h"
 
 namespace mindspore {
 static int GetNextTag() {
@@ -270,7 +271,7 @@ EquivPtr PatternEngine::Match(const BaseRef &pattern, const BaseRef &expr, const
   }
 
   // 2. check equal
-  if (opt::AnfEqual(pattern_ref, expr_ref)) {
+  if (AnfUtils::AnfEqual(pattern_ref, expr_ref)) {
     return equiv;
   }
 
diff --git a/mindspore/ccsrc/backend/common/optimizer/pattern_to_pattern.cc b/mindspore/ccsrc/backend/common/optimizer/pattern_to_pattern.cc
index 8fb8b66e61c..0ee3bbb32f8 100644
--- a/mindspore/ccsrc/backend/common/optimizer/pattern_to_pattern.cc
+++ b/mindspore/ccsrc/backend/common/optimizer/pattern_to_pattern.cc
@@ -75,7 +75,7 @@ bool PatternMap::Emplace(const std::string &name, const AnfNodePtr &node) {
   auto iter = node_map_.find(name);
   if (iter == node_map_.end()) {
     node_map_.emplace(name, node);
-  } else if (!opt::AnfEqual(node, iter->second)) {
+  } else if (!AnfUtils::AnfEqual(node, iter->second)) {
     MS_EXCEPTION_IF_NULL(iter->second);
     MS_LOG(INFO) << "The value of key: " << name
                  << " is not equal to origin value, value: " + node->fullname_with_scope()
@@ -110,7 +110,7 @@ bool PatternMap::Emplace(const std::string &name, const std::vector<AnfNodePtr>
     for (size_t i = 0; i < v.size(); i++) {
       MS_EXCEPTION_IF_NULL(v[i]);
       MS_EXCEPTION_IF_NULL(origin_v[i]);
-      if (!opt::AnfEqual(v[i], origin_v[i])) {
+      if (!AnfUtils::AnfEqual(v[i], origin_v[i])) {
         MS_LOG(INFO) << "The value of key: " << name
                      << " is not equal to origin value, value: " + v[i]->fullname_with_scope()
                      << " origin value: " << origin_v[i]->fullname_with_scope();
@@ -127,7 +127,9 @@ void PatternMap::Clear() {
   seq_map_.clear();
 }
 
-bool PatternMap::Check(const std::string &name, const AnfNodePtr &node) const { return opt::AnfEqual(node, Get(name)); }
+bool PatternMap::Check(const std::string &name, const AnfNodePtr &node) const {
+  return AnfUtils::AnfEqual(node, Get(name));
+}
 
 SrcPattern &SrcPattern::AddVar(const std::string &name, const PatternConditionFunc &f) {
   if (ref_map_.find(name) != ref_map_.end()) {
@@ -256,7 +258,7 @@ bool SrcPattern::match(const std::string &name, const AnfNodePtr &node, const Eq
       // prim
       MS_EXCEPTION_IF_NULL(pattern_node.p_);
       MS_EXCEPTION_IF_NULL(match_node);
-      if (!opt::AnfEqual(pattern_node.p_, match_node)) {
+      if (!AnfUtils::AnfEqual(pattern_node.p_, match_node)) {
         MS_LOG(EXCEPTION) << "The value of Primitive is not equal to matched value, pattern value: " +
                                pattern_node.p_->ToString()
                           << " matched value: " + match_node->ToString() + ", node name: " + name;
@@ -367,7 +369,7 @@ DstPattern &DstPattern::AddCNode(const string &name, const std::initializer_list
     for (size_t i = 0; i < anf_inputs.size(); i++) {
       MS_EXCEPTION_IF_NULL(anf_inputs[i]);
       MS_EXCEPTION_IF_NULL(cnode->input(i));
-      if (!opt::AnfEqual(anf_inputs[i], cnode->input(i))) {
+      if (!AnfUtils::AnfEqual(anf_inputs[i], cnode->input(i))) {
         MS_LOG(INTERNAL_EXCEPTION)
           << "The actual input does not correspond to the input of the pattern, the input index: " << i
           << ", actual input: " << anf_inputs[i]->DebugString()
diff --git a/mindspore/ccsrc/backend/common/pass/custom_defined_depend.cc b/mindspore/ccsrc/backend/common/pass/custom_defined_depend.cc
index 1e26f3dfe5f..a5a312b2a55 100644
--- a/mindspore/ccsrc/backend/common/pass/custom_defined_depend.cc
+++ b/mindspore/ccsrc/backend/common/pass/custom_defined_depend.cc
@@ -58,13 +58,13 @@ bool FileExists(const string &filename) {
 
 std::string GetRankID() {
   uint32_t rank_id = 0;
-#if !defined(BUILD_LITE)
+//#if !defined(BUILD_LITE)
   if (distributed::collective::CollectiveManager::instance()->initialized()) {
     rank_id = CommManager::GetInstance().GetRank();
   } else {
     rank_id = MsContext::GetInstance()->get_param<uint32_t>(MS_CTX_DEVICE_ID);
   }
-#endif
+//#endif
   return std::to_string(rank_id);
 }
 
diff --git a/mindspore/ccsrc/backend/common/pass/other/add_attr_to_dump.cc b/mindspore/ccsrc/backend/common/pass/other/add_attr_to_dump.cc
index afe0eeac9bf..a617e5d073c 100644
--- a/mindspore/ccsrc/backend/common/pass/other/add_attr_to_dump.cc
+++ b/mindspore/ccsrc/backend/common/pass/other/add_attr_to_dump.cc
@@ -69,7 +69,6 @@ const AnfNodePtr AddAttrToDump::Process(const FuncGraphPtr &func_graph, const An
   auto primitive = GetCNodePrimitive(cnode);
   MS_EXCEPTION_IF_NULL(primitive);
 
-#if !defined(BUILD_LITE)
   if (common::GetDumpSliceSize() > 0) {
     constexpr int64_t kMegaBytes = 1LL << 20;
     int64_t slice_size_in_bytes = common::GetDumpSliceSize() * kMegaBytes;
@@ -77,7 +76,6 @@ const AnfNodePtr AddAttrToDump::Process(const FuncGraphPtr &func_graph, const An
     (void)primitive->AddAttr("wait_time", MakeValue<int>(common::GetDumpWaitTime()));
     (void)primitive->AddAttr("slice_sync", MakeValue<bool>(true));
   }
-#endif
 
   return cnode;
 }
diff --git a/mindspore/ccsrc/backend/ge_backend/CMakeLists.txt b/mindspore/ccsrc/backend/ge_backend/CMakeLists.txt
index d88d0073397..341fe5d59c2 100644
--- a/mindspore/ccsrc/backend/ge_backend/CMakeLists.txt
+++ b/mindspore/ccsrc/backend/ge_backend/CMakeLists.txt
@@ -7,7 +7,7 @@ if(ENABLE_D OR ENABLE_ACL)
     list(APPEND _GE_BACKEND_SRC_LIST $<TARGET_OBJECTS:_mindspore_ge_utils_obj>)
     list(APPEND _GE_BACKEND_SRC_LIST $<TARGET_OBJECTS:_mindspore_ge_pass_obj>)
     list(APPEND _GE_BACKEND_SRC_LIST $<TARGET_OBJECTS:_mindspore_ge_executor_obj>)
-
+    list(APPEND _GE_BACKEND_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/graph_ir/callbacks_ge.cc")
 
     set_property(SOURCE ${_GE_BACKEND_SRC_LIST}
           PROPERTY COMPILE_DEFINITIONS SUBMODULE_ID=mindspore::SubModuleId::SM_RUNTIME_FRAMEWORK)
diff --git a/mindspore/ccsrc/backend/ge_backend/graph_ir/CMakeLists.txt b/mindspore/ccsrc/backend/ge_backend/graph_ir/CMakeLists.txt
index 9ab80751d81..006c794587e 100644
--- a/mindspore/ccsrc/backend/ge_backend/graph_ir/CMakeLists.txt
+++ b/mindspore/ccsrc/backend/ge_backend/graph_ir/CMakeLists.txt
@@ -1,27 +1,54 @@
 if(ENABLE_D OR ENABLE_ACL)
     file(GLOB_RECURSE _TRANSFORM_SRC_LIST RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "*.cc")
-    if(BUILD_LITE)
-        list(REMOVE_ITEM _TRANSFORM_SRC_LIST "callbacks_ge.cc")
-    endif()
     set_property(SOURCE ${_TRANSFORM_SRC_LIST} PROPERTY COMPILE_DEFINITIONS
       SUBMODULE_ID=mindspore::SubModuleId::SM_GE_ADPT)
 
-      # mindspore_graph_ir is used by GE and lite.
-    if(BUILD_LITE)
+    # mindspore_graph_ir is used by GE and lite.
+    if(TARGET mindspore_ascend_res_manager)
+        add_library(mindspore_graph_ir SHARED ${_TRANSFORM_SRC_LIST})
+        target_link_libraries(mindspore_graph_ir PRIVATE mindspore_ascend_res_manager)
+    else()
+        file(STRINGS "${CMAKE_CURRENT_SOURCE_DIR}/../../../../../version.txt" VERSION)
+        add_definitions(-DVERSION="${VERSION}")
+        list(REMOVE_ITEM _TRANSFORM_SRC_LIST "callbacks_ge.cc")
         add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/../../../plugin/res_manager/ascend/op_adapter/
             _mindspore_ascend_op_adapter_obj)
         add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/../../../plugin/res_manager/ascend/symbol_interface
             _mindspore_ascend_symbol_obj)
         list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/config_manager.cc")
         list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../common/debug/common.cc")
+
+        list(APPEND _TRANSFORM_SRC_LIST
+            "${CMAKE_CURRENT_SOURCE_DIR}/../../../backend/common/optimizer/graph_optimizer.cc")
+        list(APPEND _TRANSFORM_SRC_LIST
+            "${CMAKE_CURRENT_SOURCE_DIR}/../../../backend/common/optimizer/pattern_engine.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../backend/common/optimizer/visitor.cc")
+
+        if(NOT WIN32)
+            list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/anfalgo.cc")
+            list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/utils.cc")
+            list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/parallel_context.cc")
+        endif()
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/convert_utils.cc")
+
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../kernel/kernel_info.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/comm_manager.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/compile_cache_context.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../common/debug/mindir_exporter.cc")
+
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../../ops/kernel/common/format_utils.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../../ops/kernel/common/kernel_factory.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../../ops/kernel/common/kernel_tensor.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../../ops/kernel/common/device_address.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../../ops/kernel/common/kernel.cc")
+        list(APPEND _TRANSFORM_SRC_LIST
+            "${CMAKE_CURRENT_SOURCE_DIR}/../../../../ops/kernel/common/kernel_build_info.cc")
+        list(APPEND _TRANSFORM_SRC_LIST "${CMAKE_CURRENT_SOURCE_DIR}/../../../minddata/dataset/core/types.cc")
         add_library(mindspore_graph_ir SHARED ${_TRANSFORM_SRC_LIST} $<TARGET_OBJECTS:_mindspore_ascend_symbol_obj>
             $<TARGET_OBJECTS:_mindspore_ascend_op_adapter_obj>)
-    else()
-        add_library(mindspore_graph_ir SHARED ${_TRANSFORM_SRC_LIST})
-        target_link_libraries(mindspore_graph_ir PRIVATE mindspore_ascend_res_manager)
     endif()
 
-    target_link_libraries(mindspore_graph_ir PRIVATE mindspore_core mindspore_ops)
+    target_link_libraries(mindspore_graph_ir PRIVATE mindspore_core mindspore_ops mindspore::protobuf)
     find_library(ACL ascendcl ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
     find_library(GE_RUNNER ge_runner ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
     find_library(GRAPH graph ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
diff --git a/mindspore/ccsrc/backend/ge_backend/runtime/actor/actor_common.cc b/mindspore/ccsrc/backend/ge_backend/runtime/actor/actor_common.cc
index b77d1e04574..e693f4c8cc6 100644
--- a/mindspore/ccsrc/backend/ge_backend/runtime/actor/actor_common.cc
+++ b/mindspore/ccsrc/backend/ge_backend/runtime/actor/actor_common.cc
@@ -25,10 +25,8 @@
 #include "utils/ms_utils.h"
 #include "include/common/utils/anfalgo.h"
 #include "include/backend/mem_reuse/mem_tracker.h"
-#ifndef BUILD_LITE
 #include "backend/ge_backend/runtime/actor/memory_manager_actor.h"
 #include "backend/ge_backend/utils/device_address_utils.h"
-#endif
 #include "runtime/device/res_manager/hal_res_manager.h"
 
 namespace mindspore {
diff --git a/mindspore/ccsrc/cmake/ascend_compile_config.cmake b/mindspore/ccsrc/cmake/ascend_compile_config.cmake
index d72efcde036..cfb774ee751 100644
--- a/mindspore/ccsrc/cmake/ascend_compile_config.cmake
+++ b/mindspore/ccsrc/cmake/ascend_compile_config.cmake
@@ -1,9 +1,9 @@
 include(${CMAKE_SOURCE_DIR}/cmake/graphengine_variables.cmake)
 include_directories(${CMAKE_CURRENT_SOURCE_DIR}/plugin/device/ascend)
-add_subdirectory(backend/ge_backend)
 add_subdirectory(plugin/res_manager/ascend)
 add_subdirectory(plugin/res_manager/ascend/collective)
 add_subdirectory(plugin/device/ascend)
+add_subdirectory(backend/ge_backend)
 enable_directory_when_only_build_plugins(plugin/device/ascend)
 enable_directory_when_only_build_plugins(plugin/res_manager/ascend/collective)
 enable_directory_when_only_build_plugins(plugin/res_manager/ascend/hccl_adapter/plugin)
diff --git a/mindspore/ccsrc/debug/execute_order_tracker/execute_order_tracker.cc b/mindspore/ccsrc/debug/execute_order_tracker/execute_order_tracker.cc
index 40487dbd566..89a298f7c1c 100644
--- a/mindspore/ccsrc/debug/execute_order_tracker/execute_order_tracker.cc
+++ b/mindspore/ccsrc/debug/execute_order_tracker/execute_order_tracker.cc
@@ -237,18 +237,12 @@ std::vector<uint32_t> ExecuteOrderTracker::GetCommRanks(const std::string &group
   if (group_name == "hccl_world_group") {
     uint32_t rank_size = 1;
 
-#if !defined(BUILD_LITE)
     rank_size = distributed::collective::CollectiveManager::instance()->global_rank_size();
-#endif
 
     comm_ranks.resize(rank_size);
     std::iota(comm_ranks.begin(), comm_ranks.end(), 0);
   } else {
-#if !defined(BUILD_LITE)
     comm_ranks = distributed::collective::CollectiveManager::instance()->GetGroupRanks(group_name);
-#else
-    comm_ranks = {0};
-#endif
   }
 
   comm_ranks_cache_[group_name] = comm_ranks;
diff --git a/mindspore/ccsrc/include/backend/data_queue/data_queue_mgr.h b/mindspore/ccsrc/include/backend/data_queue/data_queue_mgr.h
index 3f3f1055983..1ca2cc21296 100644
--- a/mindspore/ccsrc/include/backend/data_queue/data_queue_mgr.h
+++ b/mindspore/ccsrc/include/backend/data_queue/data_queue_mgr.h
@@ -28,10 +28,8 @@
 #include "utils/callback_handler.h"
 #include "include/backend/visible.h"
 #include "include/backend/data_queue/data_queue.h"
-#ifndef BUILD_LITE
 #include "ir/anf.h"
 #include "common/kernel.h"
-#endif
 
 namespace mindspore {
 namespace device {
@@ -143,7 +141,6 @@ class BACKEND_EXPORT DataQueueMgr {
   HANDLER_DEFINE(bool, CleanTdtHandle);
   HANDLER_DEFINE(bool, DestoryTdtHandle);
 };
-#ifndef BUILD_LITE
 BACKEND_EXPORT void UpdateGetNextNode(const AnfNodePtr &data_kernel);
 
 BACKEND_EXPORT void UpdateGetNextNode(const PrimitivePtr &primitive, const std::vector<kernel::KernelTensor *> &inputs,
@@ -161,7 +158,6 @@ BACKEND_EXPORT void UpdateGetNextWithDataQueueItems(const std::vector<kernel::Ke
 BACKEND_EXPORT void RetryPeakItemFromDataQueue(const AnfNodePtr &data_kernel,
                                                const std::shared_ptr<BlockingQueue> &data_queue,
                                                std::vector<device::DataQueueItem> *data);
-#endif
 #define REGISTER_DATA_QUEUE_CREATOR(device_name, creator)                         \
   struct device_name##DataQueueCreatorClass {                                     \
     device_name##DataQueueCreatorClass() {                                        \
diff --git a/mindspore/ccsrc/include/backend/optimizer/helper.h b/mindspore/ccsrc/include/backend/optimizer/helper.h
index 554d2c655bc..39f03561237 100644
--- a/mindspore/ccsrc/include/backend/optimizer/helper.h
+++ b/mindspore/ccsrc/include/backend/optimizer/helper.h
@@ -218,8 +218,6 @@ BACKEND_COMMON_EXPORT std::shared_ptr<std::vector<std::pair<AnfNodePtr, int>>> G
   const FuncGraphPtr &graph, const AnfNodePtr &node, size_t output_index);
 bool IsNotRealUsedByOthers(const FuncGraphPtr &graph, const AnfNodePtr &node);
 
-bool AnfEqual(const BaseRef &a, const BaseRef &b);
-
 bool CNodeTypeEqual(const BaseRef &a, const BaseRef &b);
 
 AnfNodePtr SexpToNode(const BaseRef &sexp, const BaseRef &graph, PrimitiveVarMap *primitive_vars,
diff --git a/mindspore/ccsrc/include/common/thread_pool.h b/mindspore/ccsrc/include/common/thread_pool.h
index 86d96e6d620..f9fb681e74c 100644
--- a/mindspore/ccsrc/include/common/thread_pool.h
+++ b/mindspore/ccsrc/include/common/thread_pool.h
@@ -31,9 +31,6 @@
 #include "utils/log_adapter.h"
 #include "include/common/visible.h"
 
-#ifdef PARALLEL_INFERENCE
-#define BIND_CORE
-#endif
 #ifdef __ANDROID__
 #define BIND_CORE
 #include <sched.h>
diff --git a/mindspore/ccsrc/kernel/graph_kernel/graph_kernel_json_generator.cc b/mindspore/ccsrc/kernel/graph_kernel/graph_kernel_json_generator.cc
index d98e0d2678e..97e0c29efd0 100644
--- a/mindspore/ccsrc/kernel/graph_kernel/graph_kernel_json_generator.cc
+++ b/mindspore/ccsrc/kernel/graph_kernel/graph_kernel_json_generator.cc
@@ -29,14 +29,14 @@
 #include "backend/common/graph_kernel/graph_kernel_flags.h"
 #include "kernel/graph_kernel/graph_kernel_json_flags.h"
 #include "include/common/symbol_engine/symbol_engine_impl.h"
-#ifdef MSLITE_ENABLE_GRAPH_KERNEL
-#ifdef ENABLE_GPU
-#include <cuda.h>
-#endif
-#else
+//#ifdef MSLITE_ENABLE_GRAPH_KERNEL
+//#ifdef ENABLE_GPU
+//#include <cuda.h>
+//#endif
+//#else
 #include "common/oplib/oplib.h"
 #include "runtime/hardware/device_context_manager.h"
-#endif
+//#endif
 #include "mindspore/ops/op_def/auto_generate/gen_ops_primitive_c.h"
 
 namespace mindspore::graphkernel {
@@ -773,9 +773,9 @@ OpInfoPtr GraphKernelJsonGenerator::ExtractOpInfo(const AnfNodePtr &anf_node) co
     OpInfoExtractor e;
     return e.Run(anf_node);
   } else {
-#ifdef MSLITE_ENABLE_GRAPH_KERNEL
-    MS_LOG(EXCEPTION) << "OpLib is not supported.";
-#else
+    //#ifdef MSLITE_ENABLE_GRAPH_KERNEL
+    //    MS_LOG(EXCEPTION) << "OpLib is not supported.";
+    //#else
     OpImplyType imply_type;
     const auto &flags = GraphKernelFlags::GetInstance();
 
@@ -785,7 +785,7 @@ OpInfoPtr GraphKernelJsonGenerator::ExtractOpInfo(const AnfNodePtr &anf_node) co
       imply_type = OpImplyType::kImplyAKG;
     }
     return kernel::OpLib::FindOp(AnfUtils::GetCNodeName(anf_node), imply_type);
-#endif
+    //#endif
   }
 }
 
@@ -1385,45 +1385,45 @@ void GetCpuInfo(nlohmann::json *target_info) {
   return;
 }
 
-#ifdef MSLITE_ENABLE_GRAPH_KERNEL
-#ifdef ENABLE_GPU
-bool GetGpuInfo(nlohmann::json *target_info) {
-  int major_version = -1;
-  auto ret = cuDeviceGetAttribute(&major_version, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, 0);
-  if (ret != CUDA_SUCCESS) {
-    const char *msg = nullptr;
-    cuGetErrorName(ret, &msg);
-    MS_LOG(WARNING) << "Get CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR fail, error message: " << msg;
-    return false;
-  }
-  int minor_version = -1;
-  auto ret = cuDeviceGetAttribute(&minor_version, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, 0);
-  if (ret != CUDA_SUCCESS) {
-    const char *msg = nullptr;
-    cuGetErrorName(ret, &msg);
-    MS_LOG(WARNING) << "Get CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR fail, error message: " << msg;
-    return false;
-  }
-  int sm_count = -1;
-  auto ret = cuDeviceGetAttribute(&sm_count, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, 0);
-  if (ret != CUDA_SUCCESS) {
-    const char *msg = nullptr;
-    cuGetErrorName(ret, &msg);
-    MS_LOG(WARNING) << "Get CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT fail, error message: " << msg;
-    return false;
-  }
-  if (major_version == -1 || minor_version == -1 || sm_count == -1) {
-    return false;
-  } else {
-    (*target_info)[kJsonKeyComputeCapability] = std::to_string(major_version) + "." + std::to_string(minor_version);
-    (*target_info)[kJsonKeySmCount] = sm_count;
-  }
-  return true;
-}
-#else
-bool GetGpuInfo(nlohmann::json *target_info) { return false; }
-#endif
-#else
+//#ifdef MSLITE_ENABLE_GRAPH_KERNEL
+//#ifdef ENABLE_GPU
+// bool GetGpuInfo(nlohmann::json *target_info) {
+//  int major_version = -1;
+//  auto ret = cuDeviceGetAttribute(&major_version, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, 0);
+//  if (ret != CUDA_SUCCESS) {
+//    const char *msg = nullptr;
+//    cuGetErrorName(ret, &msg);
+//    MS_LOG(WARNING) << "Get CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR fail, error message: " << msg;
+//    return false;
+//  }
+//  int minor_version = -1;
+//  auto ret = cuDeviceGetAttribute(&minor_version, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, 0);
+//  if (ret != CUDA_SUCCESS) {
+//    const char *msg = nullptr;
+//    cuGetErrorName(ret, &msg);
+//    MS_LOG(WARNING) << "Get CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR fail, error message: " << msg;
+//    return false;
+//  }
+//  int sm_count = -1;
+//  auto ret = cuDeviceGetAttribute(&sm_count, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, 0);
+//  if (ret != CUDA_SUCCESS) {
+//    const char *msg = nullptr;
+//    cuGetErrorName(ret, &msg);
+//    MS_LOG(WARNING) << "Get CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT fail, error message: " << msg;
+//    return false;
+//  }
+//  if (major_version == -1 || minor_version == -1 || sm_count == -1) {
+//    return false;
+//  } else {
+//    (*target_info)[kJsonKeyComputeCapability] = std::to_string(major_version) + "." + std::to_string(minor_version);
+//    (*target_info)[kJsonKeySmCount] = sm_count;
+//  }
+//  return true;
+//}
+//#else
+// bool GetGpuInfo(nlohmann::json *target_info) { return false; }
+//#endif
+//#else
 bool GetGpuInfo(nlohmann::json *target_info) {
   const auto &device_context = device::DeviceContextManager::GetInstance().GetOrCreateDeviceContext(
     {kGPUDevice, MsContext::GetInstance()->get_param<uint32_t>(MS_CTX_DEVICE_ID)});
@@ -1441,7 +1441,7 @@ bool GetGpuInfo(nlohmann::json *target_info) {
   }
   return true;
 }
-#endif
+//#endif
 }  // namespace
 
 void TargetInfoSetter::GetTargetInfo() {
diff --git a/mindspore/ccsrc/minddata/dataset/api/vision.cc b/mindspore/ccsrc/minddata/dataset/api/vision.cc
index 435128d3eeb..034bc9422c0 100644
--- a/mindspore/ccsrc/minddata/dataset/api/vision.cc
+++ b/mindspore/ccsrc/minddata/dataset/api/vision.cc
@@ -93,12 +93,6 @@
 #include "minddata/dataset/kernels/ir/vision/vertical_flip_ir.h"
 #include "minddata/dataset/util/log_adapter.h"
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include "minddata/dataset/kernels/ir/vision/pad_ir.h"
-#include "minddata/dataset/kernels/ir/vision/rescale_ir.h"
-#include "minddata/dataset/kernels/ir/vision/swap_red_blue_ir.h"
-#endif
-
 #include "minddata/dataset/kernels/image/image_utils.h"
 #if defined(ENABLE_FFMPEG)
 #include "minddata/dataset/kernels/image/video_utils.h"
@@ -650,12 +644,8 @@ Pad::Pad(const std::vector<int32_t> &padding, const std::vector<uint8_t> &fill_v
     : data_(std::make_shared<Data>(padding, fill_value, padding_mode)) {}
 
 std::shared_ptr<TensorOperation> Pad::Parse() {
-#if defined(ENABLE_MINDDATA_PYTHON)
-  return std::make_shared<PadOperation>(data_->padding_, data_->fill_value_, data_->padding_mode_);
-#else
   MS_LOG(ERROR) << "Unsupported Pad.";
   return nullptr;
-#endif
 }
 
 // PadToSize Transform Operation.
@@ -1239,12 +1229,8 @@ struct Rescale::Data {
 Rescale::Rescale(float rescale, float shift) : data_(std::make_shared<Data>(rescale, shift)) {}
 
 std::shared_ptr<TensorOperation> Rescale::Parse() {
-#if defined(ENABLE_MINDDATA_PYTHON)
-  return std::make_shared<RescaleOperation>(data_->rescale_, data_->shift_);
-#else
   MS_LOG(ERROR) << "Unsupported Rescale.";
   return nullptr;
-#endif
 }
 
 // Resize Transform Operation.
@@ -1410,12 +1396,8 @@ std::shared_ptr<TensorOperation> Solarize::Parse() { return std::make_shared<Sol
 // SwapRedBlue Transform Operation.
 SwapRedBlue::SwapRedBlue() = default;
 std::shared_ptr<TensorOperation> SwapRedBlue::Parse() {
-#if defined(ENABLE_MINDDATA_PYTHON)
-  return std::make_shared<SwapRedBlueOperation>();
-#else
   MS_LOG(ERROR) << "Unsupported SwapRedBlue.";
   return nullptr;
-#endif
 }
 
 // ToTensor Transform Operation.
diff --git a/mindspore/ccsrc/minddata/dataset/core/CMakeLists.txt b/mindspore/ccsrc/minddata/dataset/core/CMakeLists.txt
index d4de59f7a63..7c54cdb769e 100644
--- a/mindspore/ccsrc/minddata/dataset/core/CMakeLists.txt
+++ b/mindspore/ccsrc/minddata/dataset/core/CMakeLists.txt
@@ -33,12 +33,12 @@ if(ENABLE_D)
         )
 endif()
 
-if(NOT MSLITE_ENABLE_ACL)
-    set(DATASET_CORE_SRC_FILES
-        ${DATASET_CORE_SRC_FILES}
-        types.cc # in lite, src code has types.cc impl
-        )
-endif()
+#if(NOT MSLITE_ENABLE_ACL)
+#    set(DATASET_CORE_SRC_FILES
+#        ${DATASET_CORE_SRC_FILES}
+#        types.cc # in lite, src code has types.cc impl
+#        )
+#endif()
 
 ms_protobuf_generate(EXAMPLE_SRCS EXAMPLE_HDRS example.proto)
 ms_protobuf_generate(FEATURE_SRCS FEATURE_HDRS feature.proto)
diff --git a/mindspore/ccsrc/minddata/dataset/core/data_type.cc b/mindspore/ccsrc/minddata/dataset/core/data_type.cc
index 79ef80ab93b..98a72ca2abd 100644
--- a/mindspore/ccsrc/minddata/dataset/core/data_type.cc
+++ b/mindspore/ccsrc/minddata/dataset/core/data_type.cc
@@ -14,10 +14,6 @@
  * limitations under the License.
  */
 #include "minddata/dataset/core/data_type.h"
-#ifdef ENABLE_MINDDATA_PYTHON
-#include "minddata/dataset/core/pybind_support.h"
-#endif
-
 #include "minddata/dataset/util/log_adapter.h"
 
 namespace mindspore {
@@ -30,64 +26,6 @@ uint8_t DataType::SizeInBytes() const {
   }
 }
 
-#ifdef ENABLE_MINDDATA_PYTHON
-py::dtype DataType::AsNumpyType() const {
-  if (type_ < DataType::NUM_OF_TYPES) {
-    return py::dtype(kTypeInfo[type_].pybindType_);
-  } else {
-    return py::dtype("unknown");
-  }
-}
-#endif
-
-#if defined(ENABLE_MINDDATA_PYTHON)
-uint8_t DataType::AsCVType() const {
-  uint8_t res = kCVInvalidType;
-  if (type_ < DataType::NUM_OF_TYPES) {
-    res = kTypeInfo[type_].cvType_;
-  }
-
-  if (res == kCVInvalidType) {
-    std::string type_name = "unknown";
-    if (type_ < DataType::NUM_OF_TYPES) {
-      type_name = std::string(kTypeInfo[type_].name_);
-    }
-    std::string err_msg = "Cannot convert [" + type_name + "] to OpenCV type.";
-    err_msg += " Currently unsupported data type: [uint32, int64, uint64, string]";
-    MS_LOG(ERROR) << err_msg;
-  }
-
-  return res;
-}
-
-DataType DataType::FromCVType(int cv_type) {
-  auto depth = static_cast<uchar>(cv_type) & static_cast<uchar>(CV_MAT_DEPTH_MASK);
-  switch (depth) {
-    case CV_8S:
-      return DataType(DataType::DE_INT8);
-    case CV_8U:
-      return DataType(DataType::DE_UINT8);
-    case CV_16S:
-      return DataType(DataType::DE_INT16);
-    case CV_16U:
-      return DataType(DataType::DE_UINT16);
-    case CV_32S:
-      return DataType(DataType::DE_INT32);
-    case CV_16F:
-      return DataType(DataType::DE_FLOAT16);
-    case CV_32F:
-      return DataType(DataType::DE_FLOAT32);
-    case CV_64F:
-      return DataType(DataType::DE_FLOAT64);
-    default:
-      std::string err_msg = "Cannot convert from OpenCV type, unknown CV type.";
-      err_msg += " Currently supported data type: [int8, uint8, int16, uint16, int32, float16, float32, float64]";
-      MS_LOG(ERROR) << err_msg;
-      return DataType(DataType::DE_UNKNOWN);
-  }
-}
-#endif
-
 DataType::DataType(const std::string &type_str) {
   if (type_str == "bool") {
     type_ = DE_BOOL;
@@ -117,10 +55,6 @@ DataType::DataType(const std::string &type_str) {
     type_ = DE_STRING;
   } else if (type_str == "bytes") {
     type_ = DE_BYTES;
-#ifdef ENABLE_MINDDATA_PYTHON
-  } else if (type_str == "python") {
-    type_ = DE_PYTHON;
-#endif
   } else {
     type_ = DE_UNKNOWN;
   }
@@ -133,61 +67,5 @@ std::string DataType::ToString() const {
     return "unknown";
   }
 }
-
-#ifdef ENABLE_MINDDATA_PYTHON
-DataType DataType::FromNpArray(const py::array &arr) {
-  if (py::isinstance<py::array_t<bool>>(arr)) {
-    return DataType(DataType::DE_BOOL);
-  } else if (py::isinstance<py::array_t<std::int8_t>>(arr)) {
-    return DataType(DataType::DE_INT8);
-  } else if (py::isinstance<py::array_t<std::uint8_t>>(arr)) {
-    return DataType(DataType::DE_UINT8);
-  } else if (py::isinstance<py::array_t<std::int16_t>>(arr)) {
-    return DataType(DataType::DE_INT16);
-  } else if (py::isinstance<py::array_t<std::uint16_t>>(arr)) {
-    return DataType(DataType::DE_UINT16);
-  } else if (py::isinstance<py::array_t<std::int32_t>>(arr)) {
-    return DataType(DataType::DE_INT32);
-  } else if (py::isinstance<py::array_t<std::uint32_t>>(arr)) {
-    return DataType(DataType::DE_UINT32);
-  } else if (py::isinstance<py::array_t<std::int64_t>>(arr)) {
-    return DataType(DataType::DE_INT64);
-  } else if (py::isinstance<py::array_t<std::uint64_t>>(arr)) {
-    return DataType(DataType::DE_UINT64);
-  } else if (py::isinstance<py::array_t<float16>>(arr)) {
-    return DataType(DataType::DE_FLOAT16);
-  } else if (py::isinstance<py::array_t<std::float_t>>(arr)) {
-    return DataType(DataType::DE_FLOAT32);
-  } else if (py::isinstance<py::array_t<std::double_t>>(arr)) {
-    return DataType(DataType::DE_FLOAT64);
-  } else if (arr.dtype().kind() == 'U') {
-    return DataType(DataType::DE_STRING);
-  } else if (arr.dtype().kind() == 'S') {
-    return DataType(DataType::DE_BYTES);
-  } else {
-    if (arr.size() == 0) {
-      MS_LOG(ERROR) << "Please check input data, the data of numpy array is empty.";
-    }
-    std::string err_msg = "Cannot convert from numpy type. Unknown data type is returned!";
-    err_msg +=
-      " Currently supported data type: [int8, uint8, int16, uint16, int32, uint32, int64, uint64, float16, float32, "
-      "float64, string, bytes]";
-    MS_LOG(ERROR) << err_msg;
-    return DataType(DataType::DE_UNKNOWN);
-  }
-}
-
-std::string DataType::GetPybindFormat() const {
-  std::string res;
-  if (type_ < DataType::NUM_OF_TYPES) {
-    res = kTypeInfo[type_].pybindFormatDescriptor_;
-  }
-
-  if (res.empty()) {
-    MS_LOG(ERROR) << "Cannot convert from data type to pybind format descriptor!";
-  }
-  return res;
-}
-#endif
 }  // namespace dataset
 }  // namespace mindspore
diff --git a/mindspore/ccsrc/minddata/dataset/core/data_type.h b/mindspore/ccsrc/minddata/dataset/core/data_type.h
index 1bfa80cbe10..22450e42e2d 100644
--- a/mindspore/ccsrc/minddata/dataset/core/data_type.h
+++ b/mindspore/ccsrc/minddata/dataset/core/data_type.h
@@ -16,22 +16,10 @@
 #ifndef MINDSPORE_CCSRC_MINDDATA_DATASET_CORE_DATA_TYPE_H_
 #define MINDSPORE_CCSRC_MINDDATA_DATASET_CORE_DATA_TYPE_H_
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include <opencv2/core/hal/interface.h>
-#endif
-
 #include <string>
 #include <utility>
-
-#ifdef ENABLE_MINDDATA_PYTHON
-#include "pybind11/numpy.h"
-#include "pybind11/pybind11.h"
-#include "minddata/dataset/core/pybind_support.h"
-namespace py = pybind11;
-#else
 #include "base/bfloat16.h"
 #include "base/float16.h"
-#endif
 #include "minddata/dataset/include/dataset/constants.h"
 
 namespace mindspore {
@@ -67,48 +55,6 @@ class DataType {
     const uint8_t cvType_;                      // OpenCv matching type
   };
 
-#ifdef ENABLE_MINDDATA_PYTHON
-  static inline const TypeInfo kTypeInfo[] = {
-    // name, sizeInBytes, pybindType, pybindFormatDescriptor, openCV
-    {"unknown", 0, "object", "", kCVInvalidType},                                        // DE_UNKNOWN
-    {"bool", 1, "bool", py::format_descriptor<bool>::format(), CV_8U},                   // DE_BOOL
-    {"int8", 1, "int8", py::format_descriptor<int8_t>::format(), CV_8S},                 // DE_INT8
-    {"uint8", 1, "uint8", py::format_descriptor<uint8_t>::format(), CV_8U},              // DE_UINT8
-    {"int16", 2, "int16", py::format_descriptor<int16_t>::format(), CV_16S},             // DE_INT16
-    {"uint16", 2, "uint16", py::format_descriptor<uint16_t>::format(), CV_16U},          // DE_UINT16
-    {"int32", 4, "int32", py::format_descriptor<int32_t>::format(), CV_32S},             // DE_INT32
-    {"uint32", 4, "uint32", py::format_descriptor<uint32_t>::format(), kCVInvalidType},  // DE_UINT32
-    {"int64", 8, "int64", py::format_descriptor<int64_t>::format(), kCVInvalidType},     // DE_INT64
-    {"uint64", 8, "uint64", py::format_descriptor<uint64_t>::format(), kCVInvalidType},  // DE_UINT64
-    {"float16", 2, "float16", "e", CV_16F},                                              // DE_FLOAT16
-    {"float32", 4, "float32", py::format_descriptor<float>::format(), CV_32F},           // DE_FLOAT32
-    {"float64", 8, "double", py::format_descriptor<double>::format(), CV_64F},           // DE_FLOAT64
-    {"string", 0, "str", "U", kCVInvalidType},                                           // DE_STRING
-    {"bytes", 0, "bytes", "S", CV_8U},                                                   // DE_BYTES
-    {"python", 0, "object", "O", kCVInvalidType}                                         // DE_PYTHON
-  };
-#else
-#if defined(ENABLE_MINDDATA_PYTHON)
-  static inline const TypeInfo kTypeInfo[] = {
-    // name, sizeInBytes, pybindTypem formatDescriptor, openCV
-    {"unknown", 0, "object", "", kCVInvalidType},  // DE_UNKNOWN
-    {"bool", 1, "bool", "", CV_8U},                // DE_BOOL
-    {"int8", 1, "int8", "", CV_8S},                // DE_INT8
-    {"uint8", 1, "uint8", "", CV_8U},              // DE_UINT8
-    {"int16", 2, "int16", "", CV_16S},             // DE_INT16
-    {"uint16", 2, "uint16", "", CV_16U},           // DE_UINT16
-    {"int32", 4, "int32", "", CV_32S},             // DE_INT32
-    {"uint32", 4, "uint32", "", kCVInvalidType},   // DE_UINT32
-    {"int64", 8, "int64", "", kCVInvalidType},     // DE_INT64
-    {"uint64", 8, "uint64", "", kCVInvalidType},   // DE_UINT64
-    {"float16", 2, "float16", "", CV_16F},         // DE_FLOAT16
-    {"float32", 4, "float32", "", CV_32F},         // DE_FLOAT32
-    {"float64", 8, "double", "", CV_64F},          // DE_FLOAT64
-    {"string", 0, "str", "", kCVInvalidType},      // DE_STRING
-    {"bytes", 0, "bytes", "", CV_8U},              // DE_BYTES
-    {"python", 0, "object", "O", kCVInvalidType}   // DE_PYTHON
-  };
-#else
   // android and no python
   static inline const TypeInfo kTypeInfo[] = {
     // name, sizeInBytes, formatDescriptor
@@ -129,8 +75,6 @@ class DataType {
     {"bytes", 0, "bytes", ""},                     // DE_BYTES
     {"python", 0, "object", "O", kCVInvalidType}   // DE_PYTHON
   };
-#endif
-#endif
   // No arg constructor to create an unknown shape
   DataType() : type_(DE_UNKNOWN) {}
 
@@ -165,17 +109,6 @@ class DataType {
   /// \return the number of bytes of the type.
   uint8_t SizeInBytes() const;
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-  // Convert from DataType to OpenCV type
-  /// \return
-  uint8_t AsCVType() const;
-
-  // Convert from OpenCV type to DataType
-  /// \param cv_type
-  /// \return
-  static DataType FromCVType(int cv_type);
-#endif
-
   // Returns a string representation of the type
   /// \return
   std::string ToString() const;
@@ -207,22 +140,6 @@ class DataType {
   template <typename T>
   static DataType FromCType();
 
-#ifdef ENABLE_MINDDATA_PYTHON
-  // Convert from DataType to Pybind type
-  /// \return
-  py::dtype AsNumpyType() const;
-
-  // Convert from NP type to DataType
-  /// \param type
-  /// \return
-  static DataType FromNpType(const py::dtype &type);
-
-  // Convert from NP array to DataType
-  /// \param py array
-  /// \return
-  static DataType FromNpArray(const py::array &arr);
-#endif
-
   // Get the buffer string format of the current type. Used in pybind buffer protocol.
   /// \return
   std::string GetPybindFormat() const;
diff --git a/mindspore/ccsrc/minddata/dataset/core/global_context.cc b/mindspore/ccsrc/minddata/dataset/core/global_context.cc
index 1bf9f5dafd5..b2ed8f9a257 100644
--- a/mindspore/ccsrc/minddata/dataset/core/global_context.cc
+++ b/mindspore/ccsrc/minddata/dataset/core/global_context.cc
@@ -57,9 +57,7 @@ Status GlobalContext::Init() {
 
   // Create some tensor allocators for the different types and hook them into the pool.
   tensor_allocator_ = std::make_unique<Allocator<Tensor>>(mem_pool_);
-#if defined(ENABLE_MINDDATA_PYTHON)
   cv_tensor_allocator_ = std::make_unique<Allocator<CVTensor>>(mem_pool_);
-#endif
   device_tensor_allocator_ = std::make_unique<Allocator<DeviceTensor>>(mem_pool_);
   int_allocator_ = std::make_unique<IntAlloc>(mem_pool_);
   profiler_manager_ = std::make_shared<ProfilingManager>();
diff --git a/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/CMakeLists.txt b/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/CMakeLists.txt
index e388521b19b..daa4fc5447d 100644
--- a/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/CMakeLists.txt
+++ b/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/CMakeLists.txt
@@ -48,6 +48,6 @@ set(DVPP_IMAGE_SOURCE
 endif()
 
 add_library(kernels-dvpp-image OBJECT ${DVPP_IMAGE_SOURCE})
-if(ENABLE_ACL OR MSLITE_ENABLE_ACL)
+if(ENABLE_ACL)
     add_subdirectory(utils)
 endif()
diff --git a/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/utils/CMakeLists.txt b/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/utils/CMakeLists.txt
index 5aa83166bae..b880f5ab45f 100644
--- a/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/utils/CMakeLists.txt
+++ b/mindspore/ccsrc/minddata/dataset/kernels/image/dvpp/utils/CMakeLists.txt
@@ -14,12 +14,12 @@ set(DVPP_UTILS_SRC
     # plugin
     acl_plugin.cc
     )
-if(NOT MSLITE_ENABLE_ACL)
-    set(DVPP_UTILS_SRC
-    ${DVPP_UTILS_SRC}
-    acl_env_guard.cc # in lite, src code has acl_env_guard.cc impl
-    )
-endif()
+#if(NOT MSLITE_ENABLE_ACL)
+#    set(DVPP_UTILS_SRC
+#    ${DVPP_UTILS_SRC}
+#    acl_env_guard.cc # in lite, src code has acl_env_guard.cc impl
+#    )
+#endif()
 
 if(ENABLE_D)
 set(DVPP_UTILS_SRC
@@ -33,25 +33,25 @@ endif()
 add_library(dvpp_utils SHARED ${DVPP_UTILS_SRC})
 enable_target_when_only_build_plugins(dvpp_utils)
 
-if(MSLITE_ENABLE_ACL)
-    find_library(acl_dvpp libacl_dvpp.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
-    find_library(acl libascendcl.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
-    find_library(nnopbase libnnopbase.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
-    find_library(acl_dvpp_op libacl_dvpp_op.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
-    find_library(acl_dvpp_mpi libacl_dvpp_mpi.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
-    # find acl_env_guard in ascend_kernel_plugin
-    target_link_libraries(dvpp_utils PRIVATE ascend_kernel_plugin minddata-lite ${acl} ${acl_dvpp} mindspore_core ${nnopbase} ${acl_dvpp_op} ${acl_dvpp_mpi})
-else()
+#if(MSLITE_ENABLE_ACL)
+#    find_library(acl_dvpp libacl_dvpp.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
+#    find_library(acl libascendcl.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
+#    find_library(nnopbase libnnopbase.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
+#    find_library(acl_dvpp_op libacl_dvpp_op.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
+#    find_library(acl_dvpp_mpi libacl_dvpp_mpi.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
+#    # find acl_env_guard in ascend_kernel_plugin
+#    target_link_libraries(dvpp_utils PRIVATE ascend_kernel_plugin minddata-lite ${acl} ${acl_dvpp} mindspore_core ${nnopbase} ${acl_dvpp_op} ${acl_dvpp_mpi})
+#else()
     find_library(acl_dvpp libacl_dvpp.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
     find_library(acl libascendcl.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
     find_library(nnopbase libnnopbase.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
     find_library(acl_dvpp_op libacl_dvpp_op.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
     find_library(acl_dvpp_mpi libacl_dvpp_mpi.so ${ASCEND_CANN_RUNTIME_PATH} ${ASCEND_TOOLKIT_RUNTIME_PATH})
     target_link_libraries(dvpp_utils PRIVATE _c_dataengine ${acl} ${acl_dvpp} mindspore_core ${nnopbase} ${acl_dvpp_op} ${acl_dvpp_mpi})
-endif()
+#endif()
 add_dependencies(dvpp_utils _mindspore_ascend_symbol_obj)
 target_link_libraries(dvpp_utils PRIVATE $<TARGET_OBJECTS:_mindspore_ascend_symbol_obj>)
-
-if(MSLITE_ENABLE_CLOUD_MIND_DATA)
-    add_dependencies(dvpp_utils fbs_src)
-endif()
+#
+#if(MSLITE_ENABLE_CLOUD_MIND_DATA)
+#    add_dependencies(dvpp_utils fbs_src)
+#endif()
diff --git a/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.cc b/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.cc
index 6a1452f698b..e80b4204be1 100644
--- a/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.cc
+++ b/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.cc
@@ -20,24 +20,13 @@
 #include <utility>
 #include <vector>
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include <opencv2/imgproc/types_c.h>
-#include <opencv2/imgcodecs.hpp>
-#include <opencv2/imgproc/imgproc.hpp>
-#endif
-
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include "minddata/dataset/core/cv_tensor.h"
-#endif
 #include "minddata/dataset/core/tensor.h"
 #include "minddata/dataset/core/tensor_shape.h"
 #include "minddata/dataset/include/dataset/constants.h"
 #include "minddata/dataset/kernels/image/lite_cv/image_process.h"
 #include "minddata/dataset/kernels/image/lite_cv/lite_mat.h"
 #include "minddata/dataset/kernels/image/math_utils.h"
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include "minddata/dataset/kernels/image/resize_cubic_op.h"
-#endif
+
 #include "minddata/dataset/util/random.h"
 
 constexpr int64_t hw_shape = 2;
@@ -46,55 +35,6 @@ constexpr int64_t hwc_rank = 3;
 #define MAX_INT_PRECISION 16777216  // float int precision is 16777216
 namespace mindspore {
 namespace dataset {
-#if defined(ENABLE_MINDDATA_PYTHON)
-bool IsNonEmptyPNG(const std::shared_ptr<Tensor> &input) {
-  const unsigned char kPngMagic[] = "\x89\x50\x4E\x47";
-  constexpr dsize_t kPngMagicLen = 4;
-  return input->SizeInBytes() > kPngMagicLen && memcmp(input->GetBuffer(), kPngMagic, kPngMagicLen) == 0;
-}
-
-Status Rescale(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *output, float rescale, float shift) {
-  std::shared_ptr<CVTensor> input_cv = CVTensor::AsCVTensor(input);
-  if (!input_cv->mat().data) {
-    RETURN_STATUS_UNEXPECTED("[Internal ERROR] Rescale: load image failed.");
-  }
-  cv::Mat input_image = input_cv->mat();
-  std::shared_ptr<CVTensor> output_cv;
-  RETURN_IF_NOT_OK(CVTensor::CreateEmpty(input_cv->shape(), DataType(DataType::DE_FLOAT32), &output_cv));
-  try {
-    input_image.convertTo(output_cv->mat(), CV_32F, rescale, shift);
-    *output = std::static_pointer_cast<Tensor>(output_cv);
-  } catch (const cv::Exception &e) {
-    RETURN_STATUS_UNEXPECTED("Rescale: " + std::string(e.what()));
-  }
-  return Status::OK();
-}
-
-Status SwapRedAndBlue(std::shared_ptr<Tensor> input, std::shared_ptr<Tensor> *output) {
-  try {
-    RETURN_IF_NOT_OK(ValidateImage(input, "SwapRedBlue", {3, 5, 11}));
-    std::shared_ptr<CVTensor> input_cv = CVTensor::AsCVTensor(std::move(input));
-    CHECK_FAIL_RETURN_UNEXPECTED(
-      input_cv->shape().Size() > kChannelIndexHWC,
-      "SwapRedAndBlue: rank of input data should be greater than:" + std::to_string(kChannelIndexHWC) +
-        ", but got:" + std::to_string(input_cv->shape().Size()));
-    int num_channels = static_cast<int>(input_cv->shape()[kChannelIndexHWC]);
-    if (input_cv->shape().Size() != kDefaultImageRank || num_channels != kDefaultImageChannel) {
-      RETURN_STATUS_UNEXPECTED("SwapRedBlue: image shape should be in <H,W,C> format, but got:" +
-                               input_cv->shape().ToString());
-    }
-    std::shared_ptr<CVTensor> output_cv;
-    RETURN_IF_NOT_OK(CVTensor::CreateEmpty(input_cv->shape(), input_cv->type(), &output_cv));
-
-    cv::cvtColor(input_cv->mat(), output_cv->mat(), static_cast<int>(cv::COLOR_BGR2RGB));
-    *output = std::static_pointer_cast<Tensor>(output_cv);
-    return Status::OK();
-  } catch (const cv::Exception &e) {
-    RETURN_STATUS_UNEXPECTED("SwapRedBlue: " + std::string(e.what()));
-  }
-}
-#endif
-
 bool IsNonEmptyJPEG(const std::shared_ptr<Tensor> &input) {
   const unsigned char *kJpegMagic = (unsigned char *)"\xFF\xD8\xFF";
   constexpr size_t kJpegMagicLen = 3;
@@ -305,39 +245,11 @@ static LDataType GetLiteCVDataType(const DataType &data_type) {
   }
 }
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-Status DecodeCv(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *output) {
-  std::shared_ptr<CVTensor> input_cv = CVTensor::AsCVTensor(input);
-  if (!input_cv->mat().data) {
-    RETURN_STATUS_UNEXPECTED("[Internal ERROR] Decode: load image failed.");
-  }
-  try {
-    cv::Mat img_mat = cv::imdecode(input_cv->mat(), cv::IMREAD_COLOR | cv::IMREAD_IGNORE_ORIENTATION);
-    if (img_mat.data == nullptr) {
-      std::string err = "Decode: image decode failed.";
-      RETURN_STATUS_UNEXPECTED(err);
-    }
-    cv::cvtColor(img_mat, img_mat, static_cast<int>(cv::COLOR_BGR2RGB));
-    std::shared_ptr<CVTensor> output_cv;
-    const dsize_t rank_num = 3;
-    RETURN_IF_NOT_OK(CVTensor::CreateFromMat(img_mat, rank_num, &output_cv));
-    *output = std::static_pointer_cast<Tensor>(output_cv);
-    return Status::OK();
-  } catch (const cv::Exception &e) {
-    RETURN_STATUS_UNEXPECTED("Decode: " + std::string(e.what()));
-  }
-}
-#endif
-
 Status Decode(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *output) {
   if (IsNonEmptyJPEG(input)) {
     return JpegCropAndDecode(input, output);
   } else {
-#if defined(ENABLE_MINDDATA_PYTHON)
-    return DecodeCv(input, output);
-#else
     RETURN_STATUS_UNEXPECTED("Decode: Decode only supports jpeg for android");
-#endif
   }
 }
 
@@ -465,94 +377,6 @@ Status Normalize(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *
   return Status::OK();
 }
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-int GetCVInterpolationMode(InterpolationMode mode) {
-  switch (mode) {
-    case InterpolationMode::kLinear:
-      return static_cast<int>(cv::InterpolationFlags::INTER_LINEAR);
-    case InterpolationMode::kCubic:
-      return static_cast<int>(cv::InterpolationFlags::INTER_CUBIC);
-    case InterpolationMode::kArea:
-      return static_cast<int>(cv::InterpolationFlags::INTER_AREA);
-    case InterpolationMode::kNearestNeighbour:
-      return static_cast<int>(cv::InterpolationFlags::INTER_NEAREST);
-    default:
-      return static_cast<int>(cv::InterpolationFlags::INTER_LINEAR);
-  }
-}
-
-Status Resize(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *output, int32_t output_height,
-              int32_t output_width, double fx, double fy, InterpolationMode mode) {
-  std::shared_ptr<CVTensor> input_cv = CVTensor::AsCVTensor(input);
-  if (!input_cv->mat().data) {
-    RETURN_STATUS_UNEXPECTED("[Internal ERROR] Resize: load image failed.");
-  }
-  RETURN_IF_NOT_OK(ValidateImageRank("Resize", input_cv->Rank()));
-
-  cv::Mat in_image = input_cv->mat();
-  const uint32_t kResizeShapeLimits = 1000;
-  // resize image too large or too small, 1000 is arbitrarily chosen here to prevent open cv from segmentation fault
-  CHECK_FAIL_RETURN_UNEXPECTED((std::numeric_limits<int>::max() / kResizeShapeLimits) > in_image.rows,
-                               "Resize: in_image rows out of bounds.");
-  CHECK_FAIL_RETURN_UNEXPECTED((std::numeric_limits<int>::max() / kResizeShapeLimits) > in_image.cols,
-                               "Resize: in_image cols out of bounds.");
-  if (output_height > in_image.rows * kResizeShapeLimits || output_width > in_image.cols * kResizeShapeLimits) {
-    RETURN_STATUS_ERROR(
-      StatusCode::kMDShapeMisMatch,
-      "Resize: the resizing width or height is too big, it's 1000 times bigger than the original image, got output "
-      "height: " +
-        std::to_string(output_height) + ", width: " + std::to_string(output_width) +
-        ", and original image size:" + std::to_string(in_image.rows) + ", " + std::to_string(in_image.cols));
-  }
-  if (output_height == 0 || output_width == 0) {
-    RETURN_STATUS_ERROR(StatusCode::kMDShapeMisMatch,
-                        "Resize: the input value of 'resize' is invalid, width or height is zero.");
-  }
-
-  if (mode == InterpolationMode::kCubicPil) {
-    if (input_cv->shape().Size() != kDefaultImageChannel ||
-        input_cv->shape()[kChannelIndexHWC] != kDefaultImageChannel) {
-      RETURN_STATUS_UNEXPECTED("Resize: Interpolation mode PILCUBIC only supports image with 3 channels, but got: " +
-                               input_cv->shape().ToString());
-    }
-
-    LiteMat im_in;
-    LiteMat im_out;
-    std::shared_ptr<Tensor> output_tensor;
-    TensorShape new_shape = TensorShape({output_height, output_width, 3});
-    RETURN_IF_NOT_OK(Tensor::CreateEmpty(new_shape, input_cv->type(), &output_tensor));
-    uint8_t *buffer = reinterpret_cast<uint8_t *>(&(*output_tensor->begin<uint8_t>()));
-    im_out.Init(output_width, output_height, static_cast<int>(input_cv->shape()[kChannelIndexHWC]),
-                reinterpret_cast<void *>(buffer), LDataType::UINT8);
-    im_in.Init(static_cast<int>(input_cv->shape()[1]), static_cast<int>(input_cv->shape()[0]),
-               static_cast<int>(input_cv->shape()[kChannelIndexHWC]), input_cv->mat().data, LDataType::UINT8);
-    CHECK_FAIL_RETURN_UNEXPECTED(!im_out.IsEmpty(), "Resize: Init image tensor failed, return empty tensor.");
-    CHECK_FAIL_RETURN_UNEXPECTED(!im_in.IsEmpty(), "Resize: Init image tensor failed, return empty tensor.");
-    if (ResizeCubic(im_in, im_out, output_width, output_height) == false) {
-      RETURN_STATUS_UNEXPECTED("Resize: failed to do resize, please check the error msg.");
-    }
-    *output = output_tensor;
-    return Status::OK();
-  }
-  try {
-    TensorShape shape{output_height, output_width};
-    if (input_cv->Rank() == kDefaultImageRank) {
-      int num_channels = static_cast<int>(input_cv->shape()[kChannelIndexHWC]);
-      shape = shape.AppendDim(num_channels);
-    }
-    std::shared_ptr<CVTensor> output_cv;
-    RETURN_IF_NOT_OK(CVTensor::CreateEmpty(shape, input_cv->type(), &output_cv));
-
-    auto cv_mode = GetCVInterpolationMode(mode);
-    cv::resize(in_image, output_cv->mat(), cv::Size(output_width, output_height), fx, fy, cv_mode);
-    *output = std::static_pointer_cast<Tensor>(output_cv);
-    return Status::OK();
-  } catch (const cv::Exception &e) {
-    RETURN_STATUS_UNEXPECTED("Resize: " + std::string(e.what()));
-  }
-}
-
-#else
 Status Resize(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *output, int32_t output_height,
               int32_t output_width, double fx, double fy, InterpolationMode mode) {
   if (mode != InterpolationMode::kLinear) {
@@ -608,7 +432,7 @@ Status Resize(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *out
   }
   return Status::OK();
 }
-#endif
+
 
 Status ResizePreserve(const TensorRow &inputs, int32_t height, int32_t width, int32_t img_orientation,
                       TensorRow *outputs) {
diff --git a/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.h b/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.h
index e4cd22e2ef7..572c62d2b2c 100644
--- a/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.h
+++ b/mindspore/ccsrc/minddata/dataset/kernels/image/lite_image_utils.h
@@ -67,22 +67,6 @@ struct JpegErrorManagerCustom {
   jmp_buf setjmp_buffer;
 };
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-bool IsNonEmptyPNG(const std::shared_ptr<Tensor> &input);
-
-/// \brief Returns Rescaled image
-/// \param input: Tensor of shape <H,W,C> or <H,W> and any OpenCv compatible type, see CVTensor.
-/// \param rescale: rescale parameter
-/// \param shift: shift parameter
-/// \param output: Rescaled image Tensor of same input shape and type DE_FLOAT32
-Status Rescale(const std::shared_ptr<Tensor> &input, std::shared_ptr<Tensor> *output, float rescale, float shift);
-
-/// \brief Swap the red and blue pixels (RGB <-> BGR)
-/// \param input: Tensor of shape <H,W,3> and any OpenCv compatible type, see CVTensor.
-/// \param output: Swapped image of same shape and type
-Status SwapRedAndBlue(std::shared_ptr<Tensor> input, std::shared_ptr<Tensor> *output);
-#endif
-
 bool IsNonEmptyJPEG(const std::shared_ptr<Tensor> &input);
 
 void JpegSetSource(j_decompress_ptr c_info, const void *data, int64_t data_size);
diff --git a/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/pad_ir.cc b/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/pad_ir.cc
index efa2b28638c..c666ddfb506 100644
--- a/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/pad_ir.cc
+++ b/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/pad_ir.cc
@@ -18,9 +18,6 @@
 
 #include <algorithm>
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include "minddata/dataset/kernels/image/pad_op.h"
-#endif
 #if defined(ENABLE_D)
 #include "minddata/dataset/kernels/image/dvpp/ascend910b/dvpp_pad_op.h"
 #endif
@@ -30,127 +27,7 @@
 namespace mindspore {
 namespace dataset {
 namespace vision {
-#if defined(ENABLE_MINDDATA_PYTHON)
-// PadOperation
-PadOperation::PadOperation(const std::vector<int32_t> &padding, const std::vector<uint8_t> &fill_value,
-                           BorderType padding_mode, const std::string &device_target)
-    : padding_(padding), fill_value_(fill_value), padding_mode_(padding_mode), device_target_(device_target) {}
-
-PadOperation::~PadOperation() = default;
-
-std::string PadOperation::Name() const { return kPadOperation; }
-
-Status PadOperation::ValidateParams() {
-  // padding
-  RETURN_IF_NOT_OK(ValidateVectorPadding("Pad", padding_));
-  // fill_value
-  RETURN_IF_NOT_OK(ValidateVectorFillvalue("Pad", fill_value_));
-  // padding_mode
-  if (padding_mode_ != BorderType::kConstant && padding_mode_ != BorderType::kEdge &&
-      padding_mode_ != BorderType::kReflect && padding_mode_ != BorderType::kSymmetric) {
-    std::string err_msg = "Pad: Invalid BorderType, check input value of enum.";
-    LOG_AND_RETURN_STATUS_SYNTAX_ERROR(err_msg);
-  }
-  // device target
-  if (device_target_ != "CPU" && device_target_ != "Ascend") {
-    std::string err_msg = "Pad: Invalid device target. It's not CPU or Ascend.";
-    LOG_AND_RETURN_STATUS_SYNTAX_ERROR(err_msg);
-  }
-  return Status::OK();
-}
-
-std::shared_ptr<TensorOp> PadOperation::Build() {
-  constexpr size_t dimension_zero = 0;
-  constexpr size_t dimension_one = 1;
-  constexpr size_t dimension_two = 2;
-  constexpr size_t dimension_three = 3;
-  constexpr size_t size_one = 1;
-  constexpr size_t size_two = 2;
-  constexpr size_t size_three = 3;
-  int32_t pad_top, pad_bottom, pad_left, pad_right;
-  switch (padding_.size()) {
-    case size_one:
-      pad_left = padding_[dimension_zero];
-      pad_top = padding_[dimension_zero];
-      pad_right = padding_[dimension_zero];
-      pad_bottom = padding_[dimension_zero];
-      break;
-    case size_two:
-      pad_left = padding_[dimension_zero];
-      pad_right = padding_[dimension_zero];
-      pad_top = padding_[dimension_one];
-      pad_bottom = padding_[dimension_one];
-      break;
-    default:
-      pad_left = padding_[dimension_zero];
-      pad_top = padding_[dimension_one];
-      pad_right = padding_[dimension_two];
-      pad_bottom = padding_[dimension_three];
-  }
-  uint8_t fill_r, fill_g, fill_b;
-
-  fill_r = fill_value_[dimension_zero];
-  fill_g = fill_value_[dimension_zero];
-  fill_b = fill_value_[dimension_zero];
-
-  if (fill_value_.size() == size_three) {
-    fill_r = fill_value_[dimension_zero];
-    fill_g = fill_value_[dimension_one];
-    fill_b = fill_value_[dimension_two];
-  }
 
-  if (device_target_ == "CPU") {
-    std::shared_ptr<PadOp> tensor_op =
-      std::make_shared<PadOp>(pad_top, pad_bottom, pad_left, pad_right, padding_mode_, fill_r, fill_g, fill_b);
-    return tensor_op;
-#if defined(ENABLE_D)
-  } else if (device_target_ == "Ascend") {
-    std::shared_ptr<DvppPadOp> dvpp_tensor_op =
-      std::make_shared<DvppPadOp>(pad_top, pad_bottom, pad_left, pad_right, padding_mode_, fill_r, fill_g, fill_b);
-    return dvpp_tensor_op;
-#endif
-  } else {
-    MS_LOG(ERROR) << "Pad: Invalid device target. It's not CPU or Ascend.";
-    return nullptr;
-  }
-}
-
-Status PadOperation::to_json(nlohmann::json *out_json) {
-  RETURN_UNEXPECTED_IF_NULL(out_json);
-  nlohmann::json args;
-  args["padding"] = padding_;
-  args["fill_value"] = fill_value_;
-  args["padding_mode"] = padding_mode_;
-  args["device_target"] = device_target_;
-  *out_json = args;
-  return Status::OK();
-}
-
-Status PadOperation::from_json(nlohmann::json op_params, std::shared_ptr<TensorOperation> *operation) {
-  RETURN_UNEXPECTED_IF_NULL(operation);
-  RETURN_IF_NOT_OK(ValidateParamInJson(op_params, "padding", kPadOperation));
-  RETURN_IF_NOT_OK(ValidateParamInJson(op_params, "fill_value", kPadOperation));
-  RETURN_IF_NOT_OK(ValidateParamInJson(op_params, "padding_mode", kPadOperation));
-  RETURN_IF_NOT_OK(ValidateParamInJson(op_params, "device_target", kPadOperation));
-  std::vector<int32_t> padding = op_params["padding"];
-  std::vector<uint8_t> fill_value = op_params["fill_value"];
-  auto padding_mode = static_cast<BorderType>(op_params["padding_mode"]);
-  std::string device_target = op_params["device_target"];
-  *operation = std::make_shared<vision::PadOperation>(padding, fill_value, padding_mode, device_target);
-  return Status::OK();
-}
-
-MapTargetDevice PadOperation::Type() {
-  if (device_target_ == "CPU") {
-    return MapTargetDevice::kCpu;
-  } else if (device_target_ == "Ascend") {
-    return MapTargetDevice::kAscend910B;
-  } else {
-    MS_LOG(ERROR) << "Pad: Invalid device target. It's not CPU or Ascend.";
-    return MapTargetDevice::kInvalid;
-  }
-}
-#endif
 }  // namespace vision
 }  // namespace dataset
 }  // namespace mindspore
diff --git a/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/rescale_ir.cc b/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/rescale_ir.cc
index d748ddc1fcb..717d4222ae2 100644
--- a/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/rescale_ir.cc
+++ b/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/rescale_ir.cc
@@ -21,46 +21,6 @@
 namespace mindspore {
 namespace dataset {
 namespace vision {
-#if defined(ENABLE_MINDDATA_PYTHON)
-// RescaleOperation
-RescaleOperation::RescaleOperation(float rescale, float shift) : rescale_(rescale), shift_(shift) {}
-
-RescaleOperation::~RescaleOperation() = default;
-
-std::string RescaleOperation::Name() const { return kRescaleOperation; }
-
-Status RescaleOperation::ValidateParams() {
-  if (rescale_ < 0.0) {
-    std::string err_msg = "Rescale: rescale must be greater than or equal to 0, got: " + std::to_string(rescale_);
-    LOG_AND_RETURN_STATUS_SYNTAX_ERROR(err_msg);
-  }
-  return Status::OK();
-}
-
-std::shared_ptr<TensorOp> RescaleOperation::Build() {
-  std::shared_ptr<RescaleOp> tensor_op = std::make_shared<RescaleOp>(rescale_, shift_);
-  return tensor_op;
-}
-
-Status RescaleOperation::to_json(nlohmann::json *out_json) {
-  RETURN_UNEXPECTED_IF_NULL(out_json);
-  nlohmann::json args;
-  args["rescale"] = rescale_;
-  args["shift"] = shift_;
-  *out_json = args;
-  return Status::OK();
-}
-
-Status RescaleOperation::from_json(nlohmann::json op_params, std::shared_ptr<TensorOperation> *operation) {
-  RETURN_UNEXPECTED_IF_NULL(operation);
-  RETURN_IF_NOT_OK(ValidateParamInJson(op_params, "rescale", kRescaleOperation));
-  RETURN_IF_NOT_OK(ValidateParamInJson(op_params, "shift", kRescaleOperation));
-  float rescale = op_params["rescale"];
-  float shift = op_params["shift"];
-  *operation = std::make_shared<vision::RescaleOperation>(rescale, shift);
-  return Status::OK();
-}
-#endif
 }  // namespace vision
 }  // namespace dataset
 }  // namespace mindspore
diff --git a/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/swap_red_blue_ir.cc b/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/swap_red_blue_ir.cc
index 8ed116c35f9..b56b99fdf9c 100644
--- a/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/swap_red_blue_ir.cc
+++ b/mindspore/ccsrc/minddata/dataset/kernels/ir/vision/swap_red_blue_ir.cc
@@ -15,34 +15,10 @@
  */
 #include "minddata/dataset/kernels/ir/vision/swap_red_blue_ir.h"
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include "minddata/dataset/kernels/image/swap_red_blue_op.h"
-#endif
-
 namespace mindspore {
 namespace dataset {
 namespace vision {
-#if defined(ENABLE_MINDDATA_PYTHON)
-// SwapRedBlueOperation.
-SwapRedBlueOperation::SwapRedBlueOperation() = default;
-
-SwapRedBlueOperation::~SwapRedBlueOperation() = default;
-
-std::string SwapRedBlueOperation::Name() const { return kSwapRedBlueOperation; }
-
-Status SwapRedBlueOperation::ValidateParams() { return Status::OK(); }
-
-std::shared_ptr<TensorOp> SwapRedBlueOperation::Build() {
-  std::shared_ptr<SwapRedBlueOp> tensor_op = std::make_shared<SwapRedBlueOp>();
-  return tensor_op;
-}
 
-Status SwapRedBlueOperation::from_json(nlohmann::json op_params, std::shared_ptr<TensorOperation> *operation) {
-  RETURN_UNEXPECTED_IF_NULL(operation);
-  *operation = std::make_shared<vision::SwapRedBlueOperation>();
-  return Status::OK();
-}
-#endif
 }  // namespace vision
 }  // namespace dataset
 }  // namespace mindspore
diff --git a/mindspore/ccsrc/minddata/dataset/util/log_adapter.h b/mindspore/ccsrc/minddata/dataset/util/log_adapter.h
index 6dcc8f20833..5f886102faf 100644
--- a/mindspore/ccsrc/minddata/dataset/util/log_adapter.h
+++ b/mindspore/ccsrc/minddata/dataset/util/log_adapter.h
@@ -16,12 +16,7 @@
 #ifndef MINDSPORE_CCSRC_MINDDATA_DATASET_UTIL_LOG_ADAPTER_H_
 #define MINDSPORE_CCSRC_MINDDATA_DATASET_UTIL_LOG_ADAPTER_H_
 
-#if defined(ENABLE_MINDDATA_PYTHON)
-#include "utils/log_adapter.h"
-#define DATASET_SRC_FILE_NAME FILE_NAME
-#else
 #include "mindspore/lite/src/common/log_adapter.h"
 #define DATASET_SRC_FILE_NAME LITE_FILE_NAME
-#endif
 
 #endif  // MINDSPORE_CCSRC_MINDDATA_DATASET_UTIL_LOG_ADAPTER_H_
diff --git a/mindspore/ccsrc/pipeline/jit/ps/compile_cache_manager.cc b/mindspore/ccsrc/pipeline/jit/ps/compile_cache_manager.cc
index 445f91cf9b8..caff57bce65 100644
--- a/mindspore/ccsrc/pipeline/jit/ps/compile_cache_manager.cc
+++ b/mindspore/ccsrc/pipeline/jit/ps/compile_cache_manager.cc
@@ -37,9 +37,7 @@
 #endif
 #include "include/common/utils/compile_cache_context.h"
 #include "include/common/utils/config_manager.h"
-#if !defined(BUILD_LITE)
 #include "include/backend/distributed/collective/collective_manager.h"
-#endif
 
 namespace mindspore {
 #ifndef MINDIR_EXPORT_TENSOR_LAYOUT_CLIP
@@ -97,11 +95,7 @@ namespace {
 std::string GetCompileCacheDir() {
   static const std::string user_defined_path = Common::GetUserDefineCachePath();
 
-#if !defined(BUILD_LITE)
   bool is_distributed = distributed::collective::CollectiveManager::instance()->initialized();
-#else
-  bool is_distributed = !IsStandAlone();
-#endif
   static const uint32_t rank_id = is_distributed ? GetRank() : 0;
   static const std::string compile_cache_dir = user_defined_path + "rank_" + std::to_string(rank_id);
   return compile_cache_dir;
diff --git a/mindspore/ccsrc/plugin/device/cpu/kernel/CMakeLists.txt b/mindspore/ccsrc/plugin/device/cpu/kernel/CMakeLists.txt
index c166b2a7043..9335e96cf2e 100644
--- a/mindspore/ccsrc/plugin/device/cpu/kernel/CMakeLists.txt
+++ b/mindspore/ccsrc/plugin/device/cpu/kernel/CMakeLists.txt
@@ -1,16 +1,5 @@
 file(GLOB_RECURSE CPU_KERNEL_OBJECTS RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} "*.cc")
 
-if(BUILD_LITE)
-    # mslite do not support python op
-    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -mavx ")
-    set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -maxv ")
-    string(REPLACE "-Wall" "" CMAKE_C_FLAGS ${CMAKE_C_FLAGS})
-    string(REPLACE "-Wall" "" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})
-    list(REMOVE_ITEM CPU_KERNEL_OBJECTS "pyexecute/py_execute_cpu_kernel.cc")
-    list(REMOVE_ITEM CPU_KERNEL_OBJECTS "pyfunc/py_func_cpu_kernel.cc")
-    list(REMOVE_ITEM CPU_KERNEL_OBJECTS "opaque_predicate_kernel.cc")
-endif()
-
 if(ENABLE_AKG AND ${CMAKE_SYSTEM_NAME} MATCHES "Linux" AND ENABLE_CPU)
     set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fopenmp")
 else()
diff --git a/mindspore/ccsrc/plugin/device/cpu/kernel/akg/akg_kernel_loader.cc b/mindspore/ccsrc/plugin/device/cpu/kernel/akg/akg_kernel_loader.cc
index d66ad0bfa22..743808d51c8 100644
--- a/mindspore/ccsrc/plugin/device/cpu/kernel/akg/akg_kernel_loader.cc
+++ b/mindspore/ccsrc/plugin/device/cpu/kernel/akg/akg_kernel_loader.cc
@@ -23,11 +23,7 @@
 #include <cerrno>
 #include <iostream>
 
-#ifdef BUILD_LITE
-#include "src/common/log_adapter.h"
-#else
 #include "utils/log_adapter.h"
-#endif
 
 namespace mindspore {
 namespace kernel {
diff --git a/mindspore/ccsrc/plugin/res_manager/ascend/mem_manager/ascend_memory_pool.cc b/mindspore/ccsrc/plugin/res_manager/ascend/mem_manager/ascend_memory_pool.cc
index 6bfcd501183..07de4ac1648 100644
--- a/mindspore/ccsrc/plugin/res_manager/ascend/mem_manager/ascend_memory_pool.cc
+++ b/mindspore/ccsrc/plugin/res_manager/ascend/mem_manager/ascend_memory_pool.cc
@@ -135,9 +135,7 @@ static uint64_t GetPid() {
 
 int32_t GetDeviceId() {
   int32_t device_id = 0;
-#if !defined(BUILD_LITE)
   device_id = static_cast<int32_t>(DistributedMeta::GetInstance()->local_rank_id());
-#endif
   return device_id;
 }
 
diff --git a/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/custom_op_infer.cc b/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/custom_op_infer.cc
index 097dc9fcea1..8ca39ac217a 100644
--- a/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/custom_op_infer.cc
+++ b/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/custom_op_infer.cc
@@ -16,15 +16,15 @@
 
 #include <string>
 #include <vector>
-#ifdef MSLITE_ENABLE_GRAPH_KERNEL
-#include <fstream>
-#include <memory>
-#include <unordered_map>
-#include "nlohmann/json.hpp"
-#include "plugin/res_manager/ascend/op_adapter/transform_util.h"
-#include "backend/common/graph_kernel/model/op_register.h"
-#include "backend/common/graph_kernel/core/value_depend_op_utils.h"
-#endif
+//#ifdef MSLITE_ENABLE_GRAPH_KERNEL
+//#include <fstream>
+//#include <memory>
+//#include <unordered_map>
+//#include "nlohmann/json.hpp"
+//#include "plugin/res_manager/ascend/op_adapter/transform_util.h"
+//#include "backend/common/graph_kernel/model/op_register.h"
+//#include "backend/common/graph_kernel/core/value_depend_op_utils.h"
+//#endif
 #include "utils/log_adapter.h"
 #include "graph/operator.h"
 
@@ -87,298 +87,288 @@ std::string GetCustomOpKey(const ge::Operator &op) {
 }
 }  // namespace
 
-#ifdef MSLITE_ENABLE_GRAPH_KERNEL
-using mindspore::graphkernel::inner::ConstTensorNode;
-using mindspore::graphkernel::inner::DAttrs;
-using mindspore::graphkernel::inner::Node;
-using mindspore::graphkernel::inner::NodeBase;
-using mindspore::graphkernel::inner::NodePtr;
-using mindspore::graphkernel::inner::NodePtrList;
-
-namespace {
-TypeId const ConvertGeDataType(const ge::DataType &type) {
-  static std::unordered_map<ge::DataType, TypeId> ge_ms_type = {
-    {ge::DataType::DT_FLOAT16, TypeId::kNumberTypeFloat16},
-    {ge::DataType::DT_FLOAT, TypeId::kNumberTypeFloat32},
-    {ge::DataType::DT_DOUBLE, TypeId::kNumberTypeFloat64},
-    {ge::DataType::DT_INT8, TypeId::kNumberTypeInt8},
-    {ge::DataType::DT_INT16, TypeId::kNumberTypeInt16},
-    {ge::DataType::DT_INT32, TypeId::kNumberTypeInt32},
-    {ge::DataType::DT_INT64, TypeId::kNumberTypeInt64},
-    {ge::DataType::DT_UINT8, TypeId::kNumberTypeUInt8},
-    {ge::DataType::DT_UINT16, TypeId::kNumberTypeUInt16},
-    {ge::DataType::DT_UINT32, TypeId::kNumberTypeUInt32},
-    {ge::DataType::DT_UINT64, TypeId::kNumberTypeUInt64},
-    {ge::DataType::DT_BOOL, TypeId::kNumberTypeBool},
-    {ge::DataType::DT_STRING, TypeId::kObjectTypeString},
-    {ge::DataType::DT_BF16, TypeId::kNumberTypeBFloat16},
-    {ge::DataType::DT_HIFLOAT8, TypeId::kNumberTypeHiFloat8},
-    {ge::DataType::DT_FLOAT8_E5M2, TypeId::kNumberTypeFloat8E5M2},
-    {ge::DataType::DT_FLOAT8_E4M3FN, TypeId::kNumberTypeFloat8E4M3FN},
-  };
-  auto iter = ge_ms_type.find(type);
-  if (iter != ge_ms_type.end()) {
-    return iter->second;
-  }
-  return TypeId::kTypeUnknown;
-}
-
-using ConvertFunc = std::function<tensor::TensorPtr(const nlohmann::json &)>;
-
-template <typename T, TypeId type_id>
-tensor::TensorPtr ConvertSingleValueHelper(const nlohmann::json &value) {
-  T v = value;
-  ShapeVector shape = {1};
-  return std::make_shared<tensor::Tensor>(type_id, shape, &v, type_id);
-}
-
-tensor::TensorPtr ConvertSingleValue(const nlohmann::json &value, const std::string &type) {
-  // reverse function of SetSingleValue
-  static std::unordered_map<std::string, ConvertFunc> convertTable = {
-    {"bool", ConvertSingleValueHelper<bool, TypeId::kNumberTypeBool>},
-    {"int8", ConvertSingleValueHelper<int8_t, TypeId::kNumberTypeInt8>},
-    {"int16", ConvertSingleValueHelper<int16_t, TypeId::kNumberTypeInt16>},
-    {"int32", ConvertSingleValueHelper<int32_t, TypeId::kNumberTypeInt32>},
-    {"int64", ConvertSingleValueHelper<int64_t, TypeId::kNumberTypeInt64>},
-    {"uint8", ConvertSingleValueHelper<uint8_t, TypeId::kNumberTypeUInt8>},
-    {"uint16", ConvertSingleValueHelper<uint16_t, TypeId::kNumberTypeUInt16>},
-    {"uint32", ConvertSingleValueHelper<uint32_t, TypeId::kNumberTypeUInt32>},
-    {"uint64", ConvertSingleValueHelper<uint64_t, TypeId::kNumberTypeUInt64>},
-    {"float16", ConvertSingleValueHelper<float, TypeId::kNumberTypeFloat16>},
-    {"float32", ConvertSingleValueHelper<float, TypeId::kNumberTypeFloat32>},
-    {"float64", ConvertSingleValueHelper<double, TypeId::kNumberTypeFloat64>}};
-  if (convertTable.count(type) > 0) {
-    ConvertFunc convertFunc = convertTable[type];
-    return convertFunc(value);
-  }
-  MS_LOG(WARNING) << "Fail to convert the single value: " << value << ". type is: " << type;
-  return nullptr;
-}
-
-tensor::TensorPtr ConvertListValue(const nlohmann::json &value, const std::string &type, const ShapeVector &shape) {
-  // reverse function of SetValueList
-  tensor::TensorPtr res = nullptr;
-  if (type == "int32") {
-    std::vector<int32_t> v = value;
-    res = std::make_shared<tensor::Tensor>(TypeId::kNumberTypeInt32, shape, &v[0], TypeId::kNumberTypeInt32);
-  } else if (type == "int64") {
-    std::vector<int64_t> v = value;
-    res = std::make_shared<tensor::Tensor>(TypeId::kNumberTypeInt64, shape, &v[0], TypeId::kNumberTypeInt64);
-  }
-  MS_LOG(WARNING) << "Fail to convert the list value: " << value << ". type is: " << type;
-  return res;
-}
-
-NodePtrList GetOpInputs(const nlohmann::json &op_desc, const std::unordered_map<std::string, NodePtr> &all_tensors,
-                        const std::string &op_name) {
-  const auto &value_depend_op_info = graphkernel::ValueDependOpUtils::GetOpIndexInfo();
-  bool is_depend_value = value_depend_op_info.find(op_name) != value_depend_op_info.end();
-  NodePtrList res;
-  for (const auto &input_desc : op_desc["input_desc"]) {
-    for (const auto &item : input_desc) {
-      std::string name = item["tensor_name"];
-      auto iter = all_tensors.find(name);
-      if (iter != all_tensors.end()) {
-        res.push_back(iter->second);
-      } else {
-        // const value input
-        if (is_depend_value && item.find("value") != item.end()) {
-          auto value = item["value"];
-          std::string type = item["data_type"];
-          ShapeVector shape = item["shape"];
-          auto tensor = value.is_array() ? ConvertListValue(value, type, shape) : ConvertSingleValue(value, type);
-          if (tensor != nullptr) {
-            res.push_back(std::make_shared<ConstTensorNode>(tensor));
-            continue;
-          }
-          MS_LOG(WARNING) << "Fail to parse the const value of tensor [" << name << "]. tensor json is: " << item;
-        }
-        std::string format = item["format"];
-        NodeBase n{ShapeVector(item["shape"]), StringToTypeId(item["data_type"]), format};
-        res.push_back(std::make_shared<Node>(n));
-      }
-    }
-  }
-  return res;
-}
-
-DAttrs GetOpAttr(const nlohmann::json &op_desc) {
-  DAttrs res;
-  // no attr
-  if (op_desc.find("attr") == op_desc.end() || op_desc["attr"].is_null()) {
-    return res;
-  }
-  for (const auto &item : op_desc["attr"]) {
-    std::string name = item["name"];
-    std::string type = item["data_type"];
-    ValuePtr attr_value = nullptr;
-    if (type == "str") {
-      std::string value = item["value"];
-      attr_value = (name == "dst_type" && op_desc["name"] == "Cast") ? StringToType(value) : MakeValue(value);
-    } else if (type == "int") {
-      int64_t value = item["value"];
-      attr_value = MakeValue(value);
-    } else if (type == "bool") {
-      bool value = item["value"];
-      attr_value = MakeValue(value);
-    } else if (type == "float") {
-      float value = item["value"];
-      attr_value = MakeValue(value);
-    } else if (type == "listInt") {
-      std::vector<int64_t> value = item["value"];
-      attr_value = MakeValue(value);
-    } else if (type == "listStr") {
-      std::vector<std::string> value = item["value"];
-      attr_value = MakeValue(value);
-    } else {
-      MS_LOG(WARNING) << "Fail to parse attr [" << name << "] because its type: " << type
-                      << " is not in supported list: [str, int, bool, float, listInt, listStr]. attr json is: " << item;
-    }
-    if (attr_value != nullptr) {
-      res[name] = attr_value;
-    }
-  }
-  return res;
-}
-
-bool InferOnline(const ge::Operator &op, const nlohmann::json &js, std::vector<ge::TensorDesc> *outputs_info) {
-  if (outputs_info == nullptr) {
-    return false;
-  }
-  std::unordered_map<std::string, NodePtr> all_tensors;
-  // iter input_desc: inputs info use the real info pass by GE
-  std::vector<nlohmann::json> input_desc = js["input_desc"];
-  for (size_t i = 0; i < input_desc.size(); ++i) {
-    const auto &item = input_desc[i][0];
-    std::string input_name = "x" + std::to_string(i);
-    auto ge_desc = op.GetInputDescByName(input_name.c_str());
-    std::string format = item["format"];
-    NodeBase n{ge_desc.GetShape().GetDims(), ConvertGeDataType(ge_desc.GetDataType()), format};
-    MS_LOG(DEBUG) << "input[" << i << "]: " << n.shape << " " << TypeIdToString(n.type);
-    all_tensors[item["tensor_name"]] = std::make_shared<Node>(n);
-  }
-
-  // iter op_desc: infer each op
-  for (const auto &op_desc : js["op_desc"]) {
-    std::string op_name = op_desc["name"];
-    auto op_ptr = mindspore::graphkernel::inner::OpRegistry::Instance().NewOp(op_name);
-    auto op_inputs = GetOpInputs(op_desc, all_tensors, op_name);
-    auto op_attr = GetOpAttr(op_desc);
-    auto infer_res = op_ptr->Infer(op_inputs, op_attr);
-    std::vector<nlohmann::json> op_output_desc = op_desc["output_desc"];
-    if (infer_res.size() != op_output_desc.size()) {
-      MS_LOG(ERROR) << "For op [" << op_name
-                    << "], the length of inferred output shape list is not equal to the length of output_desc list: "
-                    << infer_res.size() << " vs " << op_output_desc.size();
-      return false;
-    }
-    for (size_t i = 0; i < op_output_desc.size(); ++i) {
-      std::string name = op_output_desc[i]["tensor_name"];
-      all_tensors[name] = std::make_shared<Node>(infer_res[i]);
-    }
-  }
-
-  // iter output_desc: combine the outputs info
-  std::vector<nlohmann::json> output_desc = js["output_desc"];
-  // format not need infer
-  std::vector<int32_t> output_formats;
-  if (op.GetAttr("output_formats", output_formats) != ge::GRAPH_SUCCESS ||
-      output_formats.size() != output_desc.size()) {
-    return false;
-  }
-
-  for (size_t i = 0; i < output_desc.size(); ++i) {
-    std::string name = output_desc[i]["tensor_name"];
-    auto iter = all_tensors.find(name);
-    if (iter == all_tensors.end()) {
-      MS_LOG(ERROR) << "Tensor [" << name << "] not found in op_desc";
-      return false;
-    }
-    auto shape = iter->second->shape;
-    (void)outputs_info->emplace_back(ge::Shape(shape), static_cast<ge::Format>(output_formats[i]),
-                                     device::ascend::TransformUtil::ConvertDataType(iter->second->type));
-    MS_LOG(DEBUG) << "output[" << i << "]: " << shape << " " << TypeIdToString(iter->second->type);
-  }
-  return true;
-}
-
-bool InputsInfoNotChanged(const ge::Operator &op, const nlohmann::json &js) {
-  std::vector<nlohmann::json> input_desc = js["input_desc"];
-  for (size_t i = 0; i < input_desc.size(); ++i) {
-    std::string input_name = "x" + std::to_string(i);
-    auto ge_desc = op.GetInputDescByName(input_name.c_str());
-    auto ge_shape = ge_desc.GetShape().GetDims();
-    auto ge_type = ge_desc.GetDataType();
-    const auto &item = input_desc[i][0];
-    ShapeVector ms_shape = item["shape"];
-    auto ms_type = StringToTypeId(item["data_type"]);
-    if (ge_shape != ms_shape || ConvertGeDataType(ge_type) != ms_type) {
-      return false;
-    }
-  }
-  return true;
-}
-
-bool Infer(const ge::Operator &op, const std::string &op_key, const std::string &info_path,
-           std::vector<ge::TensorDesc> *outputs_info) {
-  if (outputs_info == nullptr) {
-    return false;
-  }
-
-  // read akg info and parse it to json format
-  std::ifstream info_str(info_path);
-  if (!info_str.is_open()) {
-    return false;
-  }
-  nlohmann::json js;
-  info_str >> js;
-  info_str.close();
-
-  // 1) if input information not changed, reuse the outputs info saved in op attr 2) else infer online
-  if (InputsInfoNotChanged(op, js)) {
-    MS_LOG(INFO) << "Infer shape offline for op " << op_key;
-    return InferOffline(op, outputs_info);
-  }
-  MS_LOG(INFO) << "Infer shape online for op " << op_key;
-  return InferOnline(op, js, outputs_info);
-}
-}  // namespace
-
-ge::graphStatus CustomAkgOpInferFunc(ge::Operator &op) {
-  auto op_key = GetCustomOpKey(op);
-  MS_LOG(INFO) << "Start infer shape for op " << op_key;
-
-  // get akg info path of current op
-  std::string info_path;
-  auto status = op.GetAttr("info_path", info_path);
-  if (status != ge::GRAPH_SUCCESS) {
-    return status;
-  }
-
-  // infer shape
-  std::vector<ge::TensorDesc> outputs_info;
-  try {
-    if (!Infer(op, op_key, info_path, &outputs_info)) {
-      MS_LOG(ERROR) << "Failed infer shape for op " << op_key << ", akg info path: " << info_path;
-      return ge::GRAPH_FAILED;
-    }
-  } catch (std::exception &e) {
-    MS_LOG(ERROR) << "Failed infer shape for op " << op_key << ", akg info path: " << info_path
-                  << " error message: " << e.what();
-    return ge::GRAPH_FAILED;
-  }
-
-  // update output desc
-  for (size_t i = 0; i < outputs_info.size(); ++i) {
-    std::string output_name = "y" + std::to_string(i);
-    (void)op.UpdateOutputDesc(output_name, outputs_info[i]);
-  }
-  MS_LOG(INFO) << "End infer shape for op " << op_key;
-  return ge::GRAPH_SUCCESS;
-}
-#else
+//#ifdef MSLITE_ENABLE_GRAPH_KERNEL
+// using mindspore::graphkernel::inner::ConstTensorNode;
+// using mindspore::graphkernel::inner::DAttrs;
+// using mindspore::graphkernel::inner::Node;
+// using mindspore::graphkernel::inner::NodeBase;
+// using mindspore::graphkernel::inner::NodePtr;
+// using mindspore::graphkernel::inner::NodePtrList;
+//
+// namespace {
+// TypeId ConvertGeDataType(const ge::DataType &type) {
+//  static std::unordered_map<ge::DataType, TypeId> ge_ms_type = {
+//    {ge::DataType::DT_FLOAT16, TypeId::kNumberTypeFloat16}, {ge::DataType::DT_FLOAT, TypeId::kNumberTypeFloat32},
+//    {ge::DataType::DT_DOUBLE, TypeId::kNumberTypeFloat64},  {ge::DataType::DT_INT8, TypeId::kNumberTypeInt8},
+//    {ge::DataType::DT_INT16, TypeId::kNumberTypeInt16},     {ge::DataType::DT_INT32, TypeId::kNumberTypeInt32},
+//    {ge::DataType::DT_INT64, TypeId::kNumberTypeInt64},     {ge::DataType::DT_UINT8, TypeId::kNumberTypeUInt8},
+//    {ge::DataType::DT_UINT16, TypeId::kNumberTypeUInt16},   {ge::DataType::DT_UINT32, TypeId::kNumberTypeUInt32},
+//    {ge::DataType::DT_UINT64, TypeId::kNumberTypeUInt64},   {ge::DataType::DT_BOOL, TypeId::kNumberTypeBool},
+//    {ge::DataType::DT_STRING, TypeId::kObjectTypeString},   {ge::DataType::DT_BF16, TypeId::kNumberTypeBFloat16}};
+//  auto iter = ge_ms_type.find(type);
+//  if (iter != ge_ms_type.end()) {
+//    return iter->second;
+//  }
+//  return TypeId::kTypeUnknown;
+//}
+//
+// using ConvertFunc = std::function<tensor::TensorPtr(const nlohmann::json &)>;
+//
+// template <typename T, TypeId type_id>
+// tensor::TensorPtr ConvertSingleValueHelper(const nlohmann::json &value) {
+//  T v = value;
+//  ShapeVector shape = {1};
+//  return std::make_shared<tensor::Tensor>(type_id, shape, &v, type_id);
+//}
+//
+// tensor::TensorPtr ConvertSingleValue(const nlohmann::json &value, const std::string &type) {
+//  // reverse function of SetSingleValue
+//  static std::unordered_map<std::string, ConvertFunc> convertTable = {
+//    {"bool", ConvertSingleValueHelper<bool, TypeId::kNumberTypeBool>},
+//    {"int8", ConvertSingleValueHelper<int8_t, TypeId::kNumberTypeInt8>},
+//    {"int16", ConvertSingleValueHelper<int16_t, TypeId::kNumberTypeInt16>},
+//    {"int32", ConvertSingleValueHelper<int32_t, TypeId::kNumberTypeInt32>},
+//    {"int64", ConvertSingleValueHelper<int64_t, TypeId::kNumberTypeInt64>},
+//    {"uint8", ConvertSingleValueHelper<uint8_t, TypeId::kNumberTypeUInt8>},
+//    {"uint16", ConvertSingleValueHelper<uint16_t, TypeId::kNumberTypeUInt16>},
+//    {"uint32", ConvertSingleValueHelper<uint32_t, TypeId::kNumberTypeUInt32>},
+//    {"uint64", ConvertSingleValueHelper<uint64_t, TypeId::kNumberTypeUInt64>},
+//    {"float16", ConvertSingleValueHelper<float, TypeId::kNumberTypeFloat16>},
+//    {"float32", ConvertSingleValueHelper<float, TypeId::kNumberTypeFloat32>},
+//    {"float64", ConvertSingleValueHelper<double, TypeId::kNumberTypeFloat64>}};
+//  if (convertTable.count(type) > 0) {
+//    ConvertFunc convertFunc = convertTable[type];
+//    return convertFunc(value);
+//  }
+//  MS_LOG(WARNING) << "Fail to convert the single value: " << value << ". type is: " << type;
+//  return nullptr;
+//}
+//
+// tensor::TensorPtr ConvertListValue(const nlohmann::json &value, const std::string &type, const ShapeVector &shape) {
+//  // reverse function of SetValueList
+//  tensor::TensorPtr res = nullptr;
+//  if (type == "int32") {
+//    std::vector<int32_t> v = value;
+//    res = std::make_shared<tensor::Tensor>(TypeId::kNumberTypeInt32, shape, &v[0], TypeId::kNumberTypeInt32);
+//  } else if (type == "int64") {
+//    std::vector<int64_t> v = value;
+//    res = std::make_shared<tensor::Tensor>(TypeId::kNumberTypeInt64, shape, &v[0], TypeId::kNumberTypeInt64);
+//  }
+//  MS_LOG(WARNING) << "Fail to convert the list value: " << value << ". type is: " << type;
+//  return res;
+//}
+//
+// NodePtrList GetOpInputs(const nlohmann::json &op_desc, const std::unordered_map<std::string, NodePtr> &all_tensors,
+//                        const std::string &op_name) {
+//  const auto &value_depend_op_info = graphkernel::ValueDependOpUtils::GetOpIndexInfo();
+//  bool is_depend_value = value_depend_op_info.find(op_name) != value_depend_op_info.end();
+//  NodePtrList res;
+//  for (const auto &input_desc : op_desc["input_desc"]) {
+//    for (const auto &item : input_desc) {
+//      std::string name = item["tensor_name"];
+//      auto iter = all_tensors.find(name);
+//      if (iter != all_tensors.end()) {
+//        res.push_back(iter->second);
+//      } else {
+//        // const value input
+//        if (is_depend_value && item.find("value") != item.end()) {
+//          auto value = item["value"];
+//          std::string type = item["data_type"];
+//          ShapeVector shape = item["shape"];
+//          auto tensor = value.is_array() ? ConvertListValue(value, type, shape) : ConvertSingleValue(value, type);
+//          if (tensor != nullptr) {
+//            res.push_back(std::make_shared<ConstTensorNode>(tensor));
+//            continue;
+//          }
+//          MS_LOG(WARNING) << "Fail to parse the const value of tensor [" << name << "]. tensor json is: " << item;
+//        }
+//        std::string format = item["format"];
+//        NodeBase n{ShapeVector(item["shape"]), StringToTypeId(item["data_type"]), format};
+//        res.push_back(std::make_shared<Node>(n));
+//      }
+//    }
+//  }
+//  return res;
+//}
+//
+// DAttrs GetOpAttr(const nlohmann::json &op_desc) {
+//  DAttrs res;
+//  // no attr
+//  if (op_desc.find("attr") == op_desc.end() || op_desc["attr"].is_null()) {
+//    return res;
+//  }
+//  for (const auto &item : op_desc["attr"]) {
+//    std::string name = item["name"];
+//    std::string type = item["data_type"];
+//    ValuePtr attr_value = nullptr;
+//    if (type == "str") {
+//      std::string value = item["value"];
+//      attr_value = (name == "dst_type" && op_desc["name"] == "Cast") ? StringToType(value) : MakeValue(value);
+//    } else if (type == "int") {
+//      int64_t value = item["value"];
+//      attr_value = MakeValue(value);
+//    } else if (type == "bool") {
+//      bool value = item["value"];
+//      attr_value = MakeValue(value);
+//    } else if (type == "float") {
+//      float value = item["value"];
+//      attr_value = MakeValue(value);
+//    } else if (type == "listInt") {
+//      std::vector<int64_t> value = item["value"];
+//      attr_value = MakeValue(value);
+//    } else if (type == "listStr") {
+//      std::vector<std::string> value = item["value"];
+//      attr_value = MakeValue(value);
+//    } else {
+//      MS_LOG(WARNING) << "Fail to parse attr [" << name << "] because its type: " << type
+//                      << " is not in supported list: [str, int, bool, float, listInt, listStr]. attr json is: " <<
+//                      item;
+//    }
+//    if (attr_value != nullptr) {
+//      res[name] = attr_value;
+//    }
+//  }
+//  return res;
+//}
+//
+// bool InferOnline(const ge::Operator &op, const nlohmann::json &js, std::vector<ge::TensorDesc> *outputs_info) {
+//  if (outputs_info == nullptr) {
+//    return false;
+//  }
+//  std::unordered_map<std::string, NodePtr> all_tensors;
+//  // iter input_desc: inputs info use the real info pass by GE
+//  std::vector<nlohmann::json> input_desc = js["input_desc"];
+//  for (size_t i = 0; i < input_desc.size(); ++i) {
+//    const auto &item = input_desc[i][0];
+//    std::string input_name = "x" + std::to_string(i);
+//    auto ge_desc = op.GetInputDescByName(input_name.c_str());
+//    std::string format = item["format"];
+//    NodeBase n{ge_desc.GetShape().GetDims(), ConvertGeDataType(ge_desc.GetDataType()), format};
+//    MS_LOG(DEBUG) << "input[" << i << "]: " << n.shape << " " << TypeIdToString(n.type);
+//    all_tensors[item["tensor_name"]] = std::make_shared<Node>(n);
+//  }
+//
+//  // iter op_desc: infer each op
+//  for (const auto &op_desc : js["op_desc"]) {
+//    std::string op_name = op_desc["name"];
+//    auto op_ptr = mindspore::graphkernel::inner::OpRegistry::Instance().NewOp(op_name);
+//    auto op_inputs = GetOpInputs(op_desc, all_tensors, op_name);
+//    auto op_attr = GetOpAttr(op_desc);
+//    auto infer_res = op_ptr->Infer(op_inputs, op_attr);
+//    std::vector<nlohmann::json> op_output_desc = op_desc["output_desc"];
+//    if (infer_res.size() != op_output_desc.size()) {
+//      MS_LOG(ERROR) << "For op [" << op_name
+//                    << "], the length of inferred output shape list is not equal to the length of output_desc list: "
+//                    << infer_res.size() << " vs " << op_output_desc.size();
+//      return false;
+//    }
+//    for (size_t i = 0; i < op_output_desc.size(); ++i) {
+//      std::string name = op_output_desc[i]["tensor_name"];
+//      all_tensors[name] = std::make_shared<Node>(infer_res[i]);
+//    }
+//  }
+//
+//  // iter output_desc: combine the outputs info
+//  std::vector<nlohmann::json> output_desc = js["output_desc"];
+//  // format not need infer
+//  std::vector<int32_t> output_formats;
+//  if (op.GetAttr("output_formats", output_formats) != ge::GRAPH_SUCCESS ||
+//      output_formats.size() != output_desc.size()) {
+//    return false;
+//  }
+//
+//  for (size_t i = 0; i < output_desc.size(); ++i) {
+//    std::string name = output_desc[i]["tensor_name"];
+//    auto iter = all_tensors.find(name);
+//    if (iter == all_tensors.end()) {
+//      MS_LOG(ERROR) << "Tensor [" << name << "] not found in op_desc";
+//      return false;
+//    }
+//    auto shape = iter->second->shape;
+//    (void)outputs_info->emplace_back(ge::Shape(shape), static_cast<ge::Format>(output_formats[i]),
+//                                     device::ascend::TransformUtil::ConvertDataType(iter->second->type));
+//    MS_LOG(DEBUG) << "output[" << i << "]: " << shape << " " << TypeIdToString(iter->second->type);
+//  }
+//  return true;
+//}
+//
+// bool InputsInfoNotChanged(const ge::Operator &op, const nlohmann::json &js) {
+//  std::vector<nlohmann::json> input_desc = js["input_desc"];
+//  for (size_t i = 0; i < input_desc.size(); ++i) {
+//    std::string input_name = "x" + std::to_string(i);
+//    auto ge_desc = op.GetInputDescByName(input_name.c_str());
+//    auto ge_shape = ge_desc.GetShape().GetDims();
+//    auto ge_type = ge_desc.GetDataType();
+//    const auto &item = input_desc[i][0];
+//    ShapeVector ms_shape = item["shape"];
+//    auto ms_type = StringToTypeId(item["data_type"]);
+//    if (ge_shape != ms_shape || ConvertGeDataType(ge_type) != ms_type) {
+//      return false;
+//    }
+//  }
+//  return true;
+//}
+//
+// bool Infer(const ge::Operator &op, const std::string &op_key, const std::string &info_path,
+//           std::vector<ge::TensorDesc> *outputs_info) {
+//  if (outputs_info == nullptr) {
+//    return false;
+//  }
+//
+//  // read akg info and parse it to json format
+//  std::ifstream info_str(info_path);
+//  if (!info_str.is_open()) {
+//    return false;
+//  }
+//  nlohmann::json js;
+//  info_str >> js;
+//  info_str.close();
+//
+//  // 1) if input information not changed, reuse the outputs info saved in op attr 2) else infer online
+//  if (InputsInfoNotChanged(op, js)) {
+//    MS_LOG(INFO) << "Infer shape offline for op " << op_key;
+//    return InferOffline(op, outputs_info);
+//  }
+//  MS_LOG(INFO) << "Infer shape online for op " << op_key;
+//  return InferOnline(op, js, outputs_info);
+//}
+//}  // namespace
+//
+// ge::graphStatus CustomAkgOpInferFunc(ge::Operator &op) {
+//  auto op_key = GetCustomOpKey(op);
+//  MS_LOG(INFO) << "Start infer shape for op " << op_key;
+//
+//  // get akg info path of current op
+//  std::string info_path;
+//  auto status = op.GetAttr("info_path", info_path);
+//  if (status != ge::GRAPH_SUCCESS) {
+//    return status;
+//  }
+//
+//  // infer shape
+//  std::vector<ge::TensorDesc> outputs_info;
+//  try {
+//    if (!Infer(op, op_key, info_path, &outputs_info)) {
+//      MS_LOG(ERROR) << "Failed infer shape for op " << op_key << ", akg info path: " << info_path;
+//      return ge::GRAPH_FAILED;
+//    }
+//  } catch (std::exception &e) {
+//    MS_LOG(ERROR) << "Failed infer shape for op " << op_key << ", akg info path: " << info_path
+//                  << " error message: " << e.what();
+//    return ge::GRAPH_FAILED;
+//  }
+//
+//  // update output desc
+//  for (size_t i = 0; i < outputs_info.size(); ++i) {
+//    std::string output_name = "y" + std::to_string(i);
+//    (void)op.UpdateOutputDesc(output_name, outputs_info[i]);
+//  }
+//  MS_LOG(INFO) << "End infer shape for op " << op_key;
+//  return ge::GRAPH_SUCCESS;
+//}
+//#else
 ge::graphStatus CustomAkgOpInferFunc(ge::Operator &) { return ge::GRAPH_SUCCESS; }
-#endif
+//#endif
 
 ge::graphStatus CustomTbeAicpuOpInferFunc(ge::Operator &op) {
   auto op_key = GetCustomOpKey(op);
diff --git a/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.cc b/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.cc
index 62b27674d88..02e56a80379 100644
--- a/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.cc
+++ b/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.cc
@@ -52,6 +52,16 @@ class MsTensorRel {
  private:
   mutable MeTensorPtr tensor_;
 };
+
+class MsTensorRelNew {
+ public:
+  explicit MsTensorRelNew(const shared_ptr<MSTensor> &tensor) : tensor_(tensor) {}
+  ~MsTensorRelNew() = default;
+  void Rel() const { tensor_ = nullptr; }
+
+ private:
+  mutable shared_ptr<MSTensor> tensor_;
+};
 }  // namespace
 
 class TensorRefData : public tensor::TensorData {
@@ -373,6 +383,57 @@ GeTensorPtr TransformUtil::ConvertTensor(const MeTensorPtr &tensor, const std::s
   return tensor_ptr;
 }
 
+GeTensorPtr TransformUtil::ConvertTensor(const std::shared_ptr<MSTensor> &tensor, const std::string &format,
+                                         bool copy) {
+  // get tensor data type size
+  MS_EXCEPTION_IF_NULL(tensor);
+  auto me_data_type = tensor->DataType();
+#ifndef ENABLE_LITE_ACL
+  if (me_data_type == DataType::kObjectTypeString) {
+    return ConvertStringTensor(tensor, format);
+  }
+#endif
+  size_t type_size = GetDataTypeSize(static_cast<const TypeId>(me_data_type));
+  if (type_size == kErrorSize) {
+    MS_LOG(ERROR) << "The Me Tensor data type size is wrong, type size is: " << type_size;
+    return nullptr;
+  }
+
+  // get tensor buff size
+  size_t data_buff_size = tensor->DataSize();
+  if (data_buff_size == 0) {
+    MS_LOG(INFO) << "The Me Tensor data buff size is 0.";
+  }
+  // create ge tensor
+  auto desc = GetGeTensorDesc(tensor->Shape(), static_cast<const TypeId>(tensor->DataType()), format);
+  if (desc == nullptr) {
+    MS_LOG(ERROR) << "Failed to get Tensor Desc";
+    return nullptr;
+  }
+  GeTensorPtr tensor_ptr = make_shared<GeTensor>(*desc);
+  if (tensor_ptr == nullptr) {
+    MS_LOG(ERROR) << "Failed to convert Me Tensor to Ge Tensor!";
+    return nullptr;
+  }
+  if (copy) {
+    auto ret = tensor_ptr->SetData(std::static_pointer_cast<const uint8_t>(tensor->Data()).get(), data_buff_size);
+    if (ret != ge::GRAPH_SUCCESS) {
+      MS_LOG(ERROR) << "Failed to call ge::Tensor SetData(const uint8_t*, size), data size " << data_buff_size;
+      return nullptr;
+    }
+  } else {
+    MsTensorRelNew rel(tensor);
+    auto ret = tensor_ptr->SetData(static_cast<uint8_t *>(tensor->MutableData()), data_buff_size,
+                                   [rel](uint8_t *) -> void { rel.Rel(); });
+    if (ret != ge::GRAPH_SUCCESS) {
+      MS_LOG(ERROR) << "Failed to call ge::Tensor SetData(uint8_t*, size, DeleteFunc), data size " << data_buff_size;
+      return nullptr;
+    }
+  }
+  MS_LOG(DEBUG) << "Convert Me Tensor to Ge Tensor success!";
+  return tensor_ptr;
+}
+
 GeTensorPtr TransformUtil::ConvertScalar(const ValuePtr &val) {
   auto ge_tensor = ConvertAnyUtil(val, AnyTraits<ValueAny>());
   return make_shared<GeTensor>(ge_tensor);
diff --git a/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.h b/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.h
index 4b4ef556bf9..7e7907b8c7b 100644
--- a/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.h
+++ b/mindspore/ccsrc/plugin/res_manager/ascend/op_adapter/transform_util.h
@@ -28,6 +28,7 @@
 #include "utils/shape_utils.h"
 #include "plugin/res_manager/ascend/op_adapter/op_adapter_base.h"
 #include "plugin/res_manager/ascend/visible.h"
+#include "include/api/types.h"
 
 namespace mindspore::device::ascend {
 class ASCEND_RES_MANAGER_EXPORT TransformUtil {
@@ -78,6 +79,9 @@ class ASCEND_RES_MANAGER_EXPORT TransformUtil {
    * */
   static GeTensorPtr ConvertTensor(const MeTensorPtr &tensor, const std::string &format, bool copy = true);
 
+  static GeTensorPtr ConvertTensor(const std::shared_ptr<MSTensor> &tensor, const std::string &format,
+                                   bool copy = true);
+
   /*
    * Parameters:
    *     me_tensors: [vector<MeTensorPtr>] the data tensors in ME
diff --git a/mindspore/ccsrc/plugin/res_manager/ascend/symbol_interface/symbol_utils.h b/mindspore/ccsrc/plugin/res_manager/ascend/symbol_interface/symbol_utils.h
index 6847efecb3e..fb7bb08db67 100644
--- a/mindspore/ccsrc/plugin/res_manager/ascend/symbol_interface/symbol_utils.h
+++ b/mindspore/ccsrc/plugin/res_manager/ascend/symbol_interface/symbol_utils.h
@@ -18,7 +18,6 @@
 #define MINDSPORE_CCSRC_TRANSFORM_SYMBOL_SYMBOL_UTILS_H_
 #include <string>
 #include "utils/log_adapter.h"
-#ifndef BUILD_LITE
 #include "acl/acl.h"
 #include "utils/ms_exception.h"
 #include "include/backend/visible.h"
@@ -28,7 +27,6 @@ extern "C" BACKEND_EXPORT FuncGetRecentErrMsg acl_get_recent_err_msg;
 
 #ifndef ACL_ERROR_RT_DEVICE_MEM_ERROR
 #define ACL_ERROR_RT_DEVICE_MEM_ERROR 507053
-#endif
 #ifndef ACL_ERROR_RT_HBM_MULTI_BIT_ECC_ERROR
 #define ACL_ERROR_RT_HBM_MULTI_BIT_ECC_ERROR 507054
 #endif
diff --git a/mindspore/ccsrc/runtime/data_queue/data_queue_mgr.cc b/mindspore/ccsrc/runtime/data_queue/data_queue_mgr.cc
index 763f69654f8..320ac09206f 100644
--- a/mindspore/ccsrc/runtime/data_queue/data_queue_mgr.cc
+++ b/mindspore/ccsrc/runtime/data_queue/data_queue_mgr.cc
@@ -250,7 +250,6 @@ DataQueueStatus DataQueueMgr::SetThreadDevice(const std::string &channel_name) c
   return DataQueueStatus::SUCCESS;
 }
 
-#ifndef BUILD_LITE
 void UpdateGetNextWithDataQueueItems(const AnfNodePtr &data_kernel, const std::vector<device::DataQueueItem> &data) {
   auto kernel_info = dynamic_cast<device::KernelInfo *>(data_kernel->kernel_info());
   std::vector<std::shared_ptr<device::DeviceAddress>> device_tensors;
@@ -333,6 +332,5 @@ void UpdateGetNextNode(const PrimitivePtr &primitive, const std::vector<kernel::
   UpdateGetNextWithDataQueueItems(inputs, outputs, data, output_size_list);
 }
 
-#endif
 }  // namespace device
 }  // namespace mindspore
diff --git a/mindspore/ccsrc/runtime/graph_scheduler/actor/actor_common.cc b/mindspore/ccsrc/runtime/graph_scheduler/actor/actor_common.cc
index 062a0e04048..29d839fe84e 100644
--- a/mindspore/ccsrc/runtime/graph_scheduler/actor/actor_common.cc
+++ b/mindspore/ccsrc/runtime/graph_scheduler/actor/actor_common.cc
@@ -27,7 +27,6 @@
 #include "include/backend/distributed/ps/ps_context.h"
 #include "include/backend/mem_reuse/mem_tracker.h"
 #include "include/common/runtime_conf/runtime_conf.h"
-#ifndef BUILD_LITE
 #include "runtime/graph_scheduler/parameter_store.h"
 #include "include/backend/distributed/recovery/recovery_context.h"
 #include "runtime/graph_scheduler/actor/kernel_async_launch_actor.h"
@@ -37,7 +36,6 @@
 #include "runtime/device/device_address_utils.h"
 #include "runtime/hardware/device_context_manager.h"
 #include "runtime/graph_scheduler/pipeline/runtime_pipeline.h"
-#endif
 #include "mindspore/ops/op_def/auto_generate/gen_ops_primitive_s.h"
 
 namespace mindspore {
@@ -375,11 +373,9 @@ bool EnableRuntimePipeline() {
     return false;
   }
 
-#ifndef BUILD_LITE
   if (distributed::recovery::RecoveryContext::GetInstance()->enable_recovery()) {
     return false;
   }
-#endif
 
   return true;
 }
@@ -415,7 +411,6 @@ size_t GetDefragMemoryStepFreq() {
 
 bool WaitRuntimePipelineFinish(const OpContext<KernelTensor> *context, const std::string &name,
                                bool wait_kernel_launch_finish) {
-#ifndef BUILD_LITE
   uint64_t start_time = 0;
   PROFILER_START(start_time);
 
@@ -443,9 +438,6 @@ bool WaitRuntimePipelineFinish(const OpContext<KernelTensor> *context, const std
     return false;
   }
   return true;
-#else
-  return true;
-#endif
 }
 
 bool Copy(const DeviceTensor *dst_device_tensor, const DeviceTensor *src_device_tensor) {
diff --git a/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_actor.cc b/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_actor.cc
index a4eb6297b02..6163ce7fab0 100644
--- a/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_actor.cc
+++ b/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_actor.cc
@@ -128,17 +128,11 @@ void AddNodeToGraphTracker(const CNodePtr cnode, const std::string &actor_name)
   std::vector<uint32_t> comm_ranks;
   if (group_name == "hccl_world_group") {
     uint32_t rank_size = 1;
-#if !defined(BUILD_LITE)
     rank_size = distributed::collective::CollectiveManager::instance()->global_rank_size();
-#endif
     comm_ranks.resize(rank_size);
     std::iota(comm_ranks.begin(), comm_ranks.end(), 0);
   } else {
-#if !defined(BUILD_LITE)
     comm_ranks = distributed::collective::CollectiveManager::instance()->GetGroupRanks(group_name);
-#else
-    comm_ranks = {0};
-#endif
   }
   std::string comm_ranks_str = std::accumulate(
     comm_ranks.begin(), comm_ranks.end(), std::string(),
diff --git a/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_runner.cc b/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_runner.cc
index c74c57f3cd4..f323861ab2c 100644
--- a/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_runner.cc
+++ b/mindspore/ccsrc/runtime/graph_scheduler/actor/kernel_runner.cc
@@ -126,17 +126,11 @@ void AddNodeToGraphTracker(const CNodePtr cnode, const std::string &actor_name)
     std::vector<uint32_t> comm_ranks;
     if (group_name == "hccl_world_group") {
       uint32_t rank_size = 1;
-#if !defined(BUILD_LITE)
       rank_size = distributed::collective::CollectiveManager::instance()->global_rank_size();
-#endif
       comm_ranks.resize(rank_size);
       std::iota(comm_ranks.begin(), comm_ranks.end(), 0);
     } else {
-#if !defined(BUILD_LITE)
       comm_ranks = distributed::collective::CollectiveManager::instance()->GetGroupRanks(group_name);
-#else
-      comm_ranks = {0};
-#endif
     }
     std::string comm_ranks_str = std::accumulate(
       comm_ranks.begin(), comm_ranks.end(), std::string(),
diff --git a/mindspore/ccsrc/runtime/graph_scheduler/execution_order_check/comm_execution_order_check.cc b/mindspore/ccsrc/runtime/graph_scheduler/execution_order_check/comm_execution_order_check.cc
index eda99129c4f..6b8d918fcbe 100644
--- a/mindspore/ccsrc/runtime/graph_scheduler/execution_order_check/comm_execution_order_check.cc
+++ b/mindspore/ccsrc/runtime/graph_scheduler/execution_order_check/comm_execution_order_check.cc
@@ -79,9 +79,7 @@ uint32_t Process::GetRankSize() {
   static uint32_t rank_size = 1;
   static bool is_initialized = false;
   if (!is_initialized) {
-#if !defined(BUILD_LITE)
     rank_size = distributed::collective::CollectiveManager::instance()->global_rank_size();
-#endif
     is_initialized = true;
   }
 
@@ -92,11 +90,9 @@ std::string Process::GetRankID() {
   static uint32_t rank_id = 0;
   static bool is_initialized = false;
   if (!is_initialized) {
-#if !defined(BUILD_LITE)
     if (distributed::collective::CollectiveManager::instance()->initialized()) {
       rank_id = CommManager::GetInstance().GetRank();
     }
-#endif
     is_initialized = true;
   }
 
@@ -462,11 +458,7 @@ void Process::FetchCommRanksCache(const std::string &group_name) {
     comm_ranks.resize(GetRankSize());
     std::iota(comm_ranks.begin(), comm_ranks.end(), 0);
   } else {
-#if !defined(BUILD_LITE)
     comm_ranks = distributed::collective::CollectiveManager::instance()->GetGroupRanks(group_name);
-#else
-    comm_ranks = {0};
-#endif
   }
   comm_rank_cache_[group_name] = comm_ranks;
 }
diff --git a/mindspore/core/include/utils/anf_utils.h b/mindspore/core/include/utils/anf_utils.h
index 8dbb05ad482..d17cf0f1012 100644
--- a/mindspore/core/include/utils/anf_utils.h
+++ b/mindspore/core/include/utils/anf_utils.h
@@ -94,6 +94,8 @@ class MS_CORE_API AnfUtils {
   static bool IsNodeInGraphKernel(const AnfNodePtr &node);
   // check whether the node is a KernelPacket node.
   static bool IsKernelPacket(const AnfNodePtr &node);
+  // check whether two node is same
+  static bool AnfEqual(const BaseRef &a, const BaseRef &b);
   // Set dump flag to CNode's primitive.
   static void SetDumpFlag(const AnfNodePtr &node);
   // Get dump flag from CNode's primitive.
diff --git a/mindspore/core/mindrt/CMakeLists.txt b/mindspore/core/mindrt/CMakeLists.txt
index 171401d4e30..c6c1b9b45cf 100644
--- a/mindspore/core/mindrt/CMakeLists.txt
+++ b/mindspore/core/mindrt/CMakeLists.txt
@@ -26,6 +26,16 @@ else()
             )
 endif()
 
+if (CMAKE_SYSTEM_NAME MATCHES "Generic")
+    set(MINDRT_SRC ${MINDRT_SRC}
+    ${CMAKE_CURRENT_SOURCE_DIR}/src/thread/threadpool_ohos.cc
+    ${CMAKE_CURRENT_SOURCE_DIR}/src/thread/core_affinity_ohos.cc)
+else()
+    set(MINDRT_SRC ${MINDRT_SRC}
+    ${CMAKE_CURRENT_SOURCE_DIR}/src/thread/core_affinity_normal.cc
+    ${CMAKE_CURRENT_SOURCE_DIR}/src/thread/threadpool_normal.cc)
+endif()
+
 if(CMAKE_SYSTEM_NAME MATCHES "Windows")
     add_compile_definitions(BUILDING_CORE_DLL)
 endif()
diff --git a/mindspore/core/mindrt/include/thread/core_affinity.h b/mindspore/core/mindrt/include/thread/core_affinity.h
index de6db97c2b8..882c6afb7cc 100644
--- a/mindspore/core/mindrt/include/thread/core_affinity.h
+++ b/mindspore/core/mindrt/include/thread/core_affinity.h
@@ -20,18 +20,6 @@
 #include <vector>
 #include <thread>
 
-#ifdef __ANDROID__
-#define BIND_CORE
-#include <sched.h>
-#endif
-#ifdef _WIN32
-#define BIND_CORE
-#endif
-// Lite not support bind core.
-#if !defined(BUILD_LITE) && defined(__linux__)
-#define BIND_CORE
-#endif
-
 namespace mindspore {
 enum BindMode {
   Power_NoBind = 0,  // free schedule
@@ -62,12 +50,8 @@ class CoreAffinity {
   static float GetServerFrequency();
 
  private:
-#ifdef _WIN32
   int SetAffinity();
-#elif defined(BIND_CORE)
   int SetAffinity(const pthread_t &thread_id, cpu_set_t *cpu_set);
-#endif
-
   int InitBindCoreId(size_t thread_num, BindMode bind_mode);
 
   int BindThreadsToCoreList(const std::vector<Worker *> &workers);
diff --git a/mindspore/core/mindrt/include/thread/threadpool.h b/mindspore/core/mindrt/include/thread/threadpool.h
index 511c3fd2b17..187b138f73c 100644
--- a/mindspore/core/mindrt/include/thread/threadpool.h
+++ b/mindspore/core/mindrt/include/thread/threadpool.h
@@ -117,7 +117,7 @@ class Worker {
 
 #ifdef _WIN32
   uint64_t core_id() { return core_id_; }
-#elif defined(BIND_CORE)
+#elif defined(__ANDROID__) || defined(_WIN32) || defined(__linux__)
   void set_mask(const cpu_set_t &mask) { mask_ = mask; }
   pthread_t handle() {
     THREAD_TEST_TRUE(thread_ == nullptr);
@@ -137,7 +137,7 @@ class Worker {
   std::unique_ptr<std::thread> thread_{nullptr};
 #ifdef _WIN32
   uint64_t core_id_;
-#elif defined(BIND_CORE)
+#elif defined(__ANDROID__) || defined(_WIN32) || defined(__linux__)
   cpu_set_t mask_;
 #endif
   std::atomic_int status_{kThreadBusy};
diff --git a/mindspore/core/mindrt/src/thread/core_affinity.cc b/mindspore/core/mindrt/src/thread/core_affinity.cc
index eeff23dffef..77ec477ad7b 100644
--- a/mindspore/core/mindrt/src/thread/core_affinity.cc
+++ b/mindspore/core/mindrt/src/thread/core_affinity.cc
@@ -309,7 +309,7 @@ std::vector<int> CoreAffinity::GetCoreId(size_t thread_num, BindMode bind_mode)
   std::vector<int> bind_id;
 #ifdef _WIN32
   return bind_id;
-#elif defined(BIND_CORE)
+#elif defined(__ANDROID__) || defined(_WIN32) || defined(__linux__)
   if (core_num_ != sorted_id_.size()) {
     THREAD_ERROR("init sorted core id failed");
     return bind_id;
@@ -340,40 +340,12 @@ int CoreAffinity::InitBindCoreId(size_t thread_num, BindMode bind_mode) {
 #endif
   return THREAD_OK;
 }
-
-#ifdef _WIN32
 int CoreAffinity::SetAffinity() { return THREAD_OK; }
-#elif defined(BIND_CORE)
-int CoreAffinity::SetAffinity(const pthread_t &thread_id, cpu_set_t *cpu_set) {
-#ifdef __ANDROID__
-#if __ANDROID_API__ >= 21
-  THREAD_INFO("thread: %d, mask: %lu", pthread_gettid_np(thread_id), cpu_set->__bits[0]);
-  int ret = sched_setaffinity(pthread_gettid_np(thread_id), sizeof(cpu_set_t), cpu_set);
-  if (ret != THREAD_OK) {
-    THREAD_ERROR("bind thread %d to cpu failed. ERROR %d", pthread_gettid_np(thread_id), ret);
-    return THREAD_ERROR;
-  }
-#endif
-#else
-#if defined(__APPLE__)
-  THREAD_ERROR("not bind thread to apple's cpu.");
-  return THREAD_ERROR;
-#else
-  int ret = pthread_setaffinity_np(thread_id, sizeof(cpu_set_t), cpu_set);
-  if (ret != THREAD_OK) {
-    THREAD_ERROR("set thread: %lu to cpu failed", thread_id);
-    return THREAD_ERROR;
-  }
-#endif  // __APPLE__
-#endif
-  return THREAD_OK;
-}
-#endif
 
 int CoreAffinity::FreeScheduleThreads(const std::vector<Worker *> &workers) {
 #ifdef _WIN32
   return THREAD_OK;
-#elif defined(BIND_CORE)
+#elif defined(__ANDROID__) || defined(_WIN32) || defined(__linux__)
   cpu_set_t mask;
   CPU_ZERO(&mask);
   for (int i : bind_id_) {
@@ -385,14 +357,14 @@ int CoreAffinity::FreeScheduleThreads(const std::vector<Worker *> &workers) {
       return THREAD_ERROR;
     }
   }
-#endif  // BIND_CORE
+#endif
   return THREAD_OK;
 }
 
 int CoreAffinity::BindThreadsToCoreList(const std::vector<Worker *> &workers) {
 #ifdef _WIN32
   return THREAD_OK;
-#elif defined(BIND_CORE)
+#elif defined(__ANDROID__) || defined(__linux__)
   if (bind_id_.empty()) {
     THREAD_INFO("bind id is empty, it will not bind thread");
     return THREAD_OK;
@@ -411,14 +383,14 @@ int CoreAffinity::BindThreadsToCoreList(const std::vector<Worker *> &workers) {
     THREAD_INFO("set thread[%zu] affinity to core[%d] success", i, bind_id_[i % window]);
     workers[i]->set_frequency(core_freq_[bind_id_[i]]);
   }
-#endif  // BIND_CORE
+#endif  // __ANDROID__ __linux__
   return THREAD_OK;
 }
 
 int CoreAffinity::BindProcess(BindMode bind_mode) {
 #ifdef _WIN32
   return THREAD_OK;
-#elif defined(BIND_CORE)
+#elif defined(__ANDROID__) || defined(__linux__)
   if (bind_id_.empty()) {
     // initializes bind id before bind currently process
     THREAD_ERROR("bind id is empty");
@@ -436,7 +408,7 @@ int CoreAffinity::BindProcess(BindMode bind_mode) {
   return SetAffinity(pthread_self(), &mask);
 #else
   return THREAD_OK;
-#endif  // BIND_CORE
+#endif  // __ANDROID__ __linux__
 }
 
 int CoreAffinity::BindThreads(const std::vector<Worker *> &workers, BindMode bind_mode) {
diff --git a/mindspore/core/mindrt/src/thread/core_affinity_normal.cc b/mindspore/core/mindrt/src/thread/core_affinity_normal.cc
new file mode 100644
index 00000000000..3b3a9b8967d
--- /dev/null
+++ b/mindspore/core/mindrt/src/thread/core_affinity_normal.cc
@@ -0,0 +1,57 @@
+/**
+ * Copyright 2021-2023 Huawei Technologies Co., Ltd
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "thread/core_affinity.h"
+#include <string.h>
+#include <cstdlib>
+#include <string>
+#include <algorithm>
+#ifdef MS_COMPILE_IOS
+#include <sys/types.h>
+#include <sys/sysctl.h>
+#include <mach/machine.h>
+#endif  // MS_COMPILE_IOS
+#include "thread/threadpool.h"
+#ifdef _WIN32
+#include <windows.h>
+#endif
+
+namespace mindspore {
+int CoreAffinity::SetAffinity(const pthread_t &thread_id, cpu_set_t *cpu_set) {
+#ifdef __ANDROID__
+#if __ANDROID_API__ >= 21
+  THREAD_INFO("thread: %d, mask: %lu", pthread_gettid_np(thread_id), cpu_set->__bits[0]);
+  int ret = sched_setaffinity(pthread_gettid_np(thread_id), sizeof(cpu_set_t), cpu_set);
+  if (ret != THREAD_OK) {
+    THREAD_ERROR("bind thread %d to cpu failed. ERROR %d", pthread_gettid_np(thread_id), ret);
+    return THREAD_ERROR;
+  }
+#endif
+#else
+#if defined(__APPLE__)
+  THREAD_ERROR("not bind thread to apple's cpu.");
+  return THREAD_ERROR;
+#else
+  int ret = pthread_setaffinity_np(thread_id, sizeof(cpu_set_t), cpu_set);
+  if (ret != THREAD_OK) {
+    THREAD_ERROR("set thread: %lu to cpu failed", thread_id);
+    return THREAD_ERROR;
+  }
+#endif  // __APPLE__
+#endif
+  return THREAD_OK;
+}
+}  // namespace mindspore
diff --git a/mindspore/core/mindrt/src/thread/core_affinity_ohos.cc b/mindspore/core/mindrt/src/thread/core_affinity_ohos.cc
new file mode 100644
index 00000000000..44f20bcd9f0
--- /dev/null
+++ b/mindspore/core/mindrt/src/thread/core_affinity_ohos.cc
@@ -0,0 +1,36 @@
+/**
+ * Copyright 2021-2023 Huawei Technologies Co., Ltd
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "thread/core_affinity.h"
+#include <string.h>
+#include <cstdlib>
+#include <string>
+#include <algorithm>
+#ifdef MS_COMPILE_IOS
+#include <sys/types.h>
+#include <sys/sysctl.h>
+#include <mach/machine.h>
+#endif  // MS_COMPILE_IOS
+#include "thread/threadpool.h"
+#ifdef _WIN32
+#include <windows.h>
+#endif
+
+namespace mindspore {
+int CoreAffinity::SetAffinity(const pthread_t &thread_id, cpu_set_t *cpu_set) {
+  return THREAD_OK;
+}
+}  // namespace mindspore
diff --git a/mindspore/core/mindrt/src/thread/threadpool.cc b/mindspore/core/mindrt/src/thread/threadpool.cc
index ea84f779812..452c5ee8389 100644
--- a/mindspore/core/mindrt/src/thread/threadpool.cc
+++ b/mindspore/core/mindrt/src/thread/threadpool.cc
@@ -67,7 +67,7 @@ void Worker::ChildAfterFork() {
   }
 }
 
-#if defined(BIND_CORE) && !defined(__ANDROID__) && !defined(__APPLE__) && !defined(_MSC_VER) && !defined(_WIN32)
+#if defined(__linux__) && !defined(_MSC_VER)
 std::string MaskToStr(const cpu_set_t *mask) {
   std::stringstream ss;
   size_t cpu_num = static_cast<size_t>(sysconf(_SC_NPROCESSORS_ONLN));
@@ -78,184 +78,12 @@ std::string MaskToStr(const cpu_set_t *mask) {
 }
 #endif
 
-void Worker::SetAffinity() {
-#ifdef _WIN32
-  SetWindowsSelfAffinity(core_id_);
-#elif defined(BIND_CORE)
-#ifdef __ANDROID__
-  int ret = sched_setaffinity(gettid(), sizeof(cpu_set_t), &mask_);
-  if (ret != THREAD_OK) {
-    THREAD_ERROR("bind thread %d to cpu failed. ERROR %d", gettid(), errno);
-  }
-  return;
-#else
-#if !defined(__APPLE__) && !defined(_MSC_VER)
-
-  THREAD_INFO("Worker pthread_setaffinity_np, mask %s", MaskToStr(&mask_));
-  int ret = pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &mask_);
-  if (ret != THREAD_OK) {
-    THREAD_ERROR("bind thread %lu to cpu failed. ERROR %d", pthread_self(), errno);
-  }
-  return;
-#endif
-#endif
-#endif
-}
-
-std::vector<int> parse_cpu_list(const std::string &cpu_str) {
-  std::vector<int> cpus;
-  std::stringstream ss(cpu_str);
-  std::string item;
-
-  while (std::getline(ss, item, ',')) {
-    cpus.push_back(std::stoi(item));
-  }
-  return cpus;
-}
-
-#if defined(__linux__)
-void ThreadPool::ThreadPoolSetAffinity(const std::vector<pthread_t> &threads) {
-#if defined(BIND_CORE) && !defined(__ANDROID__)
-  auto thread_num = threads.size();
-  MS_LOG(INFO) << "Start to bind core for actor thread for [" << thread_num << "] threads.";
-  auto env_runtime_reserved = std::getenv("CONFIG_BIND_RUNTIME_LIST");
-  if (env_runtime_reserved == nullptr) {
-    return;
-  }
-
-  MS_LOG(WARNING) << "Start to bind core base on CONFIG_BIND_RUNTIME_LIST.";
-
-  std::vector<int> cpu_list = parse_cpu_list(std::string(env_runtime_reserved));
-  if (cpu_list.empty()) {
-    MS_LOG(WARNING) << "Cpu list is empty, bind core is not enabled.";
-    return;
-  }
-  int ret;
-  auto env_enable_fix = std::getenv("ACTOR_THREAD_FIX_BIND");
-  if (env_enable_fix != nullptr && (std::string(env_enable_fix) == "false" || std::string(env_enable_fix) == "False")) {
-    cpu_set_t cpuset;
-    CPU_ZERO(&cpuset);
-
-    for (const auto &cpu_id : cpu_list) {
-      CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
-    }
-
-    for (size_t i = 0; i < thread_num; i++) {
-      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
-      if (ret != 0) {
-        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list << " for thread " << threads[i];
-      } else {
-        MS_LOG(WARNING) << "Success to bind core to " << cpu_list << " for thread " << threads[i];
-      }
-    }
-  } else if (env_enable_fix != nullptr && (std::string(env_enable_fix) == "cluster")) {
-    int cpus_per_thread = cpu_list.size() / thread_num;
-    int offset = 0;
-
-    for (size_t i = 0; i < thread_num; i++) {
-      cpu_set_t cpuset;
-      CPU_ZERO(&cpuset);
-      std::vector<int> sub_list(cpu_list.begin() + offset, cpu_list.begin() + offset + cpus_per_thread);
-      for (const auto &cpu_id : sub_list) {
-        CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
-      }
-      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
-      if (ret != 0) {
-        MS_LOG(WARNING) << "Fail to bind core to " << sub_list << " for thread " << threads[i];
-      } else {
-        MS_LOG(WARNING) << "Success to bind core to " << sub_list << " for thread " << threads[i];
-      }
-      offset += cpus_per_thread;
-    }
-  } else {
-    for (size_t i = 0; i < thread_num; i++) {
-      cpu_set_t cpuset;
-      CPU_ZERO(&cpuset);
-      CPU_SET(cpu_list[i % cpu_list.size()], &cpuset);
-
-      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
-      if (ret != 0) {
-        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list[i % cpu_list.size()] << " for thread " << threads[i];
-      } else {
-        MS_LOG(WARNING) << "Success to bind core to " << cpu_list[i % cpu_list.size()] << " for thread " << threads[i];
-      }
-    }
-  }
-#else
-  MS_LOG(ERROR) << "Call not implemented function.";
-  MINDRT_EXIT("Not implemented error");
-#endif
-}
-
-void ThreadPool::APIThreadPoolSetAffinity(const std::vector<pthread_t> &threads, const std::vector<int> &cpu_list,
-                                          const std::string actor_thread_fix_bind) {
-#if defined(BIND_CORE) && !defined(__ANDROID__)
-  auto thread_num = threads.size();
-  MS_LOG(INFO) << "Start to bind core for actor thread for [" << thread_num << "] threads.";
-  int ret;
-  if (!actor_thread_fix_bind.empty() &&
-      (std::string(actor_thread_fix_bind) == "true" || std::string(actor_thread_fix_bind) == "True")) {
-    for (size_t i = 0; i < thread_num; i++) {
-      cpu_set_t cpuset;
-      CPU_ZERO(&cpuset);
-      CPU_SET(cpu_list[i % cpu_list.size()], &cpuset);
-
-      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
-      if (ret != 0) {
-        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list[i % cpu_list.size()] << " for thread " << threads[i];
-      } else {
-        MS_LOG(WARNING) << "Success to bind core to " << cpu_list[i % cpu_list.size()] << " for thread " << threads[i];
-      }
-    }
-  } else if (!actor_thread_fix_bind.empty() && (std::string(actor_thread_fix_bind) == "cluster")) {
-    int cpus_per_thread = cpu_list.size() / thread_num;
-    int offset = 0;
-
-    for (size_t i = 0; i < thread_num; i++) {
-      cpu_set_t cpuset;
-      CPU_ZERO(&cpuset);
-      std::vector<int> sub_list(cpu_list.begin() + offset, cpu_list.begin() + offset + cpus_per_thread);
-      for (const auto &cpu_id : sub_list) {
-        CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
-      }
-      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
-      if (ret != 0) {
-        MS_LOG(WARNING) << "Fail to bind core to " << sub_list << " for thread " << threads[i];
-      } else {
-        MS_LOG(WARNING) << "Success to bind core to " << sub_list << " for thread " << threads[i];
-      }
-      offset += cpus_per_thread;
-    }
-  } else {
-    cpu_set_t cpuset;
-    CPU_ZERO(&cpuset);
-
-    for (const auto &cpu_id : cpu_list) {
-      CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
-    }
-
-    for (size_t i = 0; i < thread_num; i++) {
-      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
-      if (ret != 0) {
-        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list << " for thread " << threads[i];
-      } else {
-        MS_LOG(WARNING) << "Success to bind core to " << cpu_list << " for thread " << threads[i];
-      }
-    }
-  }
-#else
-  MS_LOG(ERROR) << "Call not implemented function.";
-  MINDRT_EXIT("Not implemented error");
-#endif
-}
-#endif
-
 void Worker::InitWorkerMask(const std::vector<int> &core_list, const size_t workers_size) {
   core_list_ = core_list;
 #ifdef _WIN32
   static uint32_t windows_core_index = 0;
   core_id_ = windows_core_index++;
-#elif defined(BIND_CORE)
+#elif defined(__ANDROID__) || defined(__linux__)
   if (core_list.empty()) {
     return;
   }
@@ -604,7 +432,7 @@ Worker *ThreadPool::CurrentWorker() const {
 }
 
 int ThreadPool::InitAffinityInfo() {
-#ifdef BIND_CORE
+#if defined(__ANDROID__) || defined(_WIN32) || defined(__linux__)
   affinity_ = new (std::nothrow) CoreAffinity();
   THREAD_ERROR_IF_NULL(affinity_);
   int ret = affinity_->InitHardwareCoreInfo();
diff --git a/mindspore/core/mindrt/src/thread/threadpool_normal.cc b/mindspore/core/mindrt/src/thread/threadpool_normal.cc
new file mode 100644
index 00000000000..57d2d4acfe3
--- /dev/null
+++ b/mindspore/core/mindrt/src/thread/threadpool_normal.cc
@@ -0,0 +1,194 @@
+/**
+ * Copyright 2021-2023 Huawei Technologies Co., Ltd
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef _MSC_VER
+#include <sched.h>
+#include <unistd.h>
+#endif
+#include <iostream>
+#include <vector>
+#include <cstdlib>
+#include <cstring>
+#include <algorithm>
+#include <sstream>
+#include "thread/threadpool.h"
+#include "thread/core_affinity.h"
+
+namespace mindspore {
+std::vector<int> parse_cpu_list(const std::string &cpu_str) {
+  std::vector<int> cpus;
+  std::stringstream ss(cpu_str);
+  std::string item;
+
+  while (std::getline(ss, item, ',')) {
+    cpus.push_back(std::stoi(item));
+  }
+  return cpus;
+}
+
+void Worker::SetAffinity() {
+#ifdef _WIN32
+  SetWindowsSelfAffinity(core_id_);
+#elif defined(__ANDROID__)
+  int ret = sched_setaffinity(gettid(), sizeof(cpu_set_t), &mask_);
+  if (ret != THREAD_OK) {
+    THREAD_ERROR("bind thread %d to cpu failed. ERROR %d", gettid(), errno);
+  }
+  return;
+#else
+#if !defined(__APPLE__) && !defined(_MSC_VER)
+  THREAD_INFO("Worker pthread_setaffinity_np, mask %s", MaskToStr(&mask_));
+  int ret = pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &mask_);
+  if (ret != THREAD_OK) {
+    THREAD_ERROR("bind thread %lu to cpu failed. ERROR %d", pthread_self(), errno);
+  }
+  return;
+#endif
+#endif
+}
+
+void ThreadPool::ThreadPoolSetAffinity(const std::vector<pthread_t> &threads) {
+  auto thread_num = threads.size();
+  MS_LOG(INFO) << "Start to bind core for actor thread for [" << thread_num << "] threads.";
+#if defined(__linux__) && !defined(__ANDROID__) && !defined(__APPLE__) && !defined(_MSC_VER) && !defined(_WIN32)
+  auto env_runtime_reserved = std::getenv("CONFIG_BIND_RUNTIME_LIST");
+  if (env_runtime_reserved == nullptr) {
+    return;
+  }
+
+  MS_LOG(WARNING) << "Start to bind core base on CONFIG_BIND_RUNTIME_LIST.";
+
+  std::vector<int> cpu_list = parse_cpu_list(std::string(env_runtime_reserved));
+  if (cpu_list.empty()) {
+    MS_LOG(WARNING) << "Cpu list is empty, bind core is not enabled.";
+    return;
+  }
+  int ret;
+  auto env_enable_fix = std::getenv("ACTOR_THREAD_FIX_BIND");
+  if (env_enable_fix != nullptr && (std::string(env_enable_fix) == "false" || std::string(env_enable_fix) == "False")) {
+    cpu_set_t cpuset;
+    CPU_ZERO(&cpuset);
+
+    for (const auto &cpu_id : cpu_list) {
+      CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
+    }
+
+    for (size_t i = 0; i < thread_num; i++) {
+      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
+      if (ret != 0) {
+        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list << " for thread " << threads[i];
+      } else {
+        MS_LOG(WARNING) << "Success to bind core to " << cpu_list << " for thread " << threads[i];
+      }
+    }
+  } else if (env_enable_fix != nullptr && (std::string(env_enable_fix) == "cluster")) {
+    int cpus_per_thread = cpu_list.size() / thread_num;
+    int offset = 0;
+
+    for (size_t i = 0; i < thread_num; i++) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      std::vector<int> sub_list(cpu_list.begin() + offset, cpu_list.begin() + offset + cpus_per_thread);
+      for (const auto &cpu_id : sub_list) {
+        CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
+      }
+      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
+      if (ret != 0) {
+        MS_LOG(WARNING) << "Fail to bind core to " << sub_list << " for thread " << threads[i];
+      } else {
+        MS_LOG(WARNING) << "Success to bind core to " << sub_list << " for thread " << threads[i];
+      }
+      offset += cpus_per_thread;
+    }
+  } else {
+    for (size_t i = 0; i < thread_num; i++) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      CPU_SET(cpu_list[i % cpu_list.size()], &cpuset);
+
+      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
+      if (ret != 0) {
+        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list[i % cpu_list.size()] << " for thread "
+                        << threads[i];
+      } else {
+        MS_LOG(WARNING) << "Success to bind core to " << cpu_list[i % cpu_list.size()] << " for thread "
+                        << threads[i];
+      }
+    }
+  }
+#endif
+}
+
+void ThreadPool::APIThreadPoolSetAffinity(const std::vector<pthread_t> &threads, const std::vector<int> &cpu_list,
+                                          const std::string actor_thread_fix_bind) {
+  auto thread_num = threads.size();
+  MS_LOG(INFO) << "Start to bind core for actor thread for [" << thread_num << "] threads.";
+#if defined(__linux__) && !defined(__ANDROID__) && !defined(__APPLE__) && !defined(_MSC_VER) && !defined(_WIN32)
+  int ret;
+  if (!actor_thread_fix_bind.empty() &&
+      (std::string(actor_thread_fix_bind) == "true" || std::string(actor_thread_fix_bind) == "True")) {
+    for (size_t i = 0; i < thread_num; i++) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      CPU_SET(cpu_list[i % cpu_list.size()], &cpuset);
+
+      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
+      if (ret != 0) {
+        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list[i % cpu_list.size()] << " for thread "
+                        << threads[i];
+      } else {
+        MS_LOG(WARNING) << "Success to bind core to " << cpu_list[i % cpu_list.size()] << " for thread "
+                        << threads[i];
+      }
+    }
+  } else if (!actor_thread_fix_bind.empty() && (std::string(actor_thread_fix_bind) == "cluster")) {
+    int cpus_per_thread = cpu_list.size() / thread_num;
+    int offset = 0;
+
+    for (size_t i = 0; i < thread_num; i++) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      std::vector<int> sub_list(cpu_list.begin() + offset, cpu_list.begin() + offset + cpus_per_thread);
+      for (const auto &cpu_id : sub_list) {
+        CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
+      }
+      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
+      if (ret != 0) {
+        MS_LOG(WARNING) << "Fail to bind core to " << sub_list << " for thread " << threads[i];
+      } else {
+        MS_LOG(WARNING) << "Success to bind core to " << sub_list << " for thread " << threads[i];
+      }
+      offset += cpus_per_thread;
+    }
+  } else {
+    cpu_set_t cpuset;
+    CPU_ZERO(&cpuset);
+
+    for (const auto &cpu_id : cpu_list) {
+      CPU_SET(static_cast<size_t>(cpu_id), &cpuset);
+    }
+
+    for (size_t i = 0; i < thread_num; i++) {
+      ret = pthread_setaffinity_np(threads[i], sizeof(cpu_set_t), &cpuset);
+      if (ret != 0) {
+        MS_LOG(WARNING) << "Fail to bind core to " << cpu_list << " for thread " << threads[i];
+      } else {
+        MS_LOG(WARNING) << "Success to bind core to " << cpu_list << " for thread " << threads[i];
+      }
+    }
+  }
+#endif
+}
+}  // namespace mindspore
diff --git a/mindspore/core/mindrt/src/thread/threadpool_ohos.cc b/mindspore/core/mindrt/src/thread/threadpool_ohos.cc
new file mode 100644
index 00000000000..137a87e5c61
--- /dev/null
+++ b/mindspore/core/mindrt/src/thread/threadpool_ohos.cc
@@ -0,0 +1,48 @@
+/**
+ * Copyright 2021-2023 Huawei Technologies Co., Ltd
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef _MSC_VER
+#include <sched.h>
+#include <unistd.h>
+#endif
+#include <iostream>
+#include <vector>
+#include <cstdlib>
+#include <cstring>
+#include <algorithm>
+#include <sstream>
+#include "thread/threadpool.h"
+#include "thread/core_affinity.h"
+
+namespace mindspore {
+void Worker::SetAffinity() {
+  int ret = sched_setaffinity(gettid(), sizeof(cpu_set_t), &mask_);
+  if (ret != THREAD_OK) {
+    THREAD_ERROR("bind thread %d to cpu failed. ERROR %d", gettid(), errno);
+  }
+  return;
+}
+
+void ThreadPool::ThreadPoolSetAffinity(const std::vector<pthread_t> &threads) {
+  auto thread_num = threads.size();
+  MS_LOG(INFO) << "Start to bind core for actor thread for [" << thread_num << "] threads.";
+}
+
+void ThreadPool::APIThreadPoolSetAffinity(const std::vector<pthread_t> &threads, const std::vector<int> &cpu_list,
+                                          const std::string actor_thread_fix_bind) {
+  auto thread_num = threads.size();
+  MS_LOG(INFO) << "Start to bind core for actor thread for [" << thread_num << "] threads.";
+}
+}  // namespace mindspore
diff --git a/mindspore/core/utils/anf_utils.cc b/mindspore/core/utils/anf_utils.cc
index c1e5aaae4d2..b11dddff168 100644
--- a/mindspore/core/utils/anf_utils.cc
+++ b/mindspore/core/utils/anf_utils.cc
@@ -480,6 +480,65 @@ bool AnfUtils::IsKernelPacket(const AnfNodePtr &node) {
   return func_graph != nullptr && func_graph->has_attr(FUNC_GRAPH_ATTR_KERNEL_PACKET);
 }
 
+bool AnfUtils::AnfEqual(const BaseRef &a, const BaseRef &b) {
+  if (utils::isa<AnfNodePtr>(a) && utils::isa<AnfNodePtr>(b)) {
+    auto a_node = utils::cast<AnfNodePtr>(a);
+    auto b_node = utils::cast<AnfNodePtr>(b);
+    MS_EXCEPTION_IF_NULL(a_node);
+    MS_EXCEPTION_IF_NULL(b_node);
+    if (IsValueNode<Primitive>(a_node) && IsValueNode<Primitive>(b_node)) {
+      auto a_value_node = a_node->cast<ValueNodePtr>();
+      MS_EXCEPTION_IF_NULL(a_value_node);
+      auto a_value = a_value_node->value();
+      MS_EXCEPTION_IF_NULL(a_value);
+      auto a_prim = a_value->cast<PrimitivePtr>();
+      MS_EXCEPTION_IF_NULL(a_prim);
+
+      auto b_value_node = b_node->cast<ValueNodePtr>();
+      MS_EXCEPTION_IF_NULL(b_value_node);
+      auto b_value = b_value_node->value();
+      MS_EXCEPTION_IF_NULL(b_value);
+      auto b_prim = b_value->cast<PrimitivePtr>();
+      MS_EXCEPTION_IF_NULL(b_prim);
+
+      return a_prim->name() == b_prim->name();
+    } else if (a_node->isa<ValueNode>() && b_node->isa<ValueNode>()) {
+      auto a_value_node_ptr = a_node->cast<ValueNodePtr>();
+      if (a_value_node_ptr == nullptr) {
+        MS_LOG(INTERNAL_EXCEPTION) << "Cast value node ptr fail, node: " << a_node->DebugString();
+      }
+      auto a_value_ptr = a_value_node_ptr->value();
+      if (a_value_ptr == nullptr) {
+        MS_LOG(INTERNAL_EXCEPTION) << "Value ptr is nullptr, node: " << a_node->DebugString();
+      }
+
+      auto b_value_node_ptr = b_node->cast<ValueNodePtr>();
+      if (b_value_node_ptr == nullptr) {
+        MS_LOG(INTERNAL_EXCEPTION) << "Cast value node ptr fail, node: " << b_node->DebugString();
+      }
+      auto b_value_ptr = b_value_node_ptr->value();
+      if (b_value_ptr == nullptr) {
+        MS_LOG(INTERNAL_EXCEPTION) << "Value ptr is nullptr, node: " << b_node->DebugString();
+      }
+      if (a_value_ptr->isa<tensor::Tensor>() && b_value_ptr->isa<tensor::Tensor>()) {
+        auto a_tensor_ptr = a_value_ptr->cast<tensor::TensorPtr>();
+        auto b_tensor_ptr = b_value_ptr->cast<tensor::TensorPtr>();
+        if (a_tensor_ptr == nullptr || b_tensor_ptr == nullptr) {
+          MS_LOG(INTERNAL_EXCEPTION) << "Cast value node ptr fail.";
+        }
+        return a_tensor_ptr->ValueEqual(*b_tensor_ptr);
+      } else {
+        return (*a_value_ptr) == (*b_value_ptr);
+      }
+    }
+    MS_LOG(DEBUG) << "check AnfNodePtr equal";
+  }
+  if (utils::isa<FuncGraphPtr>(a) && utils::isa<FuncGraphPtr>(b)) {
+    MS_LOG(DEBUG) << "check GraphPtr equal";
+  }
+  return a == b;
+}
+
 bool AnfUtils::IsNodeInGraphKernel(const AnfNodePtr &node) {
   MS_EXCEPTION_IF_NULL(node);
   return node->func_graph() != nullptr && node->func_graph()->has_attr(FUNC_GRAPH_ATTR_GRAPH_KERNEL);
diff --git a/mindspore/ops/kernel/cpu/nnacl/CMakeLists.txt b/mindspore/ops/kernel/cpu/nnacl/CMakeLists.txt
index a43057a612f..a43577439cd 100644
--- a/mindspore/ops/kernel/cpu/nnacl/CMakeLists.txt
+++ b/mindspore/ops/kernel/cpu/nnacl/CMakeLists.txt
@@ -241,7 +241,7 @@ if(PLATFORM_ARM)
         COMPILE_FLAGS "${CMAKE_C_FLAGS} -fno-fast-math")
 endif()
 
-add_library(nnacl_mid OBJECT ${KERNEL_SRC} ${TRAIN_SRC} ${ASSEMBLY_SRC} ${MS_X86_SIMD_SRC})
+add_library(nnacl_mid OBJECT ${KERNEL_SRC} ${ASSEMBLY_SRC} ${MS_X86_SIMD_SRC})
 
 if("${CMAKE_BUILD_TYPE}" STREQUAL "Debug")
     target_compile_definitions(nnacl_mid PRIVATE ENABLE_DEBUG)
diff --git a/mindspore/ops/kernel/cpu/nnacl/kernel/arithmetic.c b/mindspore/ops/kernel/cpu/nnacl/kernel/arithmetic.c
index 8bf95cf2b04..d63d484ba29 100644
--- a/mindspore/ops/kernel/cpu/nnacl/kernel/arithmetic.c
+++ b/mindspore/ops/kernel/cpu/nnacl/kernel/arithmetic.c
@@ -385,7 +385,7 @@ int ArithmeticBroadCastConstTensor(ArithmeticStruct *arithmetic) {
 
   CalcStructMultiplesAndStrides(arithmetic);
 
-#ifdef PARALLEL_INFERENCE
+#ifdef MSLITE_ENABLE_CLOUD_INFERENCE
   bool prefer_explicit_broadcast = false;
 #else
   bool prefer_explicit_broadcast = arithmetic->ndim_ != 1;
diff --git a/tests/ut/cpp/pre_activate/common/fast_pattern_to_pattern_pass_test.cc b/tests/ut/cpp/pre_activate/common/fast_pattern_to_pattern_pass_test.cc
index bf617d7f116..3684b107b62 100644
--- a/tests/ut/cpp/pre_activate/common/fast_pattern_to_pattern_pass_test.cc
+++ b/tests/ut/cpp/pre_activate/common/fast_pattern_to_pattern_pass_test.cc
@@ -198,19 +198,19 @@ TEST_F(TestFastPatternToPatternPass, Mul0) {
   ASSERT_TRUE(check.build_pattern_map(new_node));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("a"), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("b"), b));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("c"), c));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("b"), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("c"), c));
   ASSERT_EQ(check.m_->Get("bc")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(1), b));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(2), c));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(1), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(2), c));
   ASSERT_EQ(check.m_->Get("mul")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kMulOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), check.m_->Get("bc")));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), check.m_->Get("bc")));
 }
 
 /// Feature: Fast PatternToPattern Pass
@@ -313,28 +313,28 @@ TEST_F(TestFastPatternToPatternPass, Mul0NotRoot) {
   ASSERT_TRUE(check.build_pattern_map(add1));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("a"), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("b"), b));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("c"), c));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("d"), d));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("b"), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("c"), c));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("d"), d));
 
   ASSERT_EQ(check.m_->Get("bc")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(1), b));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(2), c));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(1), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(2), c));
 
   ASSERT_EQ(check.m_->Get("mul")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kMulOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), check.m_->Get("bc")));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), check.m_->Get("bc")));
 
   ASSERT_EQ(check.m_->Get("add1")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(1), check.m_->Get("mul")));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(2), d));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(1), check.m_->Get("mul")));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(2), d));
 }
 
 /// Feature: Fast PatternToPattern Pass
@@ -452,21 +452,21 @@ TEST_F(TestFastPatternToPatternPass, Mul1) {
   ASSERT_TRUE(check.build_pattern_map(add1));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("a"), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("d"), d));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("e"), e));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("d"), d));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("e"), e));
 
   ASSERT_EQ(check.m_->Get("ad")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("ad")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("ad")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kMulOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("ad")->cast<CNodePtr>()->input(1), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("ad")->cast<CNodePtr>()->input(2), d));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("ad")->cast<CNodePtr>()->input(1), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("ad")->cast<CNodePtr>()->input(2), d));
 
   ASSERT_EQ(check.m_->Get("add1")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(1), check.m_->Get("ad")));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(2), e));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(1), check.m_->Get("ad")));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add1")->cast<CNodePtr>()->input(2), e));
 }
 
 namespace {
@@ -524,35 +524,35 @@ void Check1(const TestFastMul2 &pass, const FuncGraphIndexPtr &fg, const std::ma
 }
 
 void Check2(const CheckPattern &check, const std::map<std::string, AnfNodePtr> &node_map) {
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kA), node_map.at(kA)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kB), node_map.at(kB)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kC), node_map.at(kC)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kD), node_map.at(kD)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kE), node_map.at(kE)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kA), node_map.at(kA)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kB), node_map.at(kB)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kC), node_map.at(kC)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kD), node_map.at(kD)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kE), node_map.at(kE)));
 
   ASSERT_EQ(check.m_->Get(kAAddB)->cast<CNodePtr>()->size(), kThree);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kAAddB)->cast<CNodePtr>()->input(kZero),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kAAddB)->cast<CNodePtr>()->input(kZero),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kAAddB)->cast<CNodePtr>()->input(kOne), node_map.at(kA)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kAAddB)->cast<CNodePtr>()->input(kTwo), node_map.at(kB)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kAAddB)->cast<CNodePtr>()->input(kOne), node_map.at(kA)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kAAddB)->cast<CNodePtr>()->input(kTwo), node_map.at(kB)));
 
   ASSERT_EQ(check.m_->Get(kCAddD)->cast<CNodePtr>()->size(), kThree);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kCAddD)->cast<CNodePtr>()->input(kZero),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kCAddD)->cast<CNodePtr>()->input(kZero),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kCAddD)->cast<CNodePtr>()->input(kOne), node_map.at(kC)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kCAddD)->cast<CNodePtr>()->input(kTwo), node_map.at(kD)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kCAddD)->cast<CNodePtr>()->input(kOne), node_map.at(kC)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kCAddD)->cast<CNodePtr>()->input(kTwo), node_map.at(kD)));
 
   ASSERT_EQ(check.m_->Get(kMul)->cast<CNodePtr>()->size(), kThree);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kMul)->cast<CNodePtr>()->input(kZero),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kMul)->cast<CNodePtr>()->input(kZero),
                             NewValueNode(std::make_shared<Primitive>(kMulOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kMul)->cast<CNodePtr>()->input(kOne), node_map.at(kCAddD)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kMul)->cast<CNodePtr>()->input(kTwo), node_map.at(kAAddB)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kMul)->cast<CNodePtr>()->input(kOne), node_map.at(kCAddD)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kMul)->cast<CNodePtr>()->input(kTwo), node_map.at(kAAddB)));
 
   ASSERT_EQ(check.m_->Get(kAdd)->cast<CNodePtr>()->size(), kThree);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kAdd)->cast<CNodePtr>()->input(kZero),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kAdd)->cast<CNodePtr>()->input(kZero),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kAdd)->cast<CNodePtr>()->input(kOne), check.m_->Get(kMul)));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get(kAdd)->cast<CNodePtr>()->input(kTwo), node_map.at(kE)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kAdd)->cast<CNodePtr>()->input(kOne), check.m_->Get(kMul)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get(kAdd)->cast<CNodePtr>()->input(kTwo), node_map.at(kE)));
 }
 }  // namespace
 
diff --git a/tests/ut/cpp/pre_activate/common/pattern_to_pattern_pass_test.cc b/tests/ut/cpp/pre_activate/common/pattern_to_pattern_pass_test.cc
index 0d1c651e24d..9a3cfc2c946 100644
--- a/tests/ut/cpp/pre_activate/common/pattern_to_pattern_pass_test.cc
+++ b/tests/ut/cpp/pre_activate/common/pattern_to_pattern_pass_test.cc
@@ -223,19 +223,19 @@ TEST_F(TestPatternToPatternPass, Mul0) {
   ASSERT_TRUE(check.build_pattern_map(new_node));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("a"), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("b"), b));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("c"), c));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("b"), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("c"), c));
   ASSERT_EQ(check.m_->Get("bc")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(1), b));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(2), c));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(1), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("bc")->cast<CNodePtr>()->input(2), c));
   ASSERT_EQ(check.m_->Get("mul")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kMulOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), check.m_->Get("bc")));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), check.m_->Get("bc")));
 }
 
 /// Feature: PatternToPattern Pass
@@ -263,13 +263,13 @@ TEST_F(TestPatternToPatternPass, Mul1) {
   ASSERT_TRUE(check.build_pattern_map(new_node));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("a"), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("b"), NewValueNode(1)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("b"), NewValueNode(1)));
   ASSERT_EQ(check.m_->Get("mul")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kMulOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), NewValueNode(1)));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), NewValueNode(1)));
 }
 
 /// Feature: PatternToPattern Pass
@@ -297,13 +297,13 @@ TEST_F(TestPatternToPatternPass, Mul2) {
   ASSERT_TRUE(check.build_pattern_map(new_node));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("a"), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("b"), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("b"), b));
   ASSERT_EQ(check.m_->Get("mul")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kMulOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), b));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(1), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("mul")->cast<CNodePtr>()->input(2), a));
 }
 
 /// Feature: PatternToPattern Pass
@@ -331,13 +331,13 @@ TEST_F(TestPatternToPatternPass, Mul3) {
   ASSERT_TRUE(check.build_pattern_map(new_node));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("a"), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("b"), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("b"), b));
   ASSERT_EQ(check.m_->Get("add")->cast<CNodePtr>()->size(), 3);
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add")->cast<CNodePtr>()->input(0),
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add")->cast<CNodePtr>()->input(0),
                             NewValueNode(std::make_shared<Primitive>(kAddOpName))));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add")->cast<CNodePtr>()->input(1), a));
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("add")->cast<CNodePtr>()->input(2), b));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add")->cast<CNodePtr>()->input(1), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("add")->cast<CNodePtr>()->input(2), b));
 }
 
 /// Feature: PatternToPattern Pass
@@ -361,7 +361,7 @@ TEST_F(TestPatternToPatternPass, EmptySeq) {
   ASSERT_TRUE(check.build_pattern_map(new_node));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(check.m_->Get("c_a"), a));
+  ASSERT_TRUE(AnfUtils::AnfEqual(check.m_->Get("c_a"), a));
 }
 
 /// Feature: PatternToPattern Pass
diff --git a/tests/ut/cpp/pre_activate/common/source_pattern_test.cc b/tests/ut/cpp/pre_activate/common/source_pattern_test.cc
index 09043f46f60..67200c0c54f 100644
--- a/tests/ut/cpp/pre_activate/common/source_pattern_test.cc
+++ b/tests/ut/cpp/pre_activate/common/source_pattern_test.cc
@@ -88,11 +88,11 @@ TEST_F(TestSrcPattern, Var) {
   ASSERT_TRUE(build_pattern_map(mul1_cnode));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode1"), anode1));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode2"), anode2));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode3"), anode3));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode1"), anode1));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode2"), anode2));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode3"), anode3));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
 }
 
 /// Feature: PatternToPattern Pass
@@ -118,13 +118,13 @@ TEST_F(TestSrcPattern, SeqVar) {
   ASSERT_TRUE(build_pattern_map(mul1_cnode));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode1"), anode1));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode1"), anode1));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
   auto &v = m_->GetSeq("Sv");
   ASSERT_EQ(v.size(), std::size_t(2));
-  ASSERT_TRUE(opt::AnfEqual(v[0], anode2));
-  ASSERT_TRUE(opt::AnfEqual(v[1], anode3));
+  ASSERT_TRUE(AnfUtils::AnfEqual(v[0], anode2));
+  ASSERT_TRUE(AnfUtils::AnfEqual(v[1], anode3));
 }
 
 /// Feature: PatternToPattern Pass
@@ -159,12 +159,12 @@ TEST_F(TestSrcPattern, RepeatedVar) {
   ASSERT_TRUE(build_pattern_map(mul3_cnode));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode1"), anode1));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode2"), anode2));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode3"), anode3));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul3_cnode"), mul3_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode1"), anode1));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode2"), anode2));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode3"), anode3));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul3_cnode"), mul3_cnode));
 }
 
 /// Feature: PatternToPattern Pass
@@ -202,15 +202,15 @@ TEST_F(TestSrcPattern, RepeatedSeqVar) {
   ASSERT_TRUE(build_pattern_map(mul4_cnode));
 
   // check
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("anode1"), anode1));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul3_cnode"), mul3_cnode));
-  ASSERT_TRUE(opt::AnfEqual(m_->Get("mul4_cnode"), mul4_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("anode1"), anode1));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul1_cnode"), mul1_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul2_cnode"), mul2_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul3_cnode"), mul3_cnode));
+  ASSERT_TRUE(AnfUtils::AnfEqual(m_->Get("mul4_cnode"), mul4_cnode));
   auto &v = m_->GetSeq("Sv");
   ASSERT_EQ(v.size(), std::size_t(2));
-  ASSERT_TRUE(opt::AnfEqual(v[0], anode2));
-  ASSERT_TRUE(opt::AnfEqual(v[1], anode3));
+  ASSERT_TRUE(AnfUtils::AnfEqual(v[0], anode2));
+  ASSERT_TRUE(AnfUtils::AnfEqual(v[1], anode3));
 }
 
 /// Feature: PatternToPattern Pass
